{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49759a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 21:09:56.976518: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 21:09:57.756527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import io\n",
    "import gc\n",
    "import os\n",
    "import contextlib\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async' # to fix GPU issues\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, MaxPooling2D, Conv2D, concatenate, Layer, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.activations import relu, tanh, linear\n",
    "from tensorflow.keras.utils import Progbar, to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from scipy.signal import butter, lfilter\n",
    "from joblib import Parallel, delayed #Paralleize calculation\n",
    "from sqlalchemy import create_engine, Column, Integer, ARRAY, MetaData, Table, Text\n",
    "from sqlalchemy.dialects.postgresql import ARRAY as PG_ARRAY\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random seed for reproducability\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "# check if tensorflow uses GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851aac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom adapter function for postgre\n",
    "def adapt_numpy_ndarray(numpy_array):\n",
    "    return AsIs(list(numpy_array))\n",
    "# Register the postgre-adapter\n",
    "register_adapter(np.ndarray, adapt_numpy_ndarray)\n",
    "\n",
    "# Database connection parameters and alchemy engine\n",
    "dbname = 'bathunting'\n",
    "user = 'python'\n",
    "password = 'python_password'\n",
    "host = 'localhost'\n",
    "port = '5432' \n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "\n",
    "# Get Data-From the Database to a specific target\n",
    "def get_target_data(target, limit=0, no_target=False):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    #query = \"\"\n",
    "    if no_target:\n",
    "        query = f\"SELECT new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    else:\n",
    "        query = f\"SELECT target, new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    if no_target:\n",
    "        df = pd.DataFrame(df['new_arr'].tolist())\n",
    "    return df\n",
    "\n",
    "#del\n",
    "# messy, i know\n",
    "def get_targets_to_data(limit=0):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    query = f\"SELECT target FROM batcall where 10 < ANY(new_arr) {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df\n",
    "\n",
    "#del\n",
    "def get_all_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    return all_df\n",
    "\n",
    "def get_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    df = pd.concat(all_df)\n",
    "    return df\n",
    "\n",
    "#del\n",
    "# try to visualize only the maximum values per species\n",
    "def spectrogram_range(target):\n",
    "    df = get_target_data(target, limit=0, no_target=True)\n",
    "    max_vals = df.max()\n",
    "    min_vals = df.min()\n",
    "    \n",
    "    abs_pos = max_vals.abs()\n",
    "    abs_neg = min_vals.abs()\n",
    "    mask = (abs_pos > abs_neg).astype(bool)\n",
    "    return min_vals.where(mask, max_vals), max_vals.where(mask, min_vals)\n",
    "\n",
    "#del\n",
    "def get_targets():\n",
    "    #conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    #cursor = conn.cursor()\n",
    "    query = f\"SELECT target, bat FROM batcall group by target, bat order by target\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    #conn.close()\n",
    "    return df\n",
    "   \n",
    "# Get data to work with\n",
    "def get_features_and_targets(limit=100, scaler=StandardScaler(), categorical=True):\n",
    "    data = get_data(limit=limit)\n",
    "\n",
    "    df = pd.DataFrame(data[\"new_arr\"].tolist())\n",
    "    if scaler != None:\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    labels = pd.DataFrame(data[\"target\"])\n",
    "    if categorical:\n",
    "        labels = to_categorical(labels, num_classes=19)\n",
    "    return df, labels\n",
    "\n",
    "def vogl_conversion(df):\n",
    "    data_reshaped = []\n",
    "    for _,data in df.iterrows():\n",
    "        # Normalize\n",
    "        data -= np.mean(data)\n",
    "        data /= np.std(data)\n",
    "        # Calculate spectrogram with FFT\n",
    "        stft = np.abs(librosa.stft(np.array(data), n_fft=512, hop_length=32))\n",
    "        stft = 10 * np.log10(stft)\n",
    "        stft = np.nan_to_num(stft)\n",
    "        # Scale between [0,1] and reduce shape if needed\n",
    "        stft = (stft - np.min(stft)) / (np.max(stft) - np.min(stft))\n",
    "        stft = np.reshape(stft, (257, 138, 1))\n",
    "        stft = stft[:256, -128: , :]\n",
    "        data_reshaped.append(stft)\n",
    "    return np.array(data_reshaped)\n",
    "\n",
    "# Function to create accuracy- and loss-plots\n",
    "def visualize_history(history, title_appendix=''):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model accuracy {title_appendix}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/accuracy_{title_appendix}.png')\n",
    "    plt.close()\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model loss {title_appendix}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/loss_{title_appendix}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Define Inception Module class\n",
    "class InceptionModule(Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(InceptionModule, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "\n",
    "        # Define the convolutions and pooling operations\n",
    "        self.conv1 = Conv2D(filters=filters, kernel_size=(1, 1), padding='same', activation='relu')\n",
    "        self.conv3 = Conv2D(filters=filters, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv5 = Conv2D(filters=filters, kernel_size=(5, 5), padding='same', activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply the layers in parallel\n",
    "        x1 = self.conv1(inputs)\n",
    "        x3 = self.conv3(inputs)\n",
    "        x5 = self.conv5(inputs)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        return concatenate([x1, x3, x5], axis=-1)\n",
    "\n",
    "# Define another Inception Module class\n",
    "class InceptionModule2(Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(InceptionModule2, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "\n",
    "        # Define the convolutions and pooling operations\n",
    "        self.conv1 = Conv2D(filters=filters, kernel_size=(1, 1), padding='same', activation='relu')\n",
    "        self.conv3 = Conv2D(filters=filters, kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.conv5 = Conv2D(filters=filters, kernel_size=(5, 5), padding='same', activation='relu')\n",
    "        self.pool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply the layers in parallel\n",
    "        x1 = self.conv1(inputs)\n",
    "        x3 = self.conv3(inputs)\n",
    "        x5 = self.conv5(inputs)\n",
    "        pool = self.pool(inputs)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        return concatenate([x1, x3, x5, pool], axis=-1)\n",
    "\n",
    "# For error suppression\n",
    "@contextlib.contextmanager\n",
    "def options(options):\n",
    "    old_opts = tf.config.optimizer.get_experimental_options()\n",
    "    tf.config.optimizer.set_experimental_options(options)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        tf.config.optimizer.set_experimental_options(old_opts)\n",
    "\n",
    "def generate_confusion_matrix(X_test, y_test, title_appendix):\n",
    "    # Confusion Matrix\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)  # y_test are the true labels (one-hot encoded)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Plot the confusion matrix using Seaborn\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.savefig(f'Confusion/ConfusionMatrix_CNN_{title_appendix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ce4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25512/3626693085.py:88: RuntimeWarning: divide by zero encountered in log10\n",
      "  stft = 10 * np.log10(stft)\n"
     ]
    }
   ],
   "source": [
    "df, labels = get_features_and_targets(limit=500, scaler=None)\n",
    "df_reshaped = vogl_conversion(df)\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(df, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reshaped, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e0a05-6ba5-4482-8fdd-5312242fdab1",
   "metadata": {},
   "source": [
    "Zunächst wird ein sehr simples Fast Forward Neuronal Network mit unterschiedlicher Anzahl an Neuronen getestet. Die Tests werden parallel für die Optimizer RMS-Propagation und Adam ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aae7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 18:01:25.377526: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 204694560 exceeds 10% of free system memory.\n",
      "2024-01-17 18:01:27.224890: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1c24008180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-17 18:01:27.224947: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-17 18:01:27.326203: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-17 18:01:27.694037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-17 18:01:28.132594: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 3ms/step - loss: 4024.1917 - accuracy: 0.3445\n",
      "Density: 100; Optimizer: RMSP\n",
      "Final Training Loss: 39.60721969604492\n",
      "Final Training Accuracy: 0.9446742534637451\n",
      "Final Validation Loss: 4261.18359375\n",
      "Final Validation Accuracy: 0.3521709144115448\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 6184.5674 - accuracy: 0.3760\n",
      "Density: 200; Optimizer: RMSP\n",
      "Final Training Loss: 67.37409210205078\n",
      "Final Training Accuracy: 0.9639779329299927\n",
      "Final Validation Loss: 6869.66015625\n",
      "Final Validation Accuracy: 0.36457616090774536\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 8254.5000 - accuracy: 0.3710\n",
      "Density: 300; Optimizer: RMSP\n",
      "Final Training Loss: 88.25961303710938\n",
      "Final Training Accuracy: 0.9646673798561096\n",
      "Final Validation Loss: 8311.8740234375\n",
      "Final Validation Accuracy: 0.38869744539260864\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 10102.3789 - accuracy: 0.3831\n",
      "Density: 400; Optimizer: RMSP\n",
      "Final Training Loss: 97.06326293945312\n",
      "Final Training Accuracy: 0.9677697420120239\n",
      "Final Validation Loss: 10284.84765625\n",
      "Final Validation Accuracy: 0.3893866240978241\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 12715.4238 - accuracy: 0.3936\n",
      "Density: 500; Optimizer: RMSP\n",
      "Final Training Loss: 163.6810302734375\n",
      "Final Training Accuracy: 0.9667356014251709\n",
      "Final Validation Loss: 11506.349609375\n",
      "Final Validation Accuracy: 0.3983459770679474\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 14174.4541 - accuracy: 0.3809\n",
      "Density: 600; Optimizer: RMSP\n",
      "Final Training Loss: 177.9779815673828\n",
      "Final Training Accuracy: 0.9658738374710083\n",
      "Final Validation Loss: 14950.607421875\n",
      "Final Validation Accuracy: 0.38869744539260864\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 18162.8828 - accuracy: 0.3732\n",
      "Density: 700; Optimizer: RMSP\n",
      "Final Training Loss: 229.02134704589844\n",
      "Final Training Accuracy: 0.9679421186447144\n",
      "Final Validation Loss: 16584.26953125\n",
      "Final Validation Accuracy: 0.3880082666873932\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 18459.9414 - accuracy: 0.3749\n",
      "Density: 800; Optimizer: RMSP\n",
      "Final Training Loss: 189.5780792236328\n",
      "Final Training Accuracy: 0.9662185311317444\n",
      "Final Validation Loss: 19155.943359375\n",
      "Final Validation Accuracy: 0.38731908798217773\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 20377.3340 - accuracy: 0.3875\n",
      "Density: 900; Optimizer: RMSP\n",
      "Final Training Loss: 181.19100952148438\n",
      "Final Training Accuracy: 0.9724233150482178\n",
      "Final Validation Loss: 22301.587890625\n",
      "Final Validation Accuracy: 0.3997243344783783\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 23405.9824 - accuracy: 0.3716\n",
      "Density: 1000; Optimizer: RMSP\n",
      "Final Training Loss: 183.95664978027344\n",
      "Final Training Accuracy: 0.9684591293334961\n",
      "Final Validation Loss: 24360.751953125\n",
      "Final Validation Accuracy: 0.3742246627807617\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 23388.3379 - accuracy: 0.3881\n",
      "Density: 1100; Optimizer: RMSP\n",
      "Final Training Loss: 267.9458923339844\n",
      "Final Training Accuracy: 0.9686315059661865\n",
      "Final Validation Loss: 25253.857421875\n",
      "Final Validation Accuracy: 0.3969676196575165\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 28520.7031 - accuracy: 0.3749\n",
      "Density: 1200; Optimizer: RMSP\n",
      "Final Training Loss: 336.2799072265625\n",
      "Final Training Accuracy: 0.9662185311317444\n",
      "Final Validation Loss: 28533.146484375\n",
      "Final Validation Accuracy: 0.3790489435195923\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 27001.8672 - accuracy: 0.3826\n",
      "Density: 1300; Optimizer: RMSP\n",
      "Final Training Loss: 419.61566162109375\n",
      "Final Training Accuracy: 0.9655291438102722\n",
      "Final Validation Loss: 27271.966796875\n",
      "Final Validation Accuracy: 0.41075119376182556\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 31376.8359 - accuracy: 0.3815\n",
      "Density: 1400; Optimizer: RMSP\n",
      "Final Training Loss: 259.42132568359375\n",
      "Final Training Accuracy: 0.9732850790023804\n",
      "Final Validation Loss: 32393.67578125\n",
      "Final Validation Accuracy: 0.390764981508255\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 32476.7500 - accuracy: 0.3908\n",
      "Density: 1500; Optimizer: RMSP\n",
      "Final Training Loss: 284.3419189453125\n",
      "Final Training Accuracy: 0.9698379635810852\n",
      "Final Validation Loss: 33522.28515625\n",
      "Final Validation Accuracy: 0.3969676196575165\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 36914.7461 - accuracy: 0.3743\n",
      "Density: 1600; Optimizer: RMSP\n",
      "Final Training Loss: 231.45639038085938\n",
      "Final Training Accuracy: 0.9717338681221008\n",
      "Final Validation Loss: 35412.125\n",
      "Final Validation Accuracy: 0.3942108750343323\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 32943.5234 - accuracy: 0.3964\n",
      "Density: 1700; Optimizer: RMSP\n",
      "Final Training Loss: 296.2102355957031\n",
      "Final Training Accuracy: 0.9679421186447144\n",
      "Final Validation Loss: 35968.078125\n",
      "Final Validation Accuracy: 0.39765679836273193\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 41291.0234 - accuracy: 0.3848\n",
      "Density: 1800; Optimizer: RMSP\n",
      "Final Training Loss: 296.1709289550781\n",
      "Final Training Accuracy: 0.9694932699203491\n",
      "Final Validation Loss: 41734.86328125\n",
      "Final Validation Accuracy: 0.39903515577316284\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 39160.5000 - accuracy: 0.3859\n",
      "Density: 1900; Optimizer: RMSP\n",
      "Final Training Loss: 443.16021728515625\n",
      "Final Training Accuracy: 0.9724233150482178\n",
      "Final Validation Loss: 38418.87109375\n",
      "Final Validation Accuracy: 0.3983459770679474\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 40606.0078 - accuracy: 0.3886\n",
      "Density: 2000; Optimizer: RMSP\n",
      "Final Training Loss: 307.3857727050781\n",
      "Final Training Accuracy: 0.9696656465530396\n",
      "Final Validation Loss: 41320.30078125\n",
      "Final Validation Accuracy: 0.40179187059402466\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3767.8589 - accuracy: 0.3622\n",
      "Density: 100; Optimizer: ADAM\n",
      "Final Training Loss: 52.91666030883789\n",
      "Final Training Accuracy: 0.9482936859130859\n",
      "Final Validation Loss: 3841.43896484375\n",
      "Final Validation Accuracy: 0.36802205443382263\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 6638.5913 - accuracy: 0.3688\n",
      "Density: 200; Optimizer: ADAM\n",
      "Final Training Loss: 59.83811569213867\n",
      "Final Training Accuracy: 0.9576008319854736\n",
      "Final Validation Loss: 6317.37548828125\n",
      "Final Validation Accuracy: 0.3935216963291168\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 8326.9053 - accuracy: 0.3716\n",
      "Density: 300; Optimizer: ADAM\n",
      "Final Training Loss: 92.9473648071289\n",
      "Final Training Accuracy: 0.9612202644348145\n",
      "Final Validation Loss: 8677.1435546875\n",
      "Final Validation Accuracy: 0.37215712666511536\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 10555.2871 - accuracy: 0.3793\n",
      "Density: 400; Optimizer: ADAM\n",
      "Final Training Loss: 96.27994537353516\n",
      "Final Training Accuracy: 0.96811443567276\n",
      "Final Validation Loss: 11459.4599609375\n",
      "Final Validation Accuracy: 0.3749138414859772\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 12811.8750 - accuracy: 0.3743\n",
      "Density: 500; Optimizer: ADAM\n",
      "Final Training Loss: 163.8794403076172\n",
      "Final Training Accuracy: 0.9646673798561096\n",
      "Final Validation Loss: 12922.1484375\n",
      "Final Validation Accuracy: 0.4011026918888092\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 13827.0166 - accuracy: 0.3892\n",
      "Density: 600; Optimizer: ADAM\n",
      "Final Training Loss: 144.9685516357422\n",
      "Final Training Accuracy: 0.9684591293334961\n",
      "Final Validation Loss: 14608.650390625\n",
      "Final Validation Accuracy: 0.3880082666873932\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 16273.7061 - accuracy: 0.3897\n",
      "Density: 700; Optimizer: ADAM\n",
      "Final Training Loss: 157.9589080810547\n",
      "Final Training Accuracy: 0.9701827168464661\n",
      "Final Validation Loss: 17416.25\n",
      "Final Validation Accuracy: 0.4100620150566101\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 19033.9961 - accuracy: 0.3886\n",
      "Density: 800; Optimizer: ADAM\n",
      "Final Training Loss: 195.76002502441406\n",
      "Final Training Accuracy: 0.96811443567276\n",
      "Final Validation Loss: 18694.919921875\n",
      "Final Validation Accuracy: 0.39145416021347046\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 22725.2988 - accuracy: 0.3842\n",
      "Density: 900; Optimizer: ADAM\n",
      "Final Training Loss: 128.66574096679688\n",
      "Final Training Accuracy: 0.9682868123054504\n",
      "Final Validation Loss: 22225.9921875\n",
      "Final Validation Accuracy: 0.3983459770679474\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 22840.7461 - accuracy: 0.3969\n",
      "Density: 1000; Optimizer: ADAM\n",
      "Final Training Loss: 190.04415893554688\n",
      "Final Training Accuracy: 0.9636332392692566\n",
      "Final Validation Loss: 21627.5\n",
      "Final Validation Accuracy: 0.38387319445610046\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 24652.4590 - accuracy: 0.3870\n",
      "Density: 1100; Optimizer: ADAM\n",
      "Final Training Loss: 249.14764404296875\n",
      "Final Training Accuracy: 0.9720786213874817\n",
      "Final Validation Loss: 24620.91015625\n",
      "Final Validation Accuracy: 0.40041351318359375\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 26972.8555 - accuracy: 0.3760\n",
      "Density: 1200; Optimizer: ADAM\n",
      "Final Training Loss: 233.81900024414062\n",
      "Final Training Accuracy: 0.9736297726631165\n",
      "Final Validation Loss: 28443.09375\n",
      "Final Validation Accuracy: 0.3880082666873932\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 27452.2422 - accuracy: 0.4085\n",
      "Density: 1300; Optimizer: ADAM\n",
      "Final Training Loss: 307.6842041015625\n",
      "Final Training Accuracy: 0.9672526717185974\n",
      "Final Validation Loss: 27025.796875\n",
      "Final Validation Accuracy: 0.41695383191108704\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 29687.6895 - accuracy: 0.3776\n",
      "Density: 1400; Optimizer: ADAM\n",
      "Final Training Loss: 349.55841064453125\n",
      "Final Training Accuracy: 0.9672526717185974\n",
      "Final Validation Loss: 30670.71875\n",
      "Final Validation Accuracy: 0.39558926224708557\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 30728.4688 - accuracy: 0.3958\n",
      "Density: 1500; Optimizer: ADAM\n",
      "Final Training Loss: 444.56304931640625\n",
      "Final Training Accuracy: 0.9643226265907288\n",
      "Final Validation Loss: 34780.0234375\n",
      "Final Validation Accuracy: 0.39765679836273193\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 33367.5000 - accuracy: 0.4101\n",
      "Density: 1600; Optimizer: ADAM\n",
      "Final Training Loss: 345.260009765625\n",
      "Final Training Accuracy: 0.9689761996269226\n",
      "Final Validation Loss: 35347.99609375\n",
      "Final Validation Accuracy: 0.4045485854148865\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 34238.2461 - accuracy: 0.3798\n",
      "Density: 1700; Optimizer: ADAM\n",
      "Final Training Loss: 373.9734191894531\n",
      "Final Training Accuracy: 0.9672526717185974\n",
      "Final Validation Loss: 34549.015625\n",
      "Final Validation Accuracy: 0.39007580280303955\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 36164.6953 - accuracy: 0.3908\n",
      "Density: 1800; Optimizer: ADAM\n",
      "Final Training Loss: 407.15142822265625\n",
      "Final Training Accuracy: 0.9703550338745117\n",
      "Final Validation Loss: 39412.203125\n",
      "Final Validation Accuracy: 0.3983459770679474\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 41670.1172 - accuracy: 0.3958\n",
      "Density: 1900; Optimizer: ADAM\n",
      "Final Training Loss: 386.83612060546875\n",
      "Final Training Accuracy: 0.9705274105072021\n",
      "Final Validation Loss: 40922.859375\n",
      "Final Validation Accuracy: 0.40041351318359375\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 44136.0273 - accuracy: 0.3925\n",
      "Density: 2000; Optimizer: ADAM\n",
      "Final Training Loss: 419.5311584472656\n",
      "Final Training Accuracy: 0.969148576259613\n",
      "Final Validation Loss: 44648.50390625\n",
      "Final Validation Accuracy: 0.3949000835418701\n"
     ]
    }
   ],
   "source": [
    "# Density 100 to 2000\n",
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for i in range(100, 2001, 100):\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(i, activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "        # Compile the model\n",
    "        # Compile the model\n",
    "        opti = RMSprop(learning_rate=0.001)\n",
    "        if opti_str == 'ADAM':\n",
    "            opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=RMSprop(learning_rate=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        history = model.fit(X_train_raw, y_train_raw, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test_raw, y_test_raw)\n",
    "        print('Density: %.i; Optimizer: %s' % (i, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'dens={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "        \n",
    "        # Clear Keras session\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7c4ce-f920-4eff-a0c0-30c1e17fd5a1",
   "metadata": {},
   "source": [
    "Die Performance ist unterirdisch im Vergleich zu den Ergbnissen die mit der PCA erzielt wurde. Im Folgenden wird die Anzahl der Dense-Layer mit sinkender Anzahl an Neuronen erhöht. Außerdem wird Early-Stopping implementiert um Ressourcen (in erster Linie Zeit) zu sparen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cce9374-c9ff-4cca-8d26-a2b2a5ae94dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: early stopping\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 8841.8936 - accuracy: 0.3479\n",
      "Density: 0; Optimizer: RMSP\n",
      "Final Training Loss: 1624.73291015625\n",
      "Final Training Accuracy: 0.6659772396087646\n",
      "Final Validation Loss: 8665.13671875\n",
      "Final Validation Accuracy: 0.35286009311676025\n",
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 9.4374 - accuracy: 0.0606\n",
      "Density: 1; Optimizer: RMSP\n",
      "Final Training Loss: 2.8882193565368652\n",
      "Final Training Accuracy: 0.0713547021150589\n",
      "Final Validation Loss: 17.188777923583984\n",
      "Final Validation Accuracy: 0.056512750685214996\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9392 - accuracy: 0.0502\n",
      "Density: 2; Optimizer: RMSP\n",
      "Final Training Loss: 2.934691905975342\n",
      "Final Training Accuracy: 0.05825577303767204\n",
      "Final Validation Loss: 2.9395864009857178\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9407 - accuracy: 0.0496\n",
      "Density: 3; Optimizer: RMSP\n",
      "Final Training Loss: 2.9372060298919678\n",
      "Final Training Accuracy: 0.05722164735198021\n",
      "Final Validation Loss: 2.9395620822906494\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9407 - accuracy: 0.0496\n",
      "Density: 4; Optimizer: RMSP\n",
      "Final Training Loss: 2.9372098445892334\n",
      "Final Training Accuracy: 0.05704929307103157\n",
      "Final Validation Loss: 2.9396274089813232\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 2.9407 - accuracy: 0.0496\n",
      "Density: 5; Optimizer: RMSP\n",
      "Final Training Loss: 2.937209129333496\n",
      "Final Training Accuracy: 0.05670458450913429\n",
      "Final Validation Loss: 2.939605474472046\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 6: early stopping\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 20817.1699 - accuracy: 0.3015\n",
      "Density: 0; Optimizer: ADAM\n",
      "Final Training Loss: 5302.93359375\n",
      "Final Training Accuracy: 0.5461909770965576\n",
      "Final Validation Loss: 20681.818359375\n",
      "Final Validation Accuracy: 0.3184010982513428\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9208 - accuracy: 0.0480\n",
      "Density: 1; Optimizer: ADAM\n",
      "Final Training Loss: 2.9163479804992676\n",
      "Final Training Accuracy: 0.05997931584715843\n",
      "Final Validation Loss: 2.924872636795044\n",
      "Final Validation Accuracy: 0.06753962486982346\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3.0531 - accuracy: 0.0507\n",
      "Density: 2; Optimizer: ADAM\n",
      "Final Training Loss: 2.93019437789917\n",
      "Final Training Accuracy: 0.05722164735198021\n",
      "Final Validation Loss: 2.937326669692993\n",
      "Final Validation Accuracy: 0.05306684970855713\n",
      "Epoch 6: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3.3153 - accuracy: 0.0540\n",
      "Density: 3; Optimizer: ADAM\n",
      "Final Training Loss: 2.9363410472869873\n",
      "Final Training Accuracy: 0.05601516738533974\n",
      "Final Validation Loss: 2.9391987323760986\n",
      "Final Validation Accuracy: 0.04686423018574715\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 4; Optimizer: ADAM\n",
      "Final Training Loss: 2.937164306640625\n",
      "Final Training Accuracy: 0.05394691601395607\n",
      "Final Validation Loss: 2.9394431114196777\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.9403 - accuracy: 0.0496\n",
      "Density: 5; Optimizer: ADAM\n",
      "Final Training Loss: 2.9371731281280518\n",
      "Final Training Accuracy: 0.05446397885680199\n",
      "Final Validation Loss: 2.93955135345459\n",
      "Final Validation Accuracy: 0.052377671003341675\n"
     ]
    }
   ],
   "source": [
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for i in range(6):\n",
    "        title_appendix = f'FFNN_layer{11-i}_optimizer={opti_str}'\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(pow(2,10), activation='relu'))\n",
    "        for x in range(i):\n",
    "            model.add(Dense(pow(2,10-(x+1)), activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "        optimizer=RMSprop(learning_rate=0.001)\n",
    "        if opti_str == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "        history = model.fit(X_train_raw, y_train_raw, epochs=50, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test_raw, y_test_raw)\n",
    "        print('Density: %.i; Optimizer: %s' % (i, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "        \n",
    "        # Clear Keras session\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6976e21-ca0d-444b-99da-fe9e68ae94a8",
   "metadata": {},
   "source": [
    "Eine steigende Anzahl von Layern scheind die Performance des FFNN auch nicht retten zu können. Das FFNN scheint für die Aufgabe nicht geeignet. Im Folgenden wird das Augenmerk auf Convolutional Neuronal Networks gelegt. Erneut wird mit einem Simplen Model gestartet und zunächst die Anzahl der Filter variiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe3a566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 19:57:03.238458: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-17 19:57:03.239003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6587 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-17 19:57:05.631532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-17 19:57:06.112769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0598009f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-17 19:57:06.112797: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-17 19:57:06.121540: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-17 19:57:06.214371: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.9128 - accuracy: 0.7911\n",
      "Filtersize: 16; Optimizer: RMSP\n",
      "Final Training Loss: 0.016498129814863205\n",
      "Final Training Accuracy: 0.9974147081375122\n",
      "Final Validation Loss: 0.9046211838722229\n",
      "Final Validation Accuracy: 0.7842866778373718\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 0.9014 - accuracy: 0.7834\n",
      "Filtersize: 32; Optimizer: RMSP\n",
      "Final Training Loss: 0.019769055768847466\n",
      "Final Training Accuracy: 0.9960358738899231\n",
      "Final Validation Loss: 0.8672486543655396\n",
      "Final Validation Accuracy: 0.7780840992927551\n",
      "Epoch 12: early stopping\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.9800 - accuracy: 0.7872\n",
      "Filtersize: 64; Optimizer: RMSP\n",
      "Final Training Loss: 0.043943166732788086\n",
      "Final Training Accuracy: 0.9903481602668762\n",
      "Final Validation Loss: 0.8996410965919495\n",
      "Final Validation Accuracy: 0.7856650352478027\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.7631 - accuracy: 0.7867\n",
      "Filtersize: 16; Optimizer: ADAM\n",
      "Final Training Loss: 0.00521038519218564\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 0.7191843390464783\n",
      "Final Validation Accuracy: 0.7787732481956482\n",
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.8040 - accuracy: 0.7751\n",
      "Filtersize: 32; Optimizer: ADAM\n",
      "Final Training Loss: 0.04900086298584938\n",
      "Final Training Accuracy: 0.9879351854324341\n",
      "Final Validation Loss: 0.721869707107544\n",
      "Final Validation Accuracy: 0.7863542437553406\n",
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 1.0536 - accuracy: 0.7150\n",
      "Filtersize: 64; Optimizer: ADAM\n",
      "Final Training Loss: 0.0262006763368845\n",
      "Final Training Accuracy: 0.9955188035964966\n",
      "Final Validation Loss: 0.9956095814704895\n",
      "Final Validation Accuracy: 0.7394900321960449\n"
     ]
    }
   ],
   "source": [
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for fs in [16,32,64]:\n",
    "        title_appendix = f'CNN_filtersize={fs}_optimizer={opti_str}'\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(fs, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "        optimizer=RMSprop(learning_rate=0.001)\n",
    "        if opti_str == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print('Filtersize: %.i; Optimizer: %s' % (fs, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "        \n",
    "        # Clear Keras session\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b57e4-a316-4441-9095-88499b0b1c35",
   "metadata": {},
   "source": [
    "Die Anzahl der Filter pro layer hat besten falls einen geringen Einfluss auf die Modelperformance daher wird im Folgenden eine Filtersize von 16 bevorzugt verwendet.\n",
    "\n",
    "Da der validation-loss sehr schnell wieder zu steigen beginnt bzw. nicht mehr fällt, werden im Folgenden kleinere Learnraten ausprobiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc3bf60-711b-42df-b1d6-eefb89bb731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.9799 - accuracy: 0.7872\n",
      "Learning rate: 0.00050; Optimizer: RMSP\n",
      "Final Training Loss: 0.0005260215839371085\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 0.9474879503250122\n",
      "Final Validation Accuracy: 0.7856650352478027\n",
      "Epoch 39: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.2769 - accuracy: 0.5502\n",
      "Learning rate: 0.00010; Optimizer: RMSP\n",
      "Final Training Loss: 0.8918685913085938\n",
      "Final Training Accuracy: 0.713374674320221\n",
      "Final Validation Loss: 1.232358455657959\n",
      "Final Validation Accuracy: 0.5809786319732666\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.2332 - accuracy: 0.2420\n",
      "Learning rate: 0.00005; Optimizer: RMSP\n",
      "Final Training Loss: 2.1957502365112305\n",
      "Final Training Accuracy: 0.24043433368206024\n",
      "Final Validation Loss: 2.185626745223999\n",
      "Final Validation Accuracy: 0.26946932077407837\n",
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.9402 - accuracy: 0.0540\n",
      "Learning rate: 0.00050; Optimizer: ADAM\n",
      "Final Training Loss: 2.936915874481201\n",
      "Final Training Accuracy: 0.05756635591387749\n",
      "Final Validation Loss: 2.9394147396087646\n",
      "Final Validation Accuracy: 0.04686423018574715\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.6192 - accuracy: 0.4587\n",
      "Learning rate: 0.00010; Optimizer: ADAM\n",
      "Final Training Loss: 1.4800888299942017\n",
      "Final Training Accuracy: 0.49982765316963196\n",
      "Final Validation Loss: 1.536544919013977\n",
      "Final Validation Accuracy: 0.4693315029144287\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.9741 - accuracy: 0.6792\n",
      "Learning rate: 0.00005; Optimizer: ADAM\n",
      "Final Training Loss: 0.823505163192749\n",
      "Final Training Accuracy: 0.7411237359046936\n",
      "Final Validation Loss: 0.9054637551307678\n",
      "Final Validation Accuracy: 0.6946933269500732\n"
     ]
    }
   ],
   "source": [
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for lr in [0.0005, 0.0001, 0.00005]:\n",
    "        title_appendix = f'CNN_learingrate={lr}_optimizer={opti_str}'\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "        optimizer=RMSprop(learning_rate=lr)\n",
    "        if opti_str == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print('Learning rate: %.5f; Optimizer: %s' % (lr, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "        \n",
    "        # Clear Keras session\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562b50b-4212-4b8b-a2ec-edb6e7706714",
   "metadata": {},
   "source": [
    "Die Genauigkeit verschlechtert sich mit verringerter Lernrate für die RMS-Propagation, wärend es für den Adam-Optimizer so aussieht als könnte eine Kleinere Learningrate bei mehr Epochen noch ein besseres Ergebniss erzielen wobei die Modellstabilität noch verbessert werden sollte (die gezackten Graphen deuten auf ein eine geringe Modellstabilität hin).\n",
    "\n",
    "Um auszuschließen, dass größere Lernraten sich positiv auf das Model auswirken können werden nun auch noch (Start-)Lernraten von 0.01 und 0.005 überprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1073a19f-8efc-4d69-b544-46c3d9ed1d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.8324 - accuracy: 0.7144\n",
      "Learning rate: 0.01000; Optimizer: RMSP\n",
      "Final Training Loss: 0.7688438296318054\n",
      "Final Training Accuracy: 0.81540846824646\n",
      "Final Validation Loss: 1.6582608222961426\n",
      "Final Validation Accuracy: 0.7243280410766602\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.1413 - accuracy: 0.7222\n",
      "Learning rate: 0.00500; Optimizer: RMSP\n",
      "Final Training Loss: 0.4628809094429016\n",
      "Final Training Accuracy: 0.9607031941413879\n",
      "Final Validation Loss: 1.9525973796844482\n",
      "Final Validation Accuracy: 0.727084755897522\n",
      "Epoch 8: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.9488 - accuracy: 0.0496\n",
      "Learning rate: 0.01000; Optimizer: ADAM\n",
      "Final Training Loss: 2.940347909927368\n",
      "Final Training Accuracy: 0.050155118107795715\n",
      "Final Validation Loss: 2.9431560039520264\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 35: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.8094 - accuracy: 0.0954\n",
      "Learning rate: 0.00500; Optimizer: ADAM\n",
      "Final Training Loss: 2.8170762062072754\n",
      "Final Training Accuracy: 0.086866594851017\n",
      "Final Validation Loss: 2.8087353706359863\n",
      "Final Validation Accuracy: 0.10475534200668335\n"
     ]
    }
   ],
   "source": [
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for lr in [0.01, 0.005]:\n",
    "        title_appendix = f'CNN_learingrate={lr}_optimizer={opti_str}'\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "        optimizer=RMSprop(learning_rate=lr)\n",
    "        if opti_str == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print('Learning rate: %.5f; Optimizer: %s' % (lr, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "        \n",
    "        # Clear Keras session\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca74d9a-66b1-4564-b768-780ed8b582ff",
   "metadata": {},
   "source": [
    "Um die vorherige Vermutung zum Adam-Optimizer zu überprüfen wird außerdem für den ADAM-Optizer nochmal ein Model mit einer Starting-Learing-Rate von einmal 0.00005 und einmal 0.00001 und 100 Epochen trainiert. Außerdem wird noch ein Dropout-Layer mit variierenden Dropout-Rates hinzugefügt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7577c7-cf3e-4b8c-8667-51a372785f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 21:30:35.719349: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-17 21:30:35.719413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6628 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-17 21:30:38.114267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-17 21:30:38.603851: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdb3c018900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-17 21:30:38.603900: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-17 21:30:38.616642: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-17 21:30:38.766375: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 3.0371 - accuracy: 0.5292\n",
      "Learning rate: 0.00500; Optimizer: ADAM\n",
      "Final Training Loss: 0.0998116284608841\n",
      "Final Training Accuracy: 0.9719062447547913\n",
      "Final Validation Loss: 2.8007702827453613\n",
      "Final Validation Accuracy: 0.534114420413971\n",
      "Epoch 7: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.9406 - accuracy: 0.0419\n",
      "Learning rate: 0.00500; Optimizer: ADAM\n",
      "Final Training Loss: 2.9401936531066895\n",
      "Final Training Accuracy: 0.0527404360473156\n",
      "Final Validation Loss: 2.9403390884399414\n",
      "Final Validation Accuracy: 0.06271536648273468\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 1.8579 - accuracy: 0.4112\n",
      "Learning rate: 0.00500; Optimizer: ADAM\n",
      "Final Training Loss: 1.953525424003601\n",
      "Final Training Accuracy: 0.36590829491615295\n",
      "Final Validation Loss: 1.8103857040405273\n",
      "Final Validation Accuracy: 0.4141971170902252\n"
     ]
    }
   ],
   "source": [
    "#rerun\n",
    "for opti_str in ['ADAM']:\n",
    "    for lr in [0.00005, 0.00001]:\n",
    "        for dor in [0.2,0.35,0.5]:\n",
    "            title_appendix = f'CNN_learingrate={lr}_optimizer={opti_str}_dropoutrate={dor}'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dropout(dor))\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "                    \n",
    "            optimizer=RMSprop(learning_rate=lr)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "            # Fit the model\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "        \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Learning rate: %.5f; Optimizer: %s' % (lr, opti_str))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e703d-195e-4f93-90f2-1f5b4187160c",
   "metadata": {},
   "source": [
    "Weder eine Erhöhte Lernrate noch der Adam Optimizer mit einer Start-Lernrate von 0.00005 in Verbindung mit Dropout-Regularisierung hat eine positive Wirkung auf das Model. Die Lernrate wird daher im Folgenden bei 0.001 sowohl für RMS-Propagation als auch für den Adam-Optimizer belassen.\n",
    "\n",
    "Wie verändert sich die Genauigkeit wenn die Anzahl der Layer erhöht wird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2857932a-9599-4c73-a6c3-707feba05c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 22:00:34.419321: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-17 22:00:34.419390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6600 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-17 22:00:36.788741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-17 22:00:37.307370: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe86002b680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-17 22:00:37.307433: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-17 22:00:37.312904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-17 22:00:37.403896: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8891 - accuracy: 0.7861\n",
      "Number of layers: 1; Optimizer: RMSP\n",
      "Final Training Loss: 0.012454562820494175\n",
      "Final Training Accuracy: 0.997931718826294\n",
      "Final Validation Loss: 0.8633716106414795\n",
      "Final Validation Accuracy: 0.7884217500686646\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.8002 - accuracy: 0.8247\n",
      "Number of layers: 2; Optimizer: RMSP\n",
      "Final Training Loss: 0.045018866658210754\n",
      "Final Training Accuracy: 0.9860392808914185\n",
      "Final Validation Loss: 0.8245498538017273\n",
      "Final Validation Accuracy: 0.8201240301132202\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6881 - accuracy: 0.8269\n",
      "Number of layers: 3; Optimizer: RMSP\n",
      "Final Training Loss: 0.12392747402191162\n",
      "Final Training Accuracy: 0.9572561383247375\n",
      "Final Validation Loss: 0.680514395236969\n",
      "Final Validation Accuracy: 0.8159889578819275\n",
      "Epoch 16: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6081 - accuracy: 0.8407\n",
      "Number of layers: 4; Optimizer: RMSP\n",
      "Final Training Loss: 0.22258955240249634\n",
      "Final Training Accuracy: 0.9260599613189697\n",
      "Final Validation Loss: 0.5424418449401855\n",
      "Final Validation Accuracy: 0.8449345231056213\n",
      "Epoch 30: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.5419 - accuracy: 0.8313\n",
      "Number of layers: 5; Optimizer: RMSP\n",
      "Final Training Loss: 0.2841845750808716\n",
      "Final Training Accuracy: 0.9034815430641174\n",
      "Final Validation Loss: 0.5251269936561584\n",
      "Final Validation Accuracy: 0.8270158767700195\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6529 - accuracy: 0.7795\n",
      "Number of layers: 6; Optimizer: RMSP\n",
      "Final Training Loss: 0.6254246234893799\n",
      "Final Training Accuracy: 0.7821440696716309\n",
      "Final Validation Loss: 0.6338112354278564\n",
      "Final Validation Accuracy: 0.7822191715240479\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 1.2718 - accuracy: 0.5259\n",
      "Number of layers: 7; Optimizer: RMSP\n",
      "Final Training Loss: 0.8961459994316101\n",
      "Final Training Accuracy: 0.6859703660011292\n",
      "Final Validation Loss: 1.2523308992385864\n",
      "Final Validation Accuracy: 0.5320468544960022\n",
      "Epoch 9: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9406 - accuracy: 0.0496\n",
      "Number of layers: 1; Optimizer: ADAM\n",
      "Final Training Loss: 2.9371633529663086\n",
      "Final Training Accuracy: 0.05411927029490471\n",
      "Final Validation Loss: 2.9394521713256836\n",
      "Final Validation Accuracy: 0.052377671003341675\n",
      "Epoch 12: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.8647 - accuracy: 0.7778\n",
      "Number of layers: 2; Optimizer: ADAM\n",
      "Final Training Loss: 0.04155704751610756\n",
      "Final Training Accuracy: 0.9874181151390076\n",
      "Final Validation Loss: 0.7911882400512695\n",
      "Final Validation Accuracy: 0.7953135967254639\n",
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5975 - accuracy: 0.8506\n",
      "Number of layers: 3; Optimizer: ADAM\n",
      "Final Training Loss: 0.10412570089101791\n",
      "Final Training Accuracy: 0.9624267220497131\n",
      "Final Validation Loss: 0.6078178882598877\n",
      "Final Validation Accuracy: 0.8401102423667908\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6044 - accuracy: 0.8120\n",
      "Number of layers: 4; Optimizer: ADAM\n",
      "Final Training Loss: 0.2139454185962677\n",
      "Final Training Accuracy: 0.9229575991630554\n",
      "Final Validation Loss: 0.5552670359611511\n",
      "Final Validation Accuracy: 0.8084080219268799\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5396 - accuracy: 0.8258\n",
      "Number of layers: 5; Optimizer: ADAM\n",
      "Final Training Loss: 0.21200627088546753\n",
      "Final Training Accuracy: 0.9233022928237915\n",
      "Final Validation Loss: 0.5101437568664551\n",
      "Final Validation Accuracy: 0.8387318849563599\n",
      "Epoch 32: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6279 - accuracy: 0.7889\n",
      "Number of layers: 6; Optimizer: ADAM\n",
      "Final Training Loss: 0.43258580565452576\n",
      "Final Training Accuracy: 0.8357462882995605\n",
      "Final Validation Loss: 0.5506370067596436\n",
      "Final Validation Accuracy: 0.7987594604492188\n",
      "Epoch 8: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 2.9415 - accuracy: 0.0540\n",
      "Number of layers: 7; Optimizer: ADAM\n",
      "Final Training Loss: 2.9379377365112305\n",
      "Final Training Accuracy: 0.05153395235538483\n",
      "Final Validation Loss: 2.939992666244507\n",
      "Final Validation Accuracy: 0.04686423018574715\n"
     ]
    }
   ],
   "source": [
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for opti_str in ['RMSP', 'ADAM']:\n",
    "        for i in range(7): # mehr geht mit diesem Konstrukt nicht\n",
    "            title_appendix = f'CNN_nr_of_layers={i+1}_optimizer={opti_str}'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(i):\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "            \n",
    "            optimizer=RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of layers: %.i; Optimizer: %s' % (i+1, opti_str))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ef4dc-5e4d-4023-af15-57df9351a6b7",
   "metadata": {},
   "source": [
    "Beide Optimizer scheinen sich bei 3 bzw. 4 Convolution-MaxPooling-Dropout-Layer-Kombinationen einem Optimum anzunähern. \n",
    "\n",
    "Um eine bessere Modelstabilität und ggf. auch eine bessere -performance zu erreichen werden noch verschiedene Batchsizes für das jeweilige Model ausprobiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdb8487-3b0f-447e-b098-75aa67e502e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.8346\n",
      "Number of layers: 5; Optimizer: RMSP; Batchsize: 16\n",
      "Final Training Loss: 0.22930923104286194\n",
      "Final Training Accuracy: 0.9191657900810242\n",
      "Final Validation Loss: 0.473061740398407\n",
      "Final Validation Accuracy: 0.8414885997772217\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5014 - accuracy: 0.8374\n",
      "Number of layers: 5; Optimizer: RMSP; Batchsize: 32\n",
      "Final Training Loss: 0.25608035922050476\n",
      "Final Training Accuracy: 0.90434330701828\n",
      "Final Validation Loss: 0.4716217517852783\n",
      "Final Validation Accuracy: 0.8407994508743286\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6599 - accuracy: 0.7850\n",
      "Number of layers: 5; Optimizer: RMSP; Batchsize: 64\n",
      "Final Training Loss: 0.2648763656616211\n",
      "Final Training Accuracy: 0.9024474024772644\n",
      "Final Validation Loss: 0.6087037920951843\n",
      "Final Validation Accuracy: 0.7994486689567566\n",
      "Epoch 40: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.5870 - accuracy: 0.8374\n",
      "Number of layers: 5; Optimizer: RMSP; Batchsize: 128\n",
      "Final Training Loss: 0.2031434327363968\n",
      "Final Training Accuracy: 0.923819363117218\n",
      "Final Validation Loss: 0.4993740916252136\n",
      "Final Validation Accuracy: 0.8442453742027283\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5735 - accuracy: 0.8506\n",
      "Number of layers: 4; Optimizer: ADAM; Batchsize: 16\n",
      "Final Training Loss: 0.12854968011379242\n",
      "Final Training Accuracy: 0.9544984698295593\n",
      "Final Validation Loss: 0.4737494885921478\n",
      "Final Validation Accuracy: 0.8690558075904846\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5341 - accuracy: 0.8561\n",
      "Number of layers: 4; Optimizer: ADAM; Batchsize: 32\n",
      "Final Training Loss: 0.15748049318790436\n",
      "Final Training Accuracy: 0.94122713804245\n",
      "Final Validation Loss: 0.4708065986633301\n",
      "Final Validation Accuracy: 0.8532046675682068\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5854 - accuracy: 0.8473\n",
      "Number of layers: 4; Optimizer: ADAM; Batchsize: 64\n",
      "Final Training Loss: 0.11957816779613495\n",
      "Final Training Accuracy: 0.9588072896003723\n",
      "Final Validation Loss: 0.5069839358329773\n",
      "Final Validation Accuracy: 0.848380446434021\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5708 - accuracy: 0.8346\n",
      "Number of layers: 4; Optimizer: ADAM; Batchsize: 128\n",
      "Final Training Loss: 0.1929580122232437\n",
      "Final Training Accuracy: 0.930885910987854\n",
      "Final Validation Loss: 0.5095521807670593\n",
      "Final Validation Accuracy: 0.8421778082847595\n"
     ]
    }
   ],
   "source": [
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(4,'RMSP'),(3,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            title_appendix = f'CNN_nr_of_layers={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(layer):\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "            \n",
    "            optimizer=RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=pow(2,4+bs), verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f8a94-4a56-460e-80bb-7371ba470bf0",
   "metadata": {},
   "source": [
    "Die verwendete Zahl an Layern entspricht leider nicht wie geplant dem gefundenen Optimum aus der letzten Versuchsreihe. Die Werte ist allerdings dennoch ansehnlich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f6259b-334a-4a02-892b-716a9fa6e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5969 - accuracy: 0.8445\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 16\n",
      "Final Training Loss: 0.12719391286373138\n",
      "Final Training Accuracy: 0.9569113850593567\n",
      "Final Validation Loss: 0.5409477949142456\n",
      "Final Validation Accuracy: 0.8525155186653137\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.7089 - accuracy: 0.8313\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 32\n",
      "Final Training Loss: 0.09642841666936874\n",
      "Final Training Accuracy: 0.9684591293334961\n",
      "Final Validation Loss: 0.5972511172294617\n",
      "Final Validation Accuracy: 0.8476912379264832\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6385 - accuracy: 0.8379\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 64\n",
      "Final Training Loss: 0.10456839948892593\n",
      "Final Training Accuracy: 0.9641503095626831\n",
      "Final Validation Loss: 0.55624920129776\n",
      "Final Validation Accuracy: 0.8401102423667908\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6165 - accuracy: 0.8368\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 128\n",
      "Final Training Loss: 0.16355985403060913\n",
      "Final Training Accuracy: 0.9426059722900391\n",
      "Final Validation Loss: 0.6110711693763733\n",
      "Final Validation Accuracy: 0.8304617404937744\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.7058 - accuracy: 0.7993\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 16\n",
      "Final Training Loss: 0.14578068256378174\n",
      "Final Training Accuracy: 0.9508789777755737\n",
      "Final Validation Loss: 0.6875331401824951\n",
      "Final Validation Accuracy: 0.8035837411880493\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.5789 - accuracy: 0.8363\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 32\n",
      "Final Training Loss: 0.11044716835021973\n",
      "Final Training Accuracy: 0.962082028388977\n",
      "Final Validation Loss: 0.5470614433288574\n",
      "Final Validation Accuracy: 0.8456237316131592\n",
      "Epoch 16: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.5975 - accuracy: 0.8275\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Final Training Loss: 0.09131407737731934\n",
      "Final Training Accuracy: 0.9722509384155273\n",
      "Final Validation Loss: 0.5598480701446533\n",
      "Final Validation Accuracy: 0.835286021232605\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.8473\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 128\n",
      "Final Training Loss: 0.13452349603176117\n",
      "Final Training Accuracy: 0.9550155401229858\n",
      "Final Validation Loss: 0.4864223003387451\n",
      "Final Validation Accuracy: 0.8456237316131592\n"
     ]
    }
   ],
   "source": [
    "# Ups da hab ich die Anzahl der Layer nicht richtig gesetzt. Hier nochmal richtig.\n",
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(3,'RMSP'),(2,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            title_appendix = f'CNN_nr_of_layers={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(layer):\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "            \n",
    "            optimizer=RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=pow(2,4+bs), verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e5550-3748-4236-8a49-c7a1cbc85ecf",
   "metadata": {},
   "source": [
    "Die Performance des Models lässt sich durch eine größere batchsize tatsächlich minimal erhöhen. Die Plots deuten allerdings immer noch auf ein instabiles Model hin, daher wird nun das ganze nochmal mit Batch-Normalisierung probiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f265d965-f065-4c65-9764-5ec0fc3ebaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.5133 - accuracy: 0.8600\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 16\n",
      "Final Training Loss: 0.12029051035642624\n",
      "Final Training Accuracy: 0.9551878571510315\n",
      "Final Validation Loss: 0.47580215334892273\n",
      "Final Validation Accuracy: 0.8518263101577759\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6375 - accuracy: 0.8456\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 32\n",
      "Final Training Loss: 0.08119230717420578\n",
      "Final Training Accuracy: 0.9701827168464661\n",
      "Final Validation Loss: 0.5569902062416077\n",
      "Final Validation Accuracy: 0.8580289483070374\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5868 - accuracy: 0.8379\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 64\n",
      "Final Training Loss: 0.11227884143590927\n",
      "Final Training Accuracy: 0.9598414301872253\n",
      "Final Validation Loss: 0.5436047315597534\n",
      "Final Validation Accuracy: 0.8277050256729126\n",
      "Epoch 6: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 3.3041 - accuracy: 0.1047\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 128\n",
      "Final Training Loss: 0.6936113238334656\n",
      "Final Training Accuracy: 0.7557738423347473\n",
      "Final Validation Loss: 3.3774428367614746\n",
      "Final Validation Accuracy: 0.07994486391544342\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6976 - accuracy: 0.8241\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 16\n",
      "Final Training Loss: 0.0958428680896759\n",
      "Final Training Accuracy: 0.9689761996269226\n",
      "Final Validation Loss: 0.6472266912460327\n",
      "Final Validation Accuracy: 0.8242591023445129\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6080 - accuracy: 0.8313\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 32\n",
      "Final Training Loss: 0.10982946306467056\n",
      "Final Training Accuracy: 0.961737334728241\n",
      "Final Validation Loss: 0.5458729267120361\n",
      "Final Validation Accuracy: 0.8311509490013123\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5755 - accuracy: 0.8467\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Final Training Loss: 0.040663693100214005\n",
      "Final Training Accuracy: 0.9887969493865967\n",
      "Final Validation Loss: 0.5542363524436951\n",
      "Final Validation Accuracy: 0.8414885997772217\n",
      "Epoch 6: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 3.2482 - accuracy: 0.0849\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 128\n",
      "Final Training Loss: 0.9438210725784302\n",
      "Final Training Accuracy: 0.6935539245605469\n",
      "Final Validation Loss: 3.2985074520111084\n",
      "Final Validation Accuracy: 0.07787732779979706\n"
     ]
    }
   ],
   "source": [
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(3,'RMSP'),(2,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            title_appendix = f'CNN_nr_of_layers={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}_batchnormalization'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), padding='same', input_shape=(256,128,1)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(layer):\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv2D(16, (3,3), padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "            \n",
    "            optimizer=RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=pow(2,4+bs), verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4404b-4bac-4660-a7a0-431d881d4d9c",
   "metadata": {},
   "source": [
    "Die Batchnormalisation konnte das Model leider nicht weiter stabilisieren wie man in den zugehörigen Plots erkennen kann. Auch scheint die patienc für das Ealy-Stopping zu gering zu sein. Dies wird im folgenden überprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c69251-9f4d-4f52-a3c5-b4eee94f65b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 08:11:37.015226: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-18 08:11:37.015358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6936 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-18 08:11:40.875360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-18 08:11:42.672306: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6e00063270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-18 08:11:42.672339: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-18 08:11:42.728535: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-18 08:11:43.029941: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.0291 - accuracy: 0.6979\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 16\n",
      "Final Training Loss: 0.06728831678628922\n",
      "Final Training Accuracy: 0.9775939583778381\n",
      "Final Validation Loss: 0.9586533308029175\n",
      "Final Validation Accuracy: 0.7070985436439514\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6715 - accuracy: 0.8517\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 32\n",
      "Final Training Loss: 0.04964965209364891\n",
      "Final Training Accuracy: 0.9822474718093872\n",
      "Final Validation Loss: 0.6214357614517212\n",
      "Final Validation Accuracy: 0.8442453742027283\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7811 - accuracy: 0.7927\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 64\n",
      "Final Training Loss: 0.06688006967306137\n",
      "Final Training Accuracy: 0.9772492051124573\n",
      "Final Validation Loss: 0.7461711764335632\n",
      "Final Validation Accuracy: 0.7904893159866333\n",
      "Epoch 35: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7488 - accuracy: 0.7900\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 128\n",
      "Final Training Loss: 0.05801776051521301\n",
      "Final Training Accuracy: 0.9822474718093872\n",
      "Final Validation Loss: 0.699806809425354\n",
      "Final Validation Accuracy: 0.793935239315033\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6798 - accuracy: 0.8396\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 16\n",
      "Final Training Loss: 0.057569317519664764\n",
      "Final Training Accuracy: 0.9813857078552246\n",
      "Final Validation Loss: 0.6612424254417419\n",
      "Final Validation Accuracy: 0.837353527545929\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6191 - accuracy: 0.8456\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 32\n",
      "Final Training Loss: 0.0691349059343338\n",
      "Final Training Accuracy: 0.976215124130249\n",
      "Final Validation Loss: 0.5957378149032593\n",
      "Final Validation Accuracy: 0.8463128805160522\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5584 - accuracy: 0.8523\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Final Training Loss: 0.049139317125082016\n",
      "Final Training Accuracy: 0.9841433763504028\n",
      "Final Validation Loss: 0.5058634281158447\n",
      "Final Validation Accuracy: 0.8538938760757446\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.6276 - accuracy: 0.8247\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 128\n",
      "Final Training Loss: 0.039040252566337585\n",
      "Final Training Accuracy: 0.9918993711471558\n",
      "Final Validation Loss: 0.5722787380218506\n",
      "Final Validation Accuracy: 0.8407994508743286\n"
     ]
    }
   ],
   "source": [
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(3,'RMSP'),(2,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            title_appendix = f'CNN_nr_of_layers={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}_batchnormalization'\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (3,3), padding='same', input_shape=(256,128,1)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(layer):\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv2D(16, (3,3), padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(500, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "            optimizer=RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=pow(2,4+bs), verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df48eb1-14de-4795-a3a1-fcfb87677146",
   "metadata": {},
   "source": [
    "Um die Modelgüte noch genauer/sicherer bestimmen zu können wird nun auch nochmal Cross-Validation implementiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d313a2-eb86-46a9-8fee-135675f3e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:15:54.631271: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-18 17:15:54.631334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6975 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-18 17:15:57.788859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-18 17:15:58.399173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f98340174e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-18 17:15:58.399198: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-18 17:15:58.403675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-18 17:15:58.494379: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0294854640960693, Test accuracy: 0.8097863793373108\n",
      "Test loss: 1.0344839096069336, Test accuracy: 0.8380427360534668\n",
      "Test loss: 0.9089829325675964, Test accuracy: 0.8311509490013123\n",
      "Test loss: 1.2372223138809204, Test accuracy: 0.8234483003616333\n",
      "Test loss: 0.6858605742454529, Test accuracy: 0.8820689916610718\n",
      "Average Model Performance: Loss: 0.9792070388793945; Accuracy: 0.8368994712829589\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.1162 - accuracy: 0.8556\n",
      "Validation Performance: Loss: 1.1161731481552124; Accuracy: 0.8555678129196167\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 32\n",
      "Test loss: 0.830808699131012, Test accuracy: 0.8621640205383301\n",
      "Test loss: 1.3367847204208374, Test accuracy: 0.8022053837776184\n",
      "Test loss: 1.354880452156067, Test accuracy: 0.7195038199424744\n",
      "Test loss: 0.6900981068611145, Test accuracy: 0.869655191898346\n",
      "Test loss: 0.6111257076263428, Test accuracy: 0.882758617401123\n",
      "Average Model Performance: Loss: 0.9647395372390747; Accuracy: 0.8272574067115783\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7989 - accuracy: 0.8545\n",
      "Validation Performance: Loss: 0.7988851070404053; Accuracy: 0.8544652462005615\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 64\n",
      "Test loss: 0.7935425043106079, Test accuracy: 0.8738800883293152\n",
      "Test loss: 0.622887134552002, Test accuracy: 0.8538938760757446\n",
      "Test loss: 0.7861618399620056, Test accuracy: 0.835975170135498\n",
      "Test loss: 0.7767242789268494, Test accuracy: 0.8489655256271362\n",
      "Test loss: 0.7035800218582153, Test accuracy: 0.8627586364746094\n",
      "Average Model Performance: Loss: 0.736579155921936; Accuracy: 0.8550946593284607\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8555 - accuracy: 0.8379\n",
      "Validation Performance: Loss: 0.8554630875587463; Accuracy: 0.8379272222518921\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 128\n",
      "Test loss: 0.615596354007721, Test accuracy: 0.8628532290458679\n",
      "Test loss: 0.5737914443016052, Test accuracy: 0.8628532290458679\n",
      "Test loss: 0.6249785423278809, Test accuracy: 0.8318400979042053\n",
      "Test loss: 0.5884149074554443, Test accuracy: 0.8717241287231445\n",
      "Test loss: 0.5233653783798218, Test accuracy: 0.8627586364746094\n",
      "Average Model Performance: Loss: 0.5852293252944947; Accuracy: 0.858405864238739\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.7172 - accuracy: 0.8374\n",
      "Validation Performance: Loss: 0.7172396779060364; Accuracy: 0.8373759388923645\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 16\n",
      "Epoch 46: early stopping\n",
      "Test loss: 0.9694482088088989, Test accuracy: 0.8380427360534668\n",
      "Test loss: 0.8893648386001587, Test accuracy: 0.8304617404937744\n",
      "Epoch 49: early stopping\n",
      "Test loss: 1.0365078449249268, Test accuracy: 0.7711922526359558\n",
      "Test loss: 0.7920944094657898, Test accuracy: 0.834482729434967\n",
      "Epoch 42: early stopping\n",
      "Test loss: 0.9082629084587097, Test accuracy: 0.8331034779548645\n",
      "Average Model Performance: Loss: 0.9191356420516967; Accuracy: 0.8214565873146057\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.8585 - accuracy: 0.8308\n",
      "Validation Performance: Loss: 0.8585129976272583; Accuracy: 0.8307607769966125\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 32\n",
      "Test loss: 0.7735440135002136, Test accuracy: 0.8456237316131592\n",
      "Test loss: 0.7464228868484497, Test accuracy: 0.8470020890235901\n",
      "Test loss: 0.7661433219909668, Test accuracy: 0.8449345231056213\n",
      "Test loss: 0.8424381017684937, Test accuracy: 0.834482729434967\n",
      "Test loss: 0.7469531893730164, Test accuracy: 0.8275862336158752\n",
      "Average Model Performance: Loss: 0.775100302696228; Accuracy: 0.8399258613586426\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8034 - accuracy: 0.8131\n",
      "Validation Performance: Loss: 0.803392767906189; Accuracy: 0.8131201863288879\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Test loss: 0.706816554069519, Test accuracy: 0.8428670167922974\n",
      "Test loss: 0.6959714889526367, Test accuracy: 0.8518263101577759\n",
      "Test loss: 0.7800528407096863, Test accuracy: 0.8201240301132202\n",
      "Epoch 50: early stopping\n",
      "Test loss: 0.5747197866439819, Test accuracy: 0.8413792848587036\n",
      "Test loss: 0.7568275928497314, Test accuracy: 0.8482758402824402\n",
      "Average Model Performance: Loss: 0.702877652645111; Accuracy: 0.8408944964408874\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8393 - accuracy: 0.8330\n",
      "Validation Performance: Loss: 0.8393208384513855; Accuracy: 0.8329658508300781\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 128\n",
      "Test loss: 0.6117620468139648, Test accuracy: 0.8421778082847595\n",
      "Test loss: 0.7150920033454895, Test accuracy: 0.8125430941581726\n",
      "Test loss: 0.5910828709602356, Test accuracy: 0.8311509490013123\n",
      "Test loss: 0.5395610332489014, Test accuracy: 0.8448275923728943\n",
      "Test loss: 0.5306683778762817, Test accuracy: 0.8496551513671875\n",
      "Average Model Performance: Loss: 0.5976332664489746; Accuracy: 0.8360709190368653\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6044 - accuracy: 0.8434\n",
      "Validation Performance: Loss: 0.6043817400932312; Accuracy: 0.8434399366378784\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def create_model(opti_str):# Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), padding='same', input_shape=(256,128,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    for x in range(layer):\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(16, (3,3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    optimizer=RMSprop(learning_rate=0.001)\n",
    "    if opti_str == 'ADAM':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(3,'RMSP'),(2,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            lss = []\n",
    "            acc = []\n",
    "            # Training with cross-validation\n",
    "            for train, test in kfold.split(X_train, y_train):\n",
    "                model = create_model(opti_str)\n",
    "                early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "                model.fit(X_train[train], y_train[train], epochs=50, batch_size=pow(2,4+bs), verbose=0, callbacks=[early_stopping])\n",
    "                score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "                lss.append(score[0])\n",
    "                acc.append(score[1])\n",
    "                print(f'Test loss: {score[0]}, Test accuracy: {score[1]}')\n",
    "            print(f'Average Model Performance: Loss: {sum(lss)/len(lss)}; Accuracy: {sum(acc)/len(acc)}')\n",
    "            \n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print(f'Validation Performance: Loss: {loss}; Accuracy: {accuracy}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d1c12-efe4-4afd-8a48-fefa4ba5ae37",
   "metadata": {},
   "source": [
    "Die Cross-Validation scheint die Modelperformance zu stabilisieren, die Performance der Modelle wird allerdings nicht wirklich besser eher im Gegenteil. Da das Early-Stopping sogut wie nie ausgelöst wurde kann die Performance möglicherweise noch etwas verbessert werden, indem man die Anzahl der Epochen erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872d74d4-c84d-4888-916b-a2a1d58d03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 21:10:49.274521: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-18 21:10:49.274585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6861 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-18 21:10:52.417875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-18 21:10:53.034174: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa6f002a880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-18 21:10:53.034201: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-18 21:10:53.038744: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-18 21:10:53.128059: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: early stopping\n",
      "Test loss: 0.8855775594711304, Test accuracy: 0.8649207353591919\n",
      "Epoch 40: early stopping\n",
      "Test loss: 0.8479244709014893, Test accuracy: 0.8538938760757446\n",
      "Epoch 46: early stopping\n",
      "Test loss: 0.9602503180503845, Test accuracy: 0.8421778082847595\n",
      "Epoch 43: early stopping\n",
      "Test loss: 0.9598527550697327, Test accuracy: 0.860689640045166\n",
      "Epoch 45: early stopping\n",
      "Test loss: 0.6669691801071167, Test accuracy: 0.8703448176383972\n",
      "Average Model Performance: Loss: 0.8641148567199707; Accuracy: 0.8584053754806519\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.8859 - accuracy: 0.8423\n",
      "Validation Performance: Loss: 0.885921061038971; Accuracy: 0.8423373699188232\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 32\n",
      "Epoch 62: early stopping\n",
      "Test loss: 0.709524393081665, Test accuracy: 0.8669883012771606\n",
      "Epoch 56: early stopping\n",
      "Test loss: 0.9891059398651123, Test accuracy: 0.835286021232605\n",
      "Epoch 41: early stopping\n",
      "Test loss: 0.768091082572937, Test accuracy: 0.8676774501800537\n",
      "Epoch 43: early stopping\n",
      "Test loss: 0.8309871554374695, Test accuracy: 0.834482729434967\n",
      "Epoch 51: early stopping\n",
      "Test loss: 0.6792445778846741, Test accuracy: 0.8303448557853699\n",
      "Average Model Performance: Loss: 0.7953906297683716; Accuracy: 0.8469558715820312\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8827 - accuracy: 0.8159\n",
      "Validation Performance: Loss: 0.8827264308929443; Accuracy: 0.8158765435218811\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 64\n",
      "Epoch 47: early stopping\n",
      "Test loss: 0.7819654941558838, Test accuracy: 0.8442453742027283\n",
      "Epoch 55: early stopping\n",
      "Test loss: 0.8955883383750916, Test accuracy: 0.8242591023445129\n",
      "Epoch 51: early stopping\n",
      "Test loss: 1.1297160387039185, Test accuracy: 0.7463818192481995\n",
      "Epoch 73: early stopping\n",
      "Test loss: 0.7468001842498779, Test accuracy: 0.8620689511299133\n",
      "Epoch 57: early stopping\n",
      "Test loss: 0.7316828966140747, Test accuracy: 0.8468965291976929\n",
      "Average Model Performance: Loss: 0.8571505904197693; Accuracy: 0.8247703552246094\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.9326 - accuracy: 0.8241\n",
      "Validation Performance: Loss: 0.9325690865516663; Accuracy: 0.8241455554962158\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 4; Optimizer: RMSP; Batchsize: 128\n",
      "Epoch 66: early stopping\n",
      "Test loss: 0.602371871471405, Test accuracy: 0.8683666586875916\n",
      "Epoch 62: early stopping\n",
      "Test loss: 0.8381494879722595, Test accuracy: 0.8235699534416199\n",
      "Epoch 84: early stopping\n",
      "Test loss: 0.7617252469062805, Test accuracy: 0.8435561656951904\n",
      "Epoch 49: early stopping\n",
      "Test loss: 0.6014977097511292, Test accuracy: 0.8703448176383972\n",
      "Epoch 67: early stopping\n",
      "Test loss: 0.6068209409713745, Test accuracy: 0.8668965697288513\n",
      "Average Model Performance: Loss: 0.6821130514144897; Accuracy: 0.8545468330383301\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.8318 - accuracy: 0.8473\n",
      "Validation Performance: Loss: 0.8318095207214355; Accuracy: 0.847298800945282\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 16\n",
      "Epoch 33: early stopping\n",
      "Test loss: 0.7273536324501038, Test accuracy: 0.8511371612548828\n",
      "Epoch 26: early stopping\n",
      "Test loss: 0.6513396501541138, Test accuracy: 0.8456237316131592\n",
      "Epoch 26: early stopping\n",
      "Test loss: 0.617594838142395, Test accuracy: 0.837353527545929\n",
      "Epoch 33: early stopping\n",
      "Test loss: 0.8948155641555786, Test accuracy: 0.8337931036949158\n",
      "Epoch 30: early stopping\n",
      "Test loss: 0.7063039541244507, Test accuracy: 0.834482729434967\n",
      "Average Model Performance: Loss: 0.7194815278053284; Accuracy: 0.8404780507087708\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6848 - accuracy: 0.8352\n",
      "Validation Performance: Loss: 0.6847780346870422; Accuracy: 0.8351708650588989\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 32\n",
      "Epoch 42: early stopping\n",
      "Test loss: 0.8723212480545044, Test accuracy: 0.835286021232605\n",
      "Epoch 39: early stopping\n",
      "Test loss: 0.7463904619216919, Test accuracy: 0.8407994508743286\n",
      "Epoch 37: early stopping\n",
      "Test loss: 0.6905166506767273, Test accuracy: 0.8118538856506348\n",
      "Epoch 37: early stopping\n",
      "Test loss: 0.601331353187561, Test accuracy: 0.8517241477966309\n",
      "Epoch 43: early stopping\n",
      "Test loss: 0.6867132186889648, Test accuracy: 0.8531034588813782\n",
      "Average Model Performance: Loss: 0.7194545865058899; Accuracy: 0.8385533928871155\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7146 - accuracy: 0.8506\n",
      "Validation Performance: Loss: 0.7145940065383911; Accuracy: 0.850606381893158\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Epoch 38: early stopping\n",
      "Test loss: 0.7060779333114624, Test accuracy: 0.8449345231056213\n",
      "Epoch 43: early stopping\n",
      "Test loss: 0.9305328130722046, Test accuracy: 0.7787732481956482\n",
      "Epoch 40: early stopping\n",
      "Test loss: 0.5511060953140259, Test accuracy: 0.8525155186653137\n",
      "Epoch 45: early stopping\n",
      "Test loss: 0.6397911906242371, Test accuracy: 0.838620662689209\n",
      "Epoch 39: early stopping\n",
      "Test loss: 0.6343820095062256, Test accuracy: 0.8524137735366821\n",
      "Average Model Performance: Loss: 0.6923780083656311; Accuracy: 0.8334515452384949\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6966 - accuracy: 0.8346\n",
      "Validation Performance: Loss: 0.69659024477005; Accuracy: 0.8346196413040161\n",
      "57/57 [==============================] - 0s 6ms/step\n",
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 128\n",
      "Epoch 47: early stopping\n",
      "Test loss: 0.6601154208183289, Test accuracy: 0.8277050256729126\n",
      "Epoch 51: early stopping\n",
      "Test loss: 0.7453633546829224, Test accuracy: 0.8097863793373108\n",
      "Epoch 53: early stopping\n",
      "Test loss: 0.6071366667747498, Test accuracy: 0.8435561656951904\n",
      "Epoch 51: early stopping\n",
      "Test loss: 0.5707390308380127, Test accuracy: 0.8331034779548645\n",
      "Epoch 54: early stopping\n",
      "Test loss: 0.5554189682006836, Test accuracy: 0.8496551513671875\n",
      "Average Model Performance: Loss: 0.6277546882629395; Accuracy: 0.8327612400054931\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6274 - accuracy: 0.8357\n",
      "Validation Performance: Loss: 0.627370297908783; Accuracy: 0.8357221484184265\n",
      "57/57 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def create_model(layer,opti_str):# Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), padding='same', input_shape=(256,128,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    for x in range(layer):\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(16, (3,3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    optimizer=RMSprop(learning_rate=0.001)\n",
    "    if opti_str == 'ADAM':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(3,'RMSP'),(2,'ADAM')]:\n",
    "        for bs in range(4):\n",
    "            title_appendix = f'CNN_layer={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}'\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            lss = []\n",
    "            acc = []\n",
    "            # Training with cross-validation\n",
    "            for train, test in kfold.split(X_train, y_train):\n",
    "                model = create_model(layer,opti_str)\n",
    "                early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "                model.fit(X_train[train], y_train[train], epochs=100, batch_size=pow(2,4+bs), verbose=0, callbacks=[early_stopping])\n",
    "                score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "                lss.append(score[0])\n",
    "                acc.append(score[1])\n",
    "                print(f'Test loss: {score[0]}, Test accuracy: {score[1]}')\n",
    "            print(f'Average Model Performance: Loss: {sum(lss)/len(lss)}; Accuracy: {sum(acc)/len(acc)}')\n",
    "            \n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print(f'Validation Performance: Loss: {loss}; Accuracy: {accuracy}')\n",
    "\n",
    "            generate_confusion_matrix(X_test, y_test, title_appendix)\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250706aa-ba17-420c-aa4f-63396a49555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 3; Optimizer: ADAM; Batchsize: 64\n",
      "Epoch 36: early stopping\n",
      "Test loss: 0.7482515573501587, Test accuracy: 0.8283942341804504\n",
      "Epoch 45: early stopping\n",
      "Test loss: 1.1618828773498535, Test accuracy: 0.7856650352478027\n",
      "Epoch 43: early stopping\n",
      "Test loss: 0.6247854828834534, Test accuracy: 0.8463128805160522\n",
      "Epoch 38: early stopping\n",
      "Test loss: 0.7957702875137329, Test accuracy: 0.817241370677948\n",
      "Epoch 42: early stopping\n",
      "Test loss: 0.6851301789283752, Test accuracy: 0.8455172181129456\n",
      "Average Model Performance: Loss: 0.8031640768051147; Accuracy: 0.8246261477470398\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6974 - accuracy: 0.8451\n",
      "Validation Performance: Loss: 0.6973914504051208; Accuracy: 0.8450937271118164\n",
      "57/57 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAGwCAYAAAAAFKcNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC190lEQVR4nOydd1zU9f/An8eQcQKKTPfMlRMNB2iOcqOWmKVllrPcIyUHriTFxPymmQs1tdTUQjNHjhQVRVDIvRUHICIgqKy73x/8Ortk3fE57j7yfvZ4Px7d+/P5PD+vz7jzzXsq1Gq1GoFAIBAIBAIDYWbsAAQCgUAgELzaiMKGQCAQCAQCgyIKGwKBQCAQCAyKKGwIBAKBQCAwKKKwIRAIBAKBwKCIwoZAIBAIBAKDIgobAoFAIBAIDIoobAgEAoFAIDAoFsYOwBA8P/qj5M7SHaZI7jQEZgqF5E6VmPdNUIKwNJf+Z7GqnavkzuvJ9yX1yeV7bojn8+zZbcmd/yUz4YYkHkun6pJ4ihtRsyEQCAQCgcCgvJI1GwKBQCAQmBSqbGNHYFREYUMgEAgEAkOjVhk7AqMimlEEAoFAIDA0KpU0SUeOHDlCjx49KF++PAqFgl9//VVru1qtZsaMGbi7u2NjY0PHjh25evWq1j6JiYn0798fe3t7ypQpw6effkpqaqpOcZSIwkba83QW/LyPzl8s4Y0RX/NRwFrO3XzRuerPiEsMW7SRNmO+odHguVy6E6vXeUYMH8i1K2GkplzneOhOmjdrXOTYpXR6eXmyY3swt26eJiP9Lj4+nYocnyHilJNTDjEKp3TOiRM/IzQ0hPj489y+HcGWLSuoVUu3DnvNWjRh2Y/f8Ff071yMP0WHLm21tr/V7U1WbVnCiUv7uRh/ijqv19I5TkN910vC83nVSEtLo1GjRixdujTX7QsWLGDJkiUsX76ckydPolQq6dSpE8+fP9fs079/f86fP8/+/fvZtWsXR44cYejQoTrFUSIKGzPX/s6JCzf4anBPfpk5lJb1qjFs0UbiHqcA8Cwjgya1KjH23fZ6n8PX14eFgf7MmbuI5p6diYq+wO7fN+LsXM5knEqlLdHRFxgzZpreMRVHnHJxyiFG4ZTW6e3tyfLl62nbthfduw/AwsKSXbt+xNbWptAOG1trLp+/ypwpgXlstyHyZBTfzPlOrxjBMN/1kvJ8DIVarZIkpaenk5KSopXS09PzPG+XLl2YO3cuvXv3ziUmNYsXL2batGn07NmThg0bsn79eu7fv6+pAbl48SJ79uxh1apVeHp64uXlxf/+9z9+/vln7t8v/IgohVotk/FOOvDvoa/PMzJpNXIBi0f2pU3DF38h9Ju9Cq8GNRjZu50m715CEl2nfMfmGYOpU9lNy1nQ0NfjoTsJPx3FmLE5X26FQsGtG+EsXRbMgsDcS5QFoY+zsENfM9Lv0sf3U0JC9ha4b0FD4kzl2ovbKYcYhVN3py5DK52cHImJOUPHjr4cO3Yqz/3yGvp6Mf4UIwdO4sAff720rXwldw5E/Ebv9v25dO7qS9sLO/S1sN91uXzPDfF8imPoa8bdvyXxzFu1jVmzZmnl+fv7M3PmzAKPVSgU7Nixg169egFw48YNatSowZkzZ2jcuLFmv7Zt29K4cWO+/fZb1qxZw4QJE3j8+LFme1ZWFtbW1mzdujXXQkxuGLVmIyEhgQULFtC7d29atmxJy5Yt6d27N4GBgTx8+FCSc2SrVGSr1FhZar+gVqUsOHM1RpJzWFpa0rRpQw4cPKrJU6vVHDgYSosWHibjNARyuXapnXKIUTgN/x2yt7cD4PHjJEl8pop4PqaDn58fycnJWsnPz08vV2xsTpcBV1ftwrCrq6tmW2xsLC4uLlrbLSwscHR01OxTGIxW2AgPD+e1115jyZIlODg40KZNG9q0aYODgwNLliyhTp06nD59ukBPrlVKGZma7UprKxrVqMiKnUeJT3pCtkrFrhN/E339Hg+TdevgkhdOTo5YWFgQH5eglR8f/xA3V2eTcRoCuVy71E45xCichv0OKRQKAgP9OX48nAsXrhTZZ8qI5yMBapUkycrKCnt7e61kZWVl7KsrEKMNfR01ahS+vr4sX74cxX+q/tVqNcOHD2fUqFGcOHEiX09AQMBLVUpTP+7FtE/e0Xz+6lMf/Nfu4q2J32JupqBOZXc6v1Gfi7cfSHdBAoGgRLF48Rzq13+NDh36GDsUQS6Y3PMxwXk23NxyugvExcXh7u6uyY+Li9M0q7i5uREfH691XFZWFomJiZrjC4PRChtRUVGsXbv2pYIG5JRIx40bR5MmTQr0+Pn5MX78eK08dfgvWp8ruTiy5ouPeJqeQdqzdJzL2DFp+XYqOpct2kX8PwkJiWRlZeHi6qSV7+LiTGycfs1BhnAaArlcu9ROOcQonIb7DgUFzaZr1w507NiXe/f0G70mJ8TzeTWpVq0abm5uHDhwQFO4SElJ4eTJk4wYMQKAli1bkpSUREREBB4eOc1bBw8eRKVS4enpWehzGa0Zxc3NjVOn8u6wc+rUqZfakXIj1yqlUpa57mtrVQrnMnakpD3jxPnrvNn4Nb3j/zeZmZlERkbTvp2XJk+hUNC+nRdhYREm4zQEcrl2qZ1yiFE4DfMdCgqajY9PJzp3fp/bt6Xp92XqiOcjARI1o+hKamoqZ8+e5ezZswDcvHmTs2fPcufOHRQKBWPHjmXu3LmEhITw999/89FHH1G+fHlNJ9K6devSuXNnhgwZwqlTpzh27BgjR46kX79+lC9fvtBxGK1mY+LEiQwdOpSIiAg6dOigKVjExcVx4MABVq5cycKFCyU517Fz1wGo4upITPxjgn45QFV3J3q2bgRAcuozHiQm8zAppw/HrdhHADg5lMbJoXShzhH07UqCVwcRERlNePgZRo8aglJpw9p1m/WOW2qnUmlLzRpVNZ+rVq1Eo4b1SHycREyM/os6yeHaDeGUQ4zCKa1z8eK5vPeeD76+Q0hNTcP1//sWJCen8Px53sMP/42t0obK1SpqPlesXJ46r9ci+XEKD+7F4VDGHveKrrj8v7tajSoAJMQnkhD/qFDnMMR3vaQ8H4Ohx4RcUnD69GnatXsx6vKfloCBAweydu1avvjiC9LS0hg6dChJSUl4eXmxZ88erK2tNcds3LiRkSNH0qFDB8zMzHj33XdZsmSJTnEYdejr5s2bCQoKIiIiguzsnPYsc3NzPDw8GD9+PH379tXL+99VX/eGX2DJ9oPEPX6Cg9KGDk3rMKr3m9jZ5tzM345FMSN450ue4T28GdEzZ8Kdwqz6+tmIj5kwfgRubs5ERZ1n7LgZnAo/o9c16OvMb+hrmzYt+XP/1pfy16/fwuAh43M5IofCrAZpCtduDKccYhRO3Zz5Da3Ma4jkkCET2LDhl1y3gfbQ1+atmrL+1+Uv7bPj5118OXo2vd7rRsD//F/a/l3gSpYGrtR8zm/oqz7fdbl8zw3xfIpl6OuNvGvydaFU9Tck8RQ3JjHPRmZmJgkJOT2SnZycsLTMvRmksIgl5qVFLktPCwRSIJaYN23kusR8+vUwSTxWNVpI4iluTGIhNktLS62esAKBQCAQvFIYqRnFVDCJwoZAIBAIBK80YtVXgUAgEAgEAsMhajYEAoFAIDA0JjipV3FiEh1EpcaiVAXJnbEdakrudDtwTXKnQCAQCHQjK+Oewc+RfvGQJB6ruu0K3skEEc0oAoFAIBAIDIpoRhEIBAKBwNCI0SgCgUAgEAgMihiNUjIZMXwg166EkZpyneOhO2nerLFOx5uVc6L0pKk4bg6h3K/7KLMsGItatTXbbft/TJkV6ym3Yw+OW3ZhP+8bLGrXLfY4hdOwTjnEKJwl0ymHGOXkFBSNElnY8PX1YWGgP3PmLqK5Z2eioi+w+/eNODuXK9TxitKlcfjmO8jKJmX6Fzwe9hFpq5aiSn2i2Sf73l3Sln3L4xGDSJ44ElVcLPZfLUTh4FBscQqnYZ1yiFE4S6ZTDjHKySkJKpU0SaaUyNEox0N3En46ijFjpwE5Kw3euhHO0mXBLAhcmusx/x6NYjtoKJb1GpA8aVShY1LY2lJu2x8k+40j82wkUPBoFH3iLAjhlM4phxiFs2Q65RCjKTmLYzTK86jdknisG3WVxFPclLiaDUtLS5o2bciBg0c1eWq1mgMHQ2nRwqNQjlItWpN19RJ2X87C8adfKfPdKqw6d8/7AAsLrLv0QJX6hKwb14stTuE0nFMOMQpnyXTKIUY5OQXSIPvCRnp6OikpKVopv8oaJydHLCwsiI9L0MqPj3+I2/8vR1wQ5m7uWHfrSfa9uyRPm8Sz33+j9PDRWHXspLWf5RstKbf9D8r9th/rXr6kTJ2IOiW5UOeQIk7hNJxTDjEKZ8l0yiFGOTklQ62SJskUky5sxMTE8Mknn+S7T0BAAA4ODlpJrXqS7zFFRmFG1rWrPF23kuzrV0n/YyfP9+zCumtPrd0yo87w+PPBJE/4nMyIU9j5zUThUMawsQkEAoHA9CjhfTZMurCRmJjIunXr8t3Hz8+P5ORkraQws8tz/4SERLKysnBxddLKd3FxJjbuYaHiUiU+IvvOLa287JjbmDm7aO+Y/hzVg3tkXbpA6uIFkJ2NdaduhTqHFHEKp+GccohROEumUw4xyskpGaJmw3iEhITkmw4dKnh6VysrK+zt7bWSQqHIc//MzEwiI6Np385Lk6dQKGjfzouwsIhCxZ154RzmFStr5ZlXqIgqPi7/A80UKCwtC3cOCeIUTsM55RCjcJZMpxxilJNTIA1GndSrV69eKBSKfPtY5Fdw0Jegb1cSvDqIiMhowsPPMHrUEJRKG9au21yo45//uhWHb5Zi894A0o8cwrJ2Xay79CB1ycKcHaysse33IRknj6FKfITC3gGbHr0xK+dE+tHDxRancBrWKYcYhbNkOuUQo5ycklDCF2IzamHD3d2dZcuW0bNnz1y3nz17Fg8P6XsQb90agrOTIzNnTMTNzZmoqPN06z6A+PiEgg8Gsq5cImXONJQfD8X2g4/Ijo0l9YfvSD/0Z84OKhXmlSpj17ETZg4OqFJSyLpyieRJo19qfjFknMJpWKccYhTOkumUQ4xyckqCjJtApMCo82z4+PjQuHFjZs+enev2qKgomjRpgkrHTjFi1VeBQCAQFJZimWfj1FZJPNZv+EriKW6MWrMxadIk0tLS8txes2bNQvXbEAgEAoHApJHxSBIpMGphw9vbO9/tSqWStm3bFlM0AoFAIBAYiBLejGLSQ18FAoFAIBDIH7HEvEAgEAgEhkY0owgKgyE6c8ql06ldKRvJnWmZzyV3NipXXVJf1KMbkvoMhcoAfbxdlWUkdz58Wrip+nXBENcuF6wtSknqe56VIakPpI8RDBNnsVDCCxuiGUUgEAgEAoFBETUbAoFAIBAYGLVaTOolEAgEAoHAkJTwZhRR2BAIBAKBwNCIoa8lkxHDB3LtShipKdc5HrqT5s0aG91pVs6J0pOm4rg5hHK/7qPMsmAsatXWbLft/zFlVqyn3I49OG7Zhf28b7CoXbfY48yPseOH8Tj1GvPmT9Xb4eXlyY7twdy6eZqM9Lv4+HTS2dHEsxGL1gWwO3I74feP0LazV577Tvl6AuH3j/D+YN1m5pMizuJwgrTPPCxqH/cen38pfRU4rUgxyuHa5eIcPKQ/YSf/4H5sNPdjozlwaBtvvV30OYvkEKPUcQqkoUQWNnx9fVgY6M+cuYto7tmZqOgL7P59I87O5YzmVJQujcM330FWNinTv+DxsI9IW7UUVeoTzT7Z9+6StuxbHo8YRPLEkajiYrH/aiEKB4diizM/mjRtwMef9OPc3xeL5FEqbYmOvsCYMfr/42Vja82V89dZ8GVQvvu92dmbBh71iH+g+/LTUsRZHE6pn3nX9u/RuHZbTerX61MAdv26t0hxyuHa5eK8dy+WGTPm493ahzZePTny1wk2b1lB3bq1XukYDRGnZKhU0iSZYtS1UQxFQWujHA/dSfjpKMaMzflRUygU3LoRztJlwSwIXKrXOfVx/nvoq+2goVjWa0DypFGFPqfC1pZy2/4g2W8cmWcjgYKHvuoTZ2GGviqVthwO/Y2J4/yZOPlz/o6+wJeTv8pz/8IOfc1Iv0sf308JCSn4H7K8hr6G3z/CxE++5K89oVr5zm5OBO9azugPJhL043x+XvkLP616sX6BLkNfdYlTamdBwz/1eea6DH2dNW8KHTq1xcujS7776TL01ZjXXhCm4tR1WOmdu2eYNjWA9eu25Lq9oCGlphCjoeIsjrVRnv25XBKPTcfhkniKmxJXs2FpaUnTpg05cPCoJk+tVnPgYCgtWui3wqwUzlItWpN19RJ2X87C8adfKfPdKqw6d8/7AAsLrLv0QJX6hKwb14stzrwIXDSTfXsP89fh40XyFBcKhYJZS6ax4fufuXHllrHDMRiGfOb/+N/p253NG7cX2SU1pvpdLw7nvzEzM6NPn+4olTacOhn5ysZYHHEK9MfoHUSfPXtGREQEjo6O1KtXT2vb8+fP2bJlCx999FGex6enp5Oenq6Vp1arUSgUue7v5OSIhYUF8XHayw3Hxz+kTu0ael2DFE5zN3fMu/Xk2fatPN28AYvX6lB6+GjIyiT9zxd/3Vm+0RL7KTPAyhpV4iNSpk5EnVK4vxgNce0A7/TpRqPG9WnfprfejuJm4OcfkJ2dzc+rfzF2KAbFUM/8Hzp3a4+9gx1bNv1aZJfUmOp3vTicAPXr1+bAoW1YW1uRmvqU9/sN59Il/Sb9k0OMhoxTEmTcBCIFRq3ZuHLlCnXr1qVNmzY0aNCAtm3b8uDBA8325ORkBg0alK8jICAABwcHraRWPcn3GJNEYUbWtas8XbeS7OtXSf9jJ8/37MK6a0+t3TKjzvD488EkT/iczIhT2PnNROFQxjgxAxUquBOwYDpDPxlPero8Zvar0+A1+g3uw6yx84wdiuzpN+BdDv0ZSlys7n1eBIblypUbtGrRjTfb9mbVyg2sWLGQOnWkn7W4KMghRslQq6RJMsWohY3Jkyfz+uuvEx8fz+XLl7Gzs6N169bcuXOn0A4/Pz+Sk5O1ksLMLs/9ExISycrKwsXVSSvfxcWZ2Dj9fjClcKoSH5F955ZWXnbMbcycXbR3TH+O6sE9si5dIHXxAsjOxrpTt2KL8780alIfFxcnDh/7jYdJl3iYdAkvb0+GjRjIw6RLmJmZXktdE89GlHUqy87wrZy4c5ATdw5SvpI7Y/w/47eTm40dnqQY4pn/Q4VK7ni/2YJN602zdshUv+vF4QTIzMzkxo3bnD1zjpn+gfz990U++zz/P97kHKMh4xQUHaP+S3D8+HECAgJwcnKiZs2a7Ny5k06dOuHt7c2NG4XrnGdlZYW9vb1WyqsJBXJe7sjIaNq3ezEUUqFQ0L6dF2FhEXpdhxTOzAvnMK9YWSvPvEJFVPFx+R9opkBhaVlscf6XI4dP0OqNLrRp1UOTIiOi2bo5hDateqAywarD3dv28kGHQQx461NNin/wkA3f/8zoDyYaOzxJMcQz/4f3PuhNwsNEDuw7UtQwDYKpfteLw5kbZmZmlCql31olcogRii9OvSjho1GM2mfj2bNnWFi8CEGhUPD9998zcuRI2rZty6ZNmwxy3qBvVxK8OoiIyGjCw88wetQQlEob1q7T/6/aojqf/7oVh2+WYvPeANKPHMKydl2su/QgdcnCnB2srLHt9yEZJ4+hSnyEwt4Bmx69MSvnRPrRw8UW539JTU3j4oWrWnlPnz4jMfHxS/mFRam0pWaNqprPVatWolHDeiQ+TiIm5n6hHDa2NlSq9mJUUvlK7rxWvybJSSnE3Ysn+XGK1v5ZWVk8ik/k9vWYYo2zOJyGeN8VCgXv9e/N1p9/IztbmmmY5XLtcnDOnDWJ/fv+IibmHnZ2pfHt64N3mxb09Bn4SsdoiDglQ8YFBSkwamGjTp06nD59mrp1tSem+u677wDw8fExyHm3bg3B2cmRmTMm4ubmTFTUebp1H0B8fELBBxvImXXlEilzpqH8eCi2H3xEdmwsqT98R/qhP3N2UKkwr1QZu46dMHNwQJWSQtaVSyRPGv1S84sh4ywOPDwa8ef+F0NQFwbOBGD9+i0MHjK+UI66jWrzw7Ylms/jZ+UMKd61+Q9mjQswmTiLw2mIZ+79ZksqVirP5g3SjUKRy7XLwensUo4Vq77Bzc2ZlOQnnDt3iZ4+Azl0MLTgg2UcoyHiFEiDUefZCAgI4OjRo+zevTvX7Z999hnLly/XuSq+oHk2TAWxxLy0iCXmpUMsMW/6iCXmpaNY5tnYtUgSj013/QrexsaofTb8/PzyLGgALFu2zCTb/AUCgUAg0AnRZ0MgEAgEAoFBkfGwVSkwvXGJAoFAIBAIXileyZoNQ7Q3x6UlSe40RP+KtNNrJHcqm30iudMQnEko3LTtgoIxxPsuF8pYK40dQqFIep5m7BAKxBD9K2SLjJtApOCVLGwIBAKBQGBSiGYUgUAgEAgEAsMhajYEAoFAIDA0ohlFIBAIBAKBQSnhhY0S14wSFrWPe4/Pv5S+CpxWZPeI4QO5diWM1JTrHA/dSfNmjY3qTHv2nPnB2+g0wp/mH0zgw6mLOHfttmb7tO820NB3tFYaPndZsccpZ6ccYhRO6Zxf+I0iIeWKVjpxek+R4jOEE0z/XsrNKSgaJa6w0bX9ezSu3VaT+vX6FIBdv+4tktfX14eFgf7MmbuI5p6diYq+wO7fN+LsXM5ozpnf/0RY9GW+GvUh276ZQstGdRg6eylxj5I0+7RuXJeDK+Zq0oKxHxd7nHJ1yiFG4ZTeefHCFerVbKVJ3d5+X2+XoZxyuZdycUqCWi1NkiklrrCR+OgxD+MTNKljpze5eeMOJ46FF8k7bswQVq3exLr1W7h48SqffT6Fp0+fMejjfkZxPk/P4M+TUYwb0JNm9WpS2d2Zz/p2pZKbE1v2vVh7oJSlBU5l7TXJvrRtscYpZ6ccYhRO6Z1ZWdnExydoUmLiY71dhnLK5V7KxSkJJXwG0RJX2Pg3lpaWvNO3O5s3Fm0xKUtLS5o2bciBg0c1eWq1mgMHQ2nRwsMozmyVimyVilKltLvlWJcqxZlLL9b8OH3+Gm0//ZIeo+cyZ8Vmkp7oNnbfFK+9OJxyiFE4pXcCVK9RhXOXj3I66gDLVy2kQkV3vV2GcMrlXsrFKZAGoxc2Ll68SHBwMJcuXQLg0qVLjBgxgk8++YSDBw8WeHx6ejopKSlaSV3I8cydu7XH3sGOLZt+Lcol4OTkiIWFBfFx2qsKxsc/xM3V2ShOpY01jV6ryopf9hKfmEx2topdR8KJunKTh/+/vHrrJnWZO3IAK2eMZNwAHyIuXOOzr74nO7vwpWdTvPbicMohRuGU3hlxOopRI6bQ953BTBrvT+UqFdm1ZxOlS+s/EZjUTrncS7k4JaOE12wYdTTKnj176NmzJ6VLl+bp06fs2LGDjz76iEaNGqFSqXj77bfZt28f7du3z9MREBDArFmztPJKWzlhb+NS4Pn7DXiXQ3+GEhf7sMjXYorMG/UhM5ZtouOw6ZibmVG3WkW6eHlw4UYMAF1avyjpv1alPK9VKU/XkbMJv3CVFg1qGytsgcBkObD/iOb/L5y/TMTpKM6eO0zP3l3Y+OMvJuMUmCBiUi/jMXv2bCZNmsSjR48IDg7mgw8+YMiQIezfv58DBw4wadIkvv7663wdfn5+JCcnayU7a6cCz12hkjveb7Zg0/qif5kTEhLJysrCxVX7vC4uzsTG6VeQkcJZyc2Z4NljCPsxkH3LZ7Hp64lkZWVT0SX3jlIVXZ0oa6ckJjYh1+2GilOOTjnEKJzSO/9LSvITrl+/RbXqVSTxSeGUy72Ui1MySnjNhlELG+fPn+fjjz8GoG/fvjx58oQ+ffpotvfv35/o6Oh8HVZWVtjb22slhaLgy3rvg94kPEzkwL4jBe5bEJmZmURGRtO+nZcmT6FQ0L6dF2FhEUZ32lpb4VzWgZTUpxyPukS75g1y3S/20WOSUp/iVMbeKHHKySmHGIVTeud/USptqVqtEnFx8ZL4pHDK5V7KxSmQBqNP6qVQKAAwMzPD2toaBwcHzTY7OzuSk5MNcs73+vdm68+/kZ2dLYkz6NuVBK8OIiIymvDwM4weNQSl0oa16zYbzXns7EXUajVVy7sSE/uQRT/+RtUKLvRs14Knz9L5fusfdGzRCKcy9sTEJRD0429UdnOideM6xRqnXJ1yiFE4pXXOmjuZvX8cJCbmPm5uLkz+cjTZ2Sq2b92ld4yGcMrhXsrJKQkyHrYqBUYtbFStWpWrV69So0YNAE6cOEHlypU12+/cuYO7e9F7ev8X7zdbUrFSeTZvKNoolH+zdWsIzk6OzJwxETc3Z6KiztOt+wDi4wvfJCG1M/XpM77dtJO4R0k4lFbS0bMRo97vjqWFOdnZ2Vy9c5+Qv07xJO0ZLo4OtGxYh5H9ulLK0rJY45SrUw4xCqe0zvIV3FixZhFlHcvyKCGRk2ERdO7gy6NH+g9VNYRTDvdSTk5JkHETiBQo1GrjFbeWL19OpUqV6NatW67bv/zyS+Lj41m1apVO3gpl60sRnhZyWXK7JC8xLxBIgVhivuSRlXHP4Od4FvyFJB6bQQsk8RQ3Rq3ZGD58eL7b582bV0yRCAQCgUBgQEp4zYbR+2wIBAKBQPDKI4a+CgQCgUAgeNXIzs5m+vTpVKtWDRsbG2rUqMGcOXP4d+8JtVrNjBkzcHd3x8bGho4dO3L16lXJYxGFDYFAIBAIDIxapZYk6cL8+fP5/vvv+e6777h48SLz589nwYIF/O9//9Pss2DBApYsWcLy5cs5efIkSqWSTp068fz5c0mv36gdRA2FRakKxg7hleLJ7umSO+26zpHcafb/w6ilQvXqfTWMitTPB8QzEkhDcXQQfbp8jCQe2+HfFnrf7t274+rqyurVqzV57777LjY2NmzYsAG1Wk358uWZMGECEydOBCA5ORlXV1fWrl1Lv37SLV4najYEAoFAIJAJua0Hlp6enuu+rVq14sCBA1y5cgWAqKgoQkND6dKlCwA3b94kNjaWjh07ao5xcHDA09OTEydOSBq3KGwIBAKBQGBo1CpJUkBAAA4ODlopICAg11NOmTKFfv36UadOHSwtLWnSpAljx46lf//+AMTGxgLg6uqqdZyrq6tmm1SI0SgCgUAgEBgaHftb5IWfnx/jx4/XyrOyssp13y1btrBx40Y2bdpE/fr1OXv2LGPHjqV8+fIMHDhQkngKiyhsCAQCgUBgaCSaZ8PKyirPwsV/mTRpkqZ2A6BBgwbcvn2bgIAABg4ciJubGwBxcXFas3XHxcXRuHFjSeL9hxLbjDJi+ECuXQkjNeU6x0N30rxZY+EsgGyViqU7j9N1xmo8xy6hu/8aVvwRpjWM6lFKGtPX7+WtL1fQYuz/+Oy77dyO12/aZSmv3cvLkx3bg7l18zQZ6Xfx8emkt8tQMZZ0p3hG0jrlEKOcnHLk6dOnmJlp/zNvbm6O6v8LPtWqVcPNzY0DBw5otqekpHDy5ElatmwpaSwlsrDh6+vDwkB/5sxdRHPPzkRFX2D37xtxds596XXhzCF432m2Ho1iSt92bJ8+kDE9vVi7/zQ/HT4L5IzXHrdiJ/cSkgka5sPPfv1xd7Rn+JJtPEvPLLY4c0OptCU6+gJjxkzT6/jiiLGkO8Uzks4phxjl5JQEIywx36NHD7766it+//13bt26xY4dO1i0aBG9e/cGchYlHTt2LHPnziUkJIS///6bjz76iPLly9OrVy9JL9/khr6q1WrNSrD6UtDQ1+OhOwk/HcWYsTk/agqFgls3wlm6LJgFgUv1Ouer7Pxn6Ouo73+lnJ0tMwe8rdk2YeVOrCwtmPdxF27HPabn7LX8MvVDapZ3AkClUtPB7wdG+bTmndYvlrYvaOirPnEWdmhlRvpd+vh+SkjI3nz3K2hYpak8H7k4dRn6Kp5R0ZxyiNGUnMUy9HXxMEk8tmN/KPS+T548Yfr06ezYsYP4+HjKly/P+++/z4wZMyhVqhSQ82+uv78/K1asICkpCS8vL5YtW8Zrr70mSbz/YHI1G1ZWVly8eNFgfktLS5o2bciBg0c1eWq1mgMHQ2nRwkM486FRtfKcvBzD7bicZpHLdx9y5vp9WterCkBGVjYAVpYvugKZmSkoZWHOmev3iy3O4sAUn4+cnYZALtcutVMOMcrJKWfs7OxYvHgxt2/f5tmzZ1y/fp25c+dqChqQUxibPXs2sbGxPH/+nD///FPyggYYsYPof3vT/kN2djZff/015crlVHktWrQoX096evpLY4zzqx1xcnLEwsKC+Djt5Ybj4x9Sp3aNwoZfIp2fvN2ctOfp9JqzFnOFGdlqFSN7tKbbG3UBqOpWFveydiz5LZTpH3TEppQlGw5GEpeUSkJK4VeoNMS1S40pPh85Ow2BXK5daqccYpSTUzLEQmzGYfHixTRq1IgyZcpo5avVai5evIhSqSxUc0pAQACzZs3SylOYlUZhbi9luAJgX+QVdodfIuDjrtRwL8flu/EEbvsLZwclPi3qY2luzjdDezBzw37aTPoeczMFnrUra2o+BAKBoMQi0dBXuWK0wsa8efNYsWIF33zzDe3bt9fkW1pasnbtWurVq1coT25jjsuWq5Pn/gkJiWRlZeHi6qSV7+LiTGzcQx2uoOQ5g3YcYdDbzencrDYAtSo48SDxCWv2hePToj4A9Sq7suXLATx5lk5mVjaOdrYMWPAT9aq45qeWNM7iwBSfj5ydhkAu1y61Uw4xyskpkAaj9dmYMmUKmzdvZsSIEUycOJHMTN1GK/yDlZUV9vb2Wim/GpHMzEwiI6Np385Lk6dQKGjfzouwsAi9YigpzueZWS918jMzU+TaSc/OxgpHO1tuxz/mwp043mxY+CpMQ1y71Jji85Gz0xDI5dqldsohRjk5JUOiGUTlilEn9WrevDkRERF8/vnnNGvWjI0bNxZ5JEphCPp2JcGrg4iIjCY8/AyjRw1BqbRh7brNwpkPbV6vzqq9p3BztMtpRol5yIaDkfRsWV+zz77IK5QtbYO7ox1X7z1iwS+HadeoBq3qVim2OHNDqbSlZo2qms9Vq1aiUcN6JD5OIiam8J1XDRljSXeKZySdUw4xyskpCaIZxbiULl2adevW8fPPP9OxY0eys7MNfs6tW0NwdnJk5oyJuLk5ExV1nm7dBxAfn1DwwSXYOaVvO5buOk7AzwdJTH2Ks0Np3vVqwLAuLTT7JCSn8c22v3j05CnO9kq6e9ZjaBfPYo0zNzw8GvHn/q2azwsDZwKwfv0WBg/JvbNyccdY0p3iGUnnlEOMcnIKio5JzbNx9+5dIiIi6NixI0qlUm+PWGJeWsQS8wIpEEvMC0yV4phnIy1AmrVIlH7rJPEUN0av2fg3FStWpGLFisYOQyAQCAQCaRHNKAKBQCAQCAyKjDt3SoHJzSAqEAgEAoHg1eKVrNmQS9uwpbn0tz8zO0typyH6VyT7d5DcWWV+mKS+lPSnkvqgZPcxkMu1G+L3Q2lpLbnzScYzyZ0CAyKaUQQCgUAgEBiUEj5duWhGEQgEAoFAYFBEzYZAIBAIBIZGNKMIBAKBQCAwKGI0SsnDy8uTHduDuXXzNBnpd/Hx6SSJd8TwgVy7EkZqynWOh+6kebPGersmTvyM0NAQ4uPPc/t2BFu2rKBWreomF2dRnTaff4Ny6vqXUqlOH2n2MatQE+v+U7CdtBLbiT9g/eGXYGGpU3xf+I0iIeWKVjpxeo9Ojv8ih/dIOKV1GuqZ/8PY8cN4nHqNefOnFtll6vdSbk5B0SiRhQ2l0pbo6AuMGTNNMqevrw8LA/2ZM3cRzT07ExV9gd2/b8TZuZxePm9vT5YvX0/btr3o3n0AFhaW7Nr1I7a2NiYVZ1Gdz4Jn8nTxKE16tnE+AFkXTwH/X9DoN5HsG+d4FjyTZ2v8yTz9J+gxsuHihSvUq9lKk7q9/b7Ojn8jh/dIOKV1GuKZ/0OTpg34+JN+nPv7YpFdcriXcnJKgkotTZIpJjVduVSUsir8LKQZ6Xfp4/spISF7892voGF7x0N3En46ijFjc36EFAoFt26Es3RZMAsCl+Z6jC5DX52cHImJOUPHjr4cO3Yqz/0KGvqqT5wFoY8zr6Gvpd7qj3nNxjz7fhIA1h/PIPvmeTL/2lZgHPkNff3CbxRdunWknVfPAj3/oMvQV2O+RwUhnLo5Czv0tbDPHAoe+qpU2nI49DcmjvNn4uTP+Tv6Al9O/irfY/Ib+moq91IuzuKYrjzV711JPKUDCv4tNEVKZM2G1FhaWtK0aUMOHDyqyVOr1Rw4GEqLFh6SnMPe3g6Ax4+T9HYYIk5JnWbmWLzeiqyoIzmfbe0wr1ATdVoK1gOnYzvmf1gP+BKziq/pFWv1GlU4d/kop6MOsHzVQipUdNfLYyhM/vkIp8EIXDSTfXsP89fh40V2yeVeysUpkAaTKmykpaURHBzM1KlT+e6773j06FGBx6Snp5OSkqKViruyxsnJEQsLC+LjtFcVjI9/iJurc5H9CoWCwEB/jh8P58KFK3p7DBGnlE7z2h5gbUtWdM4PhVkZFwBKefcm68xhnv+8EFXsLaz7T0ZR1lUnd8TpKEaNmELfdwYzabw/latUZNeeTZQurf+Cf1Jj6s9HOA3DO3260ahxfWb7B0rik8u9lItTMkp4M4pRR6PUq1eP0NBQHB0diYmJoU2bNjx+/JjXXnuN69evM2fOHMLCwqhWrVqejoCAAGbNmqWVZ2Zmh7mFvaHDLzYWL55D/fqv0aFDH2OHYlAsGrUl+3o06tSknIz/r87OPHNQUwDJiLuNedV6WDRqQ+bhrXmYXubA/iOa/79w/jIRp6M4e+4wPXt3YeOPv0h2DQKBLlSo4E7Agum802Mg6ekZxg5HYEhkXFCQAqPWbFy6dImsrJw+Bn5+fpQvX57bt29z6tQpbt++TcOGDZk6Nf9e2X5+fiQnJ2slM3O74ghfQ0JCIllZWbi4Omnlu7g4Exv3sEjuoKDZdO3agU6d3ufevdgiuQwRp1ROhX05zKvVJ+vsX5q8fwodqoT7WvuqHj3AzKFonb1Skp9w/fotqlWvUiSPlJjy8xFOw9CoSX1cXJw4fOw3HiZd4mHSJby8PRk2YiAPky5hZqb7T7Rc7qVcnJKhVkmTZIrJNKOcOHGCmTNn4uDgAEDp0qWZNWsWoaGh+R5nZWWFvb29VlIYYG2D/MjMzCQyMpr27bw0eQqFgvbtvAgLi9DbGxQ0Gx+fTnTu/D63b8eYZJxSOS0atUH9NIXsq2c1eerkBFRPEjErp923QuHohio5gaKgVNpStVol4uLii+SRElN+PsJpGI4cPkGrN7rQplUPTYqMiGbr5hDatOqBSo8pruVyL+XiFEiD0Sf1+qdg8Pz5c9zdtf9RqVChAg8fSl8aVSptqVmjquZz1aqVaNSwHomPk4iJuZ/3gfkQ9O1KglcHEREZTXj4GUaPGoJSacPadZv18i1ePJf33vPB13cIqalpuP5/e2NycgrPn6fr5TREnNI4FVg08iYrOvSlknvmiT8o1aY3qrg7qOJuY9HQG7Ny7qRv+59OMc6aO5m9fxwkJuY+bm4uTP5yNNnZKrZv3aWT59/I4T0STmmdUj/z1NQ0Ll64qpX39OkzEhMfv5SvC3K4l3JySkIJb0YxemGjQ4cOWFhYkJKSwuXLl3n99dc1227fvk25ctKPjfbwaMSf+1+09y8MnAnA+vVbGDxkvF7OrVtDcHZyZOaMibi5ORMVdZ5u3QcQH6/fX+DDhn0IwP79W7TyhwyZwIYN+vcxkDpOKZzm1epj5uD0YhTKv8gK34vCwpJSb32Awro0qvg7PN+0AHWSbjUS5Su4sWLNIso6luVRQiInwyLo3MGXR48e6+T5N3J4j4RTWqchnrkhkMO9lJNTCtQlvLBh1Hk2/tuxs0WLFnTq9GJGvkmTJnH37l1++uknnby6zLNRWEryEvOGQCwxLzBVxBLzJY/imGfjydgeknjsFu+UxFPcGLVmw9/fP9/tgYHSDAUTCAQCgcColPCaDaM3owgEAoFA8MqjR2ffVwmTGY0iEAgEAoHg1UTUbAgEAoFAYGhEM8qrh1w64WWrsiV3GqLTqblC+gow16+OFryTjjzcNk5Sn13P+ZL6wDCdDw3xvrsqy0jujEtLktwpF0pqZ05ri1KSO59nyXSm1RJe2BDNKAKBQCAQCAzKK1mzIRAIBAKBKWHEWSZMAlHYEAgEAoHA0IhmlJLJiOEDuXYljNSU6xwP3UnzZo1Nzunl5cmO7cHcunmajPS7+Ph0KvigfJg48TNCQ0OIjz/P7dsRbNmyglq1qhfJOXhIf8JO/sH92Gjux0Zz4NA23nq7rVGd2SoVS3efpOvs9XhOWk73OT+yYm+41l8WT9MzCPjlCG/7r8Vz0nLeCdjE1mPndI7V1J+5IeIMi9rHvcfnX0pfBU4zqTgN4TTU85E6TkP4pHYa4rfDEHFKRglfYr5EFjZ8fX1YGOjPnLmLaO7ZmajoC+z+fSPOzvpPjW4Ip1JpS3T0BcaMKfqPOIC3tyfLl6+nbdtedO8+AAsLS3bt+hFbWxu9nffuxTJjxny8W/vQxqsnR/46weYtK6hbt5bRnMEHItl67BxT3m3D9ikfMKZHS9YePMNPR6I1+yz89RjHL93mqwFvsX3KB3zQthFfbzvC4XM3Cx2nHJ65IeLs2v49Gtduq0n9en0KwK5f95pUnIZwGuL5GCJOOdxLQ/x2GCJOgTQYdbpyQ2FRqkK+24+H7iT8dBRjxub8YCgUCm7dCGfpsmAWBC7V65z6OHUZmZCRfpc+vp8SEpL/D7q5mXmhnU5OjsTEnKFjR1+OHTuVt1PH0Sh37p5h2tQA1q/bUvDOEjr/GY0yasUuytnZMvP99pptE9b8gZWlBfM+fAuAd7/+iU5NajK0U3PNPu8v3ELrupUZ2a0FUPBoFFN55gWNRtEnTl1Go8yaN4UOndri5dEl3/0KGo0it+9lYZ8PGOYZFadPX6euo1EK8z0vaDSKPnEWx3TlyYM6SuJxCP5TEk9xU+JqNiwtLWnatCEHDr4YeqlWqzlwMJQWLTxMxlkc2NvbAfD4cZIkPjMzM/r06Y5SacOpk5FGczaq5sbJK3e5HZ8EwOV7CZy58YDWdStr7XP43C3iklJRq9WEX73L7YdJtKxTOQ+rNnJ55oaO09LSknf6dmfzxu1F9pTU76XUccrxXkr122HSz7yEN6OUuA6iTk6OWFhYEB+nvQJgfPxD6tSuYTJOQ6NQKAgM9Of48XAuXLhSJFf9+rU5cGgb1tZWpKY+5f1+w7l06ZrRnJ908CDteSa9AjZirjAjW61iZNcWdGtWW7PPlHfbMHvzITrNXIeFmRkKBcx4rx0eNcoX6hxyeeaGjrNzt/bYO9ixZdOvRfKU5O+l1HHK6V5K/dshl2deEjFqYSMyMpKyZctSrVo1AH788UeWL1/OnTt3qFKlCiNHjqRfv375OtLT00lPT9fKU6vVKAwwedKrxOLFc6hf/zU6dOhTZNeVKzdo1aIb9g529OrVhRUrFtK5U78i/WgUxbnv7DV2R1wh4MO3qeHmyOV7CQTuOIqzgxKfN+oA8NORaP6+Fce3g7vi7mhH5PX7BGw7grODkha1K+kdd0mj34B3OfRnKHGxD40dikCGGOK3w2Qp2UujGLcZZdCgQVy/fh2AVatWMWzYMJo1a8bUqVNp3rw5Q4YMYc2aNfk6AgICcHBw0Epq1ZM8909ISCQrKwsXVyetfBcXZ2Lj9PvBNITTkAQFzaZr1w506vQ+9+7FFtmXmZnJjRu3OXvmHDP9A/n774t89vkgozmDQo4zqENTOjetRa3y5ejevDYD3mzMmj8jAHiekcX/fg9jQq/WtH29Gq+Vd6Kfd0M6NanJ+kNnC3UOuTxzQ8ZZoZI73m+2YNP6X4rkgZL9vZQ6TjndS6l/O0z5matVakmSXDFqYePq1avUqpXT83jZsmV8++23fPvttwwfPpygoCB++OEHvvnmm3wdfn5+JCcnayWFmV2e+2dmZhIZGU37dl6aPIVCQft2XoSFReh1HYZwGoqgoNn4+HSic+f3uX07xiDnMDMzo1Qpaacp1sX5PCPzpU5+ZgqFppNelkpFVrYq330KQi7P3JBxvvdBbxIeJnJg35Gihlmiv5dSxynne1nU3w65PPOSiFGbUWxtbUlISKBKlSrcu3ePN954Q2u7p6cnN2/mPxTRysoKKysrrbyCmlCCvl1J8OogIiKjCQ8/w+hRQ1AqbVi7brN+F2Igp1JpS80aVTWfq1atRKOG9Uh8nERMzH2dfYsXz+W993zw9R1Camoarq7OACQnp/D8eXoBR+fOzFmT2L/vL2Ji7mFnVxrfvj54t2lBT5+BevmkcLapX41V+0/jVra0phllw+Gz9PSsC0Bp61J41ChPUMhxrCwtKO9ox+lr99h1+jITenoVYH+BHJ65oeJUKBS81783W3/+jexsadb4kcP30hDPxxBxyuFeGuK3wxBxSoaMayWkwKiFjS5duvD999+zatUq2rZtyy+//EKjRo0027ds2ULNmjUlP+/WrSE4Ozkyc8ZE3NyciYo6T7fuA4iPTyj44GJ0eng04s/9WzWfFwbOBGD9+i0MHjJeZ9+wYR8CsH+/9rCyIUMmsGGDflXhzi7lWLHqG9zcnElJfsK5c5fo6TOQQwdD9fJJ4ZzyrjdLd58k4Je/SEx9hrO9kndb1WfYv4a5zh/4Nkt2hfHlhv2kPH2Oe1k7RnZtgW/r+oWOUw7P3FBxer/ZkoqVyrN5Q9FGoRg6Tqmdhng+hohTDvfSEL8dhohTMkp4nw2jzrNx//59WrduTeXKlWnWrBnff/89Hh4e1K1bl8uXLxMWFsaOHTvo2rWrTt6C5tkwFQyxAqgu82wU2mmAVV8NgVj1VTpK8qqvcnlGckAuq74WxzwbSe+1k8RTZvMhSTzFjVH/FSlfvjxnzpyhZcuW7NmzB7VazalTp9i3bx8VK1bk2LFjOhc0BAKBQCAwNUp6B1Gjz7NRpkwZvv76a77++mtjhyIQCAQCgWEo4c0oRi9sCAQCgUDwqiPnWgkpkEdjvEAgEAgEAtkiajaMiCE6jamysyR3ZkpuBLtS+q80m6dT4g6dD3sUbfXJ3HDdJY+ZEeXSmdMQlNTOnIbAEJ05DdGBt1gQzSgCgUAgEAgMibqEFzZEM4pAIBAIBAKDImo2BAKBQCAwNCW8ZkMUNgQCgUAgMDCiGaWEMmL4QK5dCSM15TrHQ3fSvFlj4TQxJ8DY8cN4nHqNefOnSuIrSpwKRydsR0/FYe1vlNm0F/tFazCvUVuz3XbkFMpuO6yVSk9boFN8Xl6e7NgezK2bp8lIv4uPTyedjs8LuTxz4ZTOKYcYDeE01HdIUDRKZGHD19eHhYH+zJm7iOaenYmKvsDu3zfi7FxOOE3ECdCkaQM+/qQf5/6+WCSPFHEqlKWx++o7yM4ide5kUsYO5Om6ZahTn2jtlxl5kqRP39GktKDZOsWoVNoSHX2BMWOm6XRcfsjlmQundE45xGgopyG+Q5KgkijJFKOujWIoClob5XjoTsJPRzFmbM7LqFAouHUjnKXLglkQuFSvcwqnbs6Chr4qlbYcDv2NieP8mTj5c/6OvsCXk7/K95gnGc8kjfPfQ19tBgzFvPbrpE4fnaffduQUFMrSpM3P+0dOl6GvGel36eP7KSEhe/Pdr6ChmqbyzIWz+JxyiFFfpy5DXwv7HcpIv1v4oPXk4VttJfE47/9LEk9xU+JqNiwtLWnatCEHDh7V5KnVag4cDKVFCw/hNAEnQOCimezbe5i/Dh/X2/FvihqnZbNWZF+/jHLCTBzW7MAucCWlOnZ7aT+L+o1xWLMD+yXrsR06DkVpe0ni1xe5PHPhlM4phxgN5TRl1CppklwxamFj1KhRHD16tOAd8yE9PZ2UlBStlF9ljZOTIxYWFsTHaS83HB//EDdXZ71iEE5pne/06UajxvWZ7R+o1/G5UdQ4zVzLY9WpJ9kP7pI6ZxLp+37D9pPRlHrzRXtw5plTPF0yjyczx/Nswwos6jWi9LT5YGa8r5lcnrlwSueUQ4yGcgpMF6MWNpYuXcqbb77Ja6+9xvz584mNjdXZERAQgIODg1ZSq54UfKDAJKlQwZ2ABdMZ+sl40tOln31QbxQKsm9c4fmmVWTfvEbG/l2k/7kLq7d9NLtkHjtI5unjqO7cJPNUKKkBfljUqotF/cbGi1sgEJgEombDyOzbt4+uXbuycOFCKleuTM+ePdm1axcqVeHuqp+fH8nJyVpJYWaX5/4JCYlkZWXh4uqkle/i4kxs3EO9rkE4pXM2alIfFxcnDh/7jYdJl3iYdAkvb0+GjRjIw6RLmOlZS1DUOFVJj8i+e1srL/vebcycXPI+Ju4BquQkzNzy70NkSOTwzIVTWqccYjSU06RRK6RJMsXohY0GDRqwePFi7t+/z4YNG0hPT6dXr15UqlSJqVOncu1a/h3qrKyssLe310qKfDoQZWZmEhkZTft2Xpo8hUJB+3ZehIVF6HUNwimd88jhE7R6owttWvXQpMiIaLZuDqFNqx6FLoRKHWf2pXOYla+klWfuXgnVw7g8j1E4OqOws0f9+JFeMUuBHJ65cErrlEOMhnIKTBdJJvVKSkqiTJkyRXJYWlrSt29f+vbty507d1izZg1r167l66+/Jjs7W4owNQR9u5Lg1UFEREYTHn6G0aOGoFTasHbdZuE0sjM1NY2LF65q5T19+ozExMcv5RdnnM93bsVu3lKs3+lPxvHDmNesg9Vb3Xm6/JucHaxtsOk7kIwTR1AnJWLmVh6bD4ehir1H5tnwQseoVNpSs0ZVzeeqVSvRqGE9Eh8nERNzX9dLBkz/mQun9E45xGgopyG+Q1JgrCaQe/fuMXnyZP744w+ePn1KzZo1CQ4OplmzZjlxqdX4+/uzcuVKkpKSaN26Nd9//z21akm7EKXOhY358+dTtWpV3nvvPQD69u3Ltm3bcHNzY/fu3TRq1KjIQVWuXJmZM2fi7+/Pn3/+WWTff9m6NQRnJ0dmzpiIm5szUVHn6dZ9APHxCQUfLJzF4jQERYkz+/pl0hZMx6b/EKx9B6KKf8DT4O/IOPr/76cqG/Mq1Sn9ZicUtqVRPX5EVlQ4z35aA1mFXzfXw6MRf+7fqvm8MHAmAOvXb2HwkPE6Xe8/yOWZC6d0TjnEaCinIb5DUqBWFX8TyOPHj2ndujXt2rXjjz/+wNnZmatXr1K2bFnNPgsWLGDJkiWsW7eOatWqMX36dDp16sSFCxewtraWLBad59moVq0aGzdupFWrVuzfv5++ffuyefNmtmzZwp07d9i3b59OrtOnT1OuXNEmf/ovBc2zITA+hlhivqB5NnRFLkvMiyXRBSUJQywxXxzzbDzwaieJx/HAHtLT07XyrKyssLKyemnfKVOmcOzYsTxHfarVasqXL8+ECROYOHEiAMnJybi6urJ27Vr69esnScygR5+N2NhYKlXKabvetWsXffv25e233+aLL74gPLzw1cUAN2/elLygIRAIBAKBqSHVaJTcRmAGBATkes6QkBCaNWuGr68vLi4uNGnShJUrV2q237x5k9jYWDp27KjJc3BwwNPTkxMnTkh6/ToXNsqWLUtMTAwAe/bs0QSpVqsl71shEAgEAsGrgFqtkCTlNgLTz88v13PeuHFD0/9i7969jBgxgtGjR7Nu3ToAzXQTrq6uWse5urrqNRVFfujcZ+Odd97hgw8+oFatWjx69IguXboAcObMGWrWrClpcAKBQCAQCF6QV5NJbqhUKpo1a8a8efMAaNKkCefOnWP58uUMHDjQkGG+hM41G0FBQYwcOZJ69eqxf/9+SpcuDcCDBw/47LPPJA9QIBAIBAK5Y4xJvdzd3alXr55WXt26dblz5w4Abm5uAMTFaQ/hj4uL02yTCp1rNiwtLTUdSf7NuHHjJAlIUDKQujOnIXDeWbShtrmRPLm15E6H+cckdwoEpopcO0QbYzRK69atuXz5slbelStXqFKlCpAzSMPNzY0DBw7QuHFjAFJSUjh58iQjRoyQNJZCFTZCQkIKLfTx8Sl4J4FAIBAIShDGKCONGzeOVq1aMW/ePPr27cupU6dYsWIFK1asAHImURs7dixz586lVq1amqGv5cuXp1evXpLGUqjCRmFPqlAoRCdRgUAgEAhMgObNm7Njxw78/PyYPXs21apVY/HixfTv31+zzxdffEFaWhpDhw4lKSkJLy8v9uzZI+kcG6DHPBtyQMyzITBVRDOKQGB6ZGXcM/g5bjftWPBOhaBKpPQTXRYHRVob5fnz51LFUeyMGD6Qa1fCSE25zvHQnTRv1lg4hbNYfTbjv0M5Z8tLqVT3T1/a1+pDP5RztmBet3mxxymc8nTKIUY5OYuKWqWQJMkVnQsb2dnZzJkzhwoVKlC6dGlu3LgBwPTp01m9erXkARoCX18fFgb6M2fuIpp7diYq+gK7f9+Is7P+E4wJZ8lzFtX3bLkfT+cP0aRnwXMAyDqnPZmORctugP4VkHK4l8JpWu+mcAqkRudmlNmzZ7Nu3Tpmz57NkCFDOHfuHNWrV2fz5s0sXrxY8lnH9KGgZpTjoTsJPx3FmLHTgJy+JrduhLN0WTALApfqdU7hLHlOfXz5NaOU6jIQ89oePFs8WpNn5lYFqwFTeL58CraTV/J8UyDZF7Vn6i2oGUUO91I4jf9ulmRncTSj3Gz0liSealH7JfEUNzrXbKxfv54VK1bQv39/zM3NNfmNGjXi0qVLkgZnCCwtLWnatCEHDr6YK16tVnPgYCgtWngIp3AaJ0ZzcywaeZMVeehfJymFle8YMnatRp2arLvTEHEKp8k75RCjnJxSIZpRdOTevXu5zhSqUqnIzCz86pZSkZ6eTkpKilbKr7LGyckRCwsL4uO0VxWMj3+Im6uzXjEIZ8lzSu0zr/sGWCvJOnNYk1eqy0Cy71wm+9JpnX2GilM4Td8phxjl5BRIg86FjXr16uW6gtwvv/xCkyZNdA7gu+++46OPPuLnn38G4Mcff6RevXrUqVOHL7/8kqysrHyPz21RGrXqic5xCATGxKJpO7KvnkX95DEA5nU8MK/+Ohl/rDVuYAKBQBKkWhtFrug8g+iMGTMYOHAg9+7dQ6VSsX37di5fvsz69evZtWuXTq65c+eyYMEC3n77bcaNG8ft27cJDAxk3LhxmJmZERQUhKWlJbNmzcrT4efnx/jx47Xyypark+f+CQmJZGVl4eLqpJXv4uJMbNxDneIXzpLrlNKncHDCvEZD0n9aqMkzr/Y6irKu2H65Vmtfq34TUN2+yPM1eX8nDBWncMrDKYcY5eSUCl2nGn/V0Llmo2fPnuzcuZM///wTpVLJjBkzuHjxIjt37uStt3TrALN27VrWrl3LL7/8wp49e5g6dSrffvstU6dOxc/Pjx9++IFNmzbl67CyssLe3l4rKRR5l/4yMzOJjIymfTsvTZ5CoaB9Oy/CwiJ0il84S65TSp9F03ao05LJvhL5wn/0V54tncSzZV9oEkDGH+tI377MKHEKpzyccohRTk6BNOhcswHg7e3N/v1F7xF7//59mjVrBuR0MDUzM9PMzw7QtGlT7t+/X+Tz/Jegb1cSvDqIiMhowsPPMHrUEJRKG9au2yycwlm8PoUCi6ZvknXmL1C9+NNHnZoMuXQKVScnoE7S7S80OdxL4TTBd1M4JUUl4yYQKdCrsAFw+vRpLl68COT04/Dw0L2nr5ubGxcuXKBy5cpcvXqV7OxsLly4QP369QE4f/48Li4u+oaYJ1u3huDs5MjMGRNxc3MmKuo83boPID4+oeCDhVM4JfSZV2+AWRln7VEoEiOHeymcpvduCqe0yLm/hRToPM/G3bt3ef/99zl27BhlypQBICkpiVatWvHzzz9TsWLFQrumT5/ODz/8QM+ePTlw4ADvvfcemzZtws/PD4VCwVdffUWfPn1YtGiRThclpisXmCpiunKBwPQojnk2Lr3WVRJPnSu7JfEUNzrXbAwePJjMzEwuXrxI7dq1Abh8+TKDBg1i8ODB7Nmzp9CuWbNmYWNjw4kTJxgyZAhTpkyhUaNGfPHFFzx9+pQePXowZ84cXUMUCAQCgUBgQuhcs2FjY8Px48dfGuYaERGBt7c3T58+lTRAfRA1GwJTRdRsCASmR3HUbFysJU3NRt2rJaRmo1KlSrlO3pWdnU358uUlCUogEAgEglcJOc/+KQU6D30NDAxk1KhRnD79YlbD06dPM2bMGBYuXJjPkQKBQCAQCEoihWpGKVu2rNbcFWlpaWRlZWFhkVMx8s//K5VKEhMTDRdtIRHNKIKSRNrFbZI7lXXfldwpEJgqxdGMcq56d0k8r9/QbfJMU6FQzSiLFy82cBgCgUAgELy6lPShr4UqbAwcONDQcQgEAoFAIHhF0XtSL4Dnz5+TkZGhlWdvb1+kgAQCgUAgeNXQbdznq4fOhY20tDQmT57Mli1bePTo0Uvbs7OzJQlMIBAIBIJXhZI+XbnOo1G++OILDh48yPfff4+VlRWrVq1i1qxZlC9fnvXr1xsiRoMwYvhArl0JIzXlOsdDd9K8WWPhFE6j+6Rwpj19xvwfNtJp4Hia9xrMhxPmcO7KDc32p8+eM2/Zejp+OJbmvQbTa5gfW34/WOxxCqdhnXKIUU5OQdHQubCxc+dOli1bxrvvvouFhQXe3t5MmzaNefPmsXHjRkPEKDm+vj4sDPRnztxFNPfsTFT0BXb/vhFn53LCKZyyj3Hmt2sIO3OOryYOZduyr2jZ5HWGfrmAuISckWKBKzdxLOJvAiYN49cfAhjQ620Cvv+RQ2GRBZiljVM4DeeUQ4xyckqBWq2QJMkVnWcQLV26tGbxtIoVK7J9+3beeOMNbt68SYMGDUhNTTVUrIWmoKGvx0N3En46ijFjpwE5SxDfuhHO0mXBLAhcqtc5hbPkOU0lxn8PfX2enkHLd4fx7YwxtHmjsSb/vdEz8PJoyKiBfeg94ks6e3sy7IOeuW6Hgoe+msq1C6d8YzQlZ3EMfY2s1LPgnQpB05jfJPEUNzrXbFSvXp2bN28CUKdOHbZs2QLk1Hj8szBbYXnw4AEzZsygffv21K1bl/r169OjRw9Wr15tsL4flpaWNG3akAMHj2ry1Go1Bw6G0qKF7ivXCmfJdJpqjNnZ2WSrVJQqZamVb12qFGcuXAWgcd2aHD55hriERNRqNaeiLnL7Xhwtm75ebHEKp+GccohRTk6pUKkVkiS5onNhY9CgQURFRQEwZcoUli5dirW1NePGjWPSpEmF9pw+fZq6deuye/duMjMzuXr1Kh4eHiiVSiZOnEibNm148uRJgZ709HRSUlK0Un6VNU5OjlhYWBAfp73ccHz8Q9xcnQsdv3CWbKepxqi0taFR3Zqs+CmE+EePyc5WsevgMaIuXeNhYhIAfiM+pHrl8rz10Tg8fD5lxPSFfPnZhzRrUKfY4hROwznlEKOcnAJp0Hk0yrhx4zT/37FjRy5dukRERAQ1a9akYcOGhfaMHTuWcePG4e/vD8CGDRv47rvvCAsL4/Hjx7Rv355p06bx7bff5usJCAhg1qxZWnkKs9IozMUQXEHJZN7EocwIWk3HD8dibmZG3ZpV6NK2BReu3QJgU8h+oi9dZ4n/WMq7lCPi3GXmLfsRF8eytGhS37jBCwSvKHLubyEFRZpnA6BKlSpUqVJF5+MiIyO1Rq988MEHfPLJJ8TFxeHq6sqCBQv4+OOPCyxs+Pn5MX78eK28suXy/gstISGRrKwsXFydtPJdXJyJjXuo83UIZ8l0mnKMldxdCV7wJU+fp5P29BnOjmWYFLCUim4uPE/PYMm6X1g8bbSmT8dr1Spz6fod1m7/o1CFDVO+duGUR4xyckqFnJtApKBQzShLliwpdCosLi4uPHjwQPM5Li6OrKwszaRgtWrVKtQ6K1ZWVtjb22ulf6/j8l8yMzOJjIymfTsvTZ5CoaB9Oy/CwiIKHb9wlmynHGK0tbbC2bEMKU/SOB55jnYtmpCVnU1WVvZL3xFzczPUKpVR4hTOkvduyskpkIZC1WwEBQUVSqZQKBg9enSh9u3VqxfDhw8nMDAQKysr5syZQ9u2bbGxsQHg8uXLVKhgmAXVgr5dSfDqICIiowkPP8PoUUNQKm1Yu26zcAqn7GM8FvE3arWaqhXdibkfx6I1m6la0Z2eb3ljaWFBswZ1WLRmM9ZWpXB3cSLi70vsPHCMiUPeL9Y4hdNwTjnEKCenFJTwCUQLV9j4Z/SJlMydO5cHDx7Qo0cPsrOzadmyJRs2bNBsVygUBAQESH5egK1bQ3B2cmTmjIm4uTkTFXWebt0HEB+fUPDBwimcJh5jatpTvl27lbiExzjYKenYuhmjBvbB8v9XaV4weQTfrt2KX+Bykp+k4e7ixKiP+tC3a/tijVM4DeeUQ4xyckpBSW9G0XmeDal5/vw5WVlZlC5dWjKnWGJeUJIQS8wLBEWjOObZOO4uzXeq1QPpv+/FQZE7iBYVa2trY4cgEAgEAoFBEaNRBAKBQCAQGJTCdb9+ddF5Ui+BQCAQCAQCXRA1GwKBQCAQGBg1ohlFZ44ePcoPP/zA9evX+eWXX6hQoQI//vgj1apVw8vLq2CBAACzfOYDMSVUBuhDbGkufTm3lkN5SX0XEu9I6jMUhujM+eSH/pI77YbJY1VogXQY4jeulLllwTuZIKoSPvZV52aUbdu20alTJ2xsbDhz5gzp6ekAJCcnM2/ePMkDFAgEAoFA7qhQSJLkis6Fjblz57J8+XJWrlyJpeWLEmbr1q2JjIyUNDiBQCAQCATyR+e67MuXL9OmTZuX8h0cHEhKSpIiJoFAIBAIXilKep8NnWs23NzcuHbt2kv5oaGhVK9eXZKgioMRwwdy7UoYqSnXOR66k+bNGpuc08vLkx3bg7l18zQZ6Xfx8elkUr5/I+W1T5z4GaGhIcTHn+f27Qi2bFlBrVq6vVseLRrzv/WB/Hk2hOjYE7Tr/KKAbGFhzthpn7Ht0AZO3jjIn2dD+Op/M3D+z+JNhUEO71FRndkqNUuPXKTr9/vxXLiT7sv3s+LYZf6ZDzAzW8XiQ+fps/ogLb7ZxVvf7WHazgjinzwr1jiFU34xSv2bNHhIf8JO/sH92Gjux0Zz4NA23nq7bZGcUqGSKMkVnQsbQ4YMYcyYMZw8eRKFQsH9+/fZuHEjEydOZMSIEToHkJGRwZYtWxg3bhzvv/8+77//PuPGjWPr1q1kZGTo7CsMvr4+LAz0Z87cRTT37ExU9AV2/74RZ+dyJuVUKm2Jjr7AmDHT9HYY0vcPUl+7t7cny5evp23bXnTvPgALC0t27foRW1ubQjtsbK25fP4q8/y+eWmbtY01dRvU5oegYN5762PGf+JH1RqVWbJ+gU5xyuU9KqozOOwqW8/cYspbDdg+uANj3qzP2pNX+SniBgDPM7O5GJfMkFa1+fnjtnzT+w1uJaYydtvJYo1TOOUVI0j/m3TvXiwzZszHu7UPbbx6cuSvE2zesoK6dWtJ4hfoj87TlavVaubNm0dAQABPnz4FclZenThxInPmzNHp5NeuXaNTp07cv38fT09PXF1dgZwVYE+ePEnFihX5448/qFmzpk7egqYrPx66k/DTUYwZm/OCKxQKbt0IZ+myYBYELtXpXEVx6tJTOyP9Ln18PyUkZK9e8RXFV9BoFH2uXZfRKE5OjsTEnKFjR1+OHTuV5355jUaJjj3BmI8nc2jPkTyPrd+4Lj/tWcPbHr2IvRcHFDwaxVTeI0M4/z0aZdTWMMoprZjZtYkmb8L2U1hZmjOvh0eux5978JgB647wx4i3cHewBQoejWIq1/4qOE0lRkP8xuk6GuXO3TNMmxrA+nVb8twn9an063/9l32u/STxvB33sySe4kbnmg2FQsHUqVNJTEzk3LlzhIWF8fDhQ50LGgAjRoygQYMGxMXFcfjwYTZv3szmzZs5fPgwcXFx1K9fn88//1xnb35YWlrStGlDDhw8qslTq9UcOBhKixa5/3AawykXiuPa7e3tAHj8OEkSX26UtiuNSqXiSfKTQu0vl/dICmejCo6cvPWQ24mpAFyOS+bM3URaV3fJ85jU9EwUgJ114f5hMNVrl6NTDjEWB2ZmZvTp0x2l0oZTJ40/eKGkN6PoPdlBqVKlqFevXpFOfuzYMU6dOoW9vf1L2+zt7ZkzZw6enp75OtLT0zXDb/9BrVajyKNE7eTkiIWFBfFx2isAxsc/pE7tGjpegeGccsHQ165QKAgM9Of48XAuXLhSZF9ulLIqxbhpn/HHjv2kpT4t1DFyeY+kcH7SshZpGZn0WnEAczMF2So1I9vWpVv9Srnun56VzbeHLtC5XkVKWxWusGGq1y5HpxxiNCT169fmwKFtWFtbkZr6lPf7DefSpZf7GQqKF50LG+3atcvzH3KAgwcPFtpVpkwZbt26xeuvv57r9lu3blGmTJl8HQEBAcyaNUsrT2FWGoX5ywUYgfxYvHgO9eu/RocOfQzit7AwZ+GKuSgUCuZO1q3PRklh38V77D5/lwAfD2o42XM5PpnAP//GubQ1Pg0qa+2bma3ii19PowamdmponIAFJZorV27QqkU37B3s6NWrCytWLKRzp35GL3DIuVZCCnQubDRu3Fjrc2ZmJmfPnuXcuXMMHDhQJ9fgwYP56KOPmD59Oh06dNDqs3HgwAHmzp3LqFGj8nX4+fkxfvx4rbyy5erkuX9CQiJZWVm4/GfkgYuLM7FxD3WK35BOuWDIaw8Kmk3Xrh3o2LEv9+7FFsmVGxYW5gSu+Ar3im4M7jOy0LUaIJ/3SApn0KHzDGpRi871KgJQy8WeB8lPWXPiqlZhI6egEc6D5Kes+KB1oWs1pIpTOOUToyHJzMzkxo3bAJw9cw4Pj4Z89vkgRo+aatS4xNBXHQkKCtJK3333HaGhoYwdO1Zrkq/CMHv2bCZPnkxgYCCNGzemfPnylC9fnsaNGxMYGMjkyZOZOXNmvg4rKyvs7e21Un41L5mZmURGRtO+3Ytp1RUKBe3beREWFqFT/IZ0ygVDXXtQ0Gx8fDrRufP73L4dI0WoWvxT0KhSvSJD+44m+XGKTsfL5T2Swvk8M/uljn5mZgqtjsP/FDTuPE5j+futKGNTqtjjFE75xFicmJmZUaqUbu+jQHokW6BiwIABvPHGGyxcuFCn4yZPnszkyZO5efMmsbE5f726ublRrVo1qUJ7iaBvVxK8OoiIyGjCw88wetQQlEob1q7bbFJOpdKWmjWqaj5XrVqJRg3rkfg4iZiY+0b3/YPU17548Vzee88HX98hpKam4erqDEBycgrPn6cXcHQONrY2VK5WUfO5QuXy1K5fi+SkFBLiEvhm1TzqNqjNyA8nYmZmRjlnx5xzJKWQlZlVqHPI5T0qqrNNTTdWnbiCm71NTjNKXBIbTl2nZ8OcWo3MbBWTdoRzMS6JJX1aoFKpSUh9DoCDTSkszQv3N40pXrtcnXKIEaT/TZo5axL79/1FTMw97OxK49vXB+82Lejpo1utuyFQleyKDekKGydOnMDa2lrv46tVq/ZSASMmJgZ/f3/WrFlT1PC02Lo1BGcnR2bOmIibmzNRUefp1n0A8fEJBR9cjE4Pj0b8uX+r5vPCwJkArF+/hcFDxudxVPH5/kHqax827EMA9u/XHqo2ZMgENmz4pVCO+o3rsGb7Ms3nL2aPAeC3zb/z/cJVmkm+fjn4o9Zxn7zzGaePnynUOeTyHhXVOeWtBiw9eomAfdEkPk3HubQ17zapyrDWtQGIf/Kcw9dy/lB4L/iw1rEr329N8yqFmyzNFK9drk45xAjS/yY5u5RjxapvcHNzJiX5CefOXaKnz0AOHQzVO0apkPO6JlKg8zwb77zzjtZntVrNgwcPOH36NNOnT8ff31+y4KKiomjatCnZ2dk6HVfQPBumglj1VVpK6qqvhkCs+iqQArms+loc82z86vaBJJ5esZsk8RQ3Ov/iOzg4aH02MzOjdu3azJ49m7ffflsnV0hISL7bb9y4oWt4AoFAIBAITAydChvZ2dkMGjSIBg0aULZs2SKfvFevXigUCvKrXMmvs6dAIBAIBHKgpA991Wk0irm5OW+//bZkq7u6u7uzfft2VCpVrkksWS8QCASCVwGVQiFJkis6D319/fXXJWve8PDwICIi72FTBdV6CAQCgUAgMH107rMxd+5czaJrHh4eKJVKre25TT2eF5MmTSItLS3P7TVr1uTQoUO6hohdqcKvDlpYnmTovlx2QRii46VcyMwu3PBSXbj0WPr5OEoqhujM+WTNx5I77T5ZK7nTEBiio6SnU21JfSceXpLUByX7N+6/lPQ7UejCxuzZs5kwYQJdu3YFwMfHR6s/xT/rkegycsTb2zvf7UqlkrZt2xbaJxAIBAKBKVLS+2wUurAxa9Yshg8frldNg0AgEAgEgpJLoQsb//SdEDUNAoFAIBDohphBVAfEMFSBQCAQCHSnpM8gqtNolNdeew1HR8d8k9wYO34Yj1OvMW9+0VcEHDF8INeuhJGacp3joTtp3qyxcJqI08vLkx3bg7l18zQZ6Xfx8elU5PikjrEkObNVapYeOkfX/+3GM2A73b/7gxVHLmiNPvv+r/P0WraXFl/vwDvwN4ZtOMLf9x4Va5zF4ZTi3Wzk2YCv185lR8Rmjt47gHen1i/tU6VmZQKC5/DHxd/Yd3UXK35fikt5F53OY+r3cvCQ/oSd/IP7sdHcj43mwKFtvPW2qI03BXQqbMyaNeulVV//m6QkLi6O2bNnS+r8N02aNuDjT/px7u+LRXb5+vqwMNCfOXMX0dyzM1HRF9j9+0acncsJpwk4lUpboqMvMGbMNL1jMnSMJckZfPwSWyNuMKVzE7aP6MSY9g1Ye+IKP4Vf0+xTxdGOKZ0b88uwtwge+CblHWwZsfEoiWmFW4xPijiLwynFu2lta8O1C9dZNHVJrtvLV3Fn6a/fcudaDKP7TODjjkNYt3gDGekZhT6HHO7lvXuxzJgxH+/WPrTx6smRv06wecsK6tatpXeMUqGWKMmVQq+NYmZmRmxsLC4uupWEi4K+a6OULV2zwH2USlsOh/7GxHH+TJz8OX9HX+DLyV/luX9BQ1+Ph+4k/HQUY8bm/GAoFApu3Qhn6bJgFgQu1Sl+4dTPWdjhhRnpd+nj+ykhIXvz3a+gYXumct1ycf576Ouon0Mpp7RmZo9mmrwJW09gZWHOvN5v5Hp8anomXgt+44cB3nhWcwUKHvpqKtcu9bsJeQ99PXrvAF9+MoOje49p8mYum0ZWVhZzR3+dp6+goa+mci+tLXRbLv7O3TNMmxrA+nVb8tynONZGWV9hgCSej+5t0PvYr7/+Gj8/P8aMGcPixYsBeP78ORMmTODnn38mPT2dTp06sWzZMlxdXSWJ9x8KXbNhiP4a0dHR+abLly9Lfs5/CFw0k317D/PX4eNFdllaWtK0aUMOHDyqyVOr1Rw4GEqLFh7CaQJOqZHLdZuqs1HFcpy8Gc/tR08AuBybxJmYBFrXdMt1/8xsFdsib1DaypLXXMsUW5zF4TQ0CoWClh08iblxl282fk1I1C/8sPO7XJta8kKO99LMzIw+fbqjVNpw6qTxZ6NWSZT0JTw8nB9++IGGDRtq5Y8bN46dO3eydetW/vrrL+7fv//SgqtSoPNoFClp3LhxnrOE/pNfUCEnPT2d9HTtatWCjnunTzcaNa5P+za99Qv8Pzg5OWJhYUF8nPZSy/HxD6lTu4ZwmoBTauRy3abq/KR1HdLSs+i1bC/mZgqyVWpGtnudbg0qa+135Mp9Jm8/yfPMbJzsrFk+wJuytlbFFmdxOA1NWacy2Ja2pf/n/Vi1IJjv563E883mzF01kzG+EzgbFl2gQ073sn792hw4tA1raytSU5/yfr/hXLp0reADX2FSU1Pp378/K1euZO7cuZr85ORkVq9ezaZNm2jfvj0AwcHB1K1bl7CwMFq0aCFZDIUubKhU0k9J4ujoyIIFC+jQoUOu28+fP0+PHj3ydQQEBDBr1iytPCvLstiUyr2zaoUK7gQsmM47PQaSrkN7pUAgkI595++y+9wdAnp7UsPZnstxSQTui8LZzhqfRlU1+zWv6sLmoW+R9DSd7Wdu8sW2MDZ80h5HpbXxgpcZCrOcCuzQvcfZsnIbANfOX+f1ZvXp+WGPQhU25MSVKzdo1aIb9g529OrVhRUrFtK5Uz+jFzik+nM9tz+wrayssLLKuxD++eef061bNzp27KhV2IiIiCAzM5OOHTtq8urUqUPlypU5ceKEpIUNnddGkRIPDw/u379PlSpVck0VKlQosEbFz8+P5ORkrWRtmfeKtI2a1MfFxYnDx37jYdIlHiZdwsvbk2EjBvIw6RJmZrrfkoSERLKysnBxddLKd3FxJjbuoc4+4ZTeKTVyuW5TdQYdiGZQq9p0fr0StVwd6N6wCgM8a7HmmHbTqU0pCyo7lqZhxXLM7NEMczMzdpy5VWxxFofT0CQnJpOVmcWtq7e18m9fvYNrhcL1wZPTvczMzOTGjducPXOOmf6B/P33RT77fJDePqlQKaRJAQEBODg4aKWAgIA8z/vzzz8TGRmZ6z6xsbGUKlWKMmXKaOW7uroSGxsr6fUbtbAxfPhwqlatmuf2ypUrExwcnK/DysoKe3t7rZRfE8qRwydo9UYX2rTqoUmREdFs3RxCm1Y99KrByczMJDIymvbtvDR5CoWC9u28CAvLe6E54Sw+p9TI5bpN1fk8M/ulTpNmCkWBnXLVajUZhewwbqrXXtxkZWZxMeoylWtU0sqvVL0isXfjCuWQ8700MzOjVCndOpWaMrn9ge3n55frvjExMYwZM4aNGzdibW3c2kCdF2KTkt698+8zUbZsWQYOHCjpOVNT07h44apW3tOnz0hMfPxSvi4EfbuS4NVBRERGEx5+htGjhqBU2rB23WbhNAGnUmlLzRpVNZ+rVq1Eo4b1SHycREzMfZOIsSQ529RyZ1XoJdwcbHOaUWKT2HDyCj3/vwnlWUYWK0Mv8uZr5XEqbU3Ssww2h18nPuUZb9WtWGxxFodTinfTxtaaCtUqaD67V3ajZv0apDx+Qvz9eH76fjOzvp9OVFg0kcfP4vlmc1q91ZLRfcYXOk453MuZsyaxf99fxMTcw86uNL59ffBu04KePtL+O6IPUnVEKKjJ5N9EREQQHx9P06ZNNXnZ2dkcOXKE7777jr1795KRkUFSUpJW7UZcXBxubrl31tYXoxY2CiImJgZ/f3/WrFlj7FAKZOvWEJydHJk5YyJubs5ERZ2nW/cBxMcnFHywcBrc6eHRiD/3b9V8Xhg4E4D167cweEjhf3ANGWNJck7p3Jilh88T8McZEtOe42xnw7tNqzOsTT0AzMwU3Ep4woToEyQ9zaCMTSnqly/Lmo/fpKaLQ7HFWRxOKd7N2o1q879fFmk+j5r5GQB/bNnLvHELOLrnGAunLGbAqPcZM3skd27EMH3ITP4OP1foOOVwL51dyrFi1Te4uTmTkvyEc+cu0dNnIIcOhuodo1QYYyG2Dh068Pfff2vlDRo0iDp16jB58mQqVaqEpaUlBw4c4N133wXg8uXL3Llzh5YtW0oaS6Hn2TAGhpxnQ1cMscS8QFqkXsZbLI8tLWKJeWmRwxLzhkDXeTYKQ3HMs/FDRWnm2Rh2V/95NgDefPNNGjdurJlnY8SIEezevZu1a9dib2/PqFGjADh+vOjTQvwbo9ZshISE5Lv9xo0bxRSJQCAQCASGQ22iS6MEBQVhZmbGu+++qzWpl9QYtbDRq1evPOfZ+Aex+JtAIBAI5I4xmlFy4/Dhw1qfra2tWbp0KUuX6jcLbGEx6mgUd3d3tm/fjkqlyjVFRhp/1jeBQCAQCARFw+jzbERE5D3EqaBaD4FAIBAI5ICxpys3NkZtRpk0aRJpaWl5bq9ZsyaHDh3S2Ss6c5ZMRIdO08YQnTmf3T9a8E46YlPeW3KnIZBLh06peZ4lz5mfS/qvk1ELG97e+X+plUolbdu2LaZoBAKBQCAwDKoS3v3QqM0oAoFAIBAIXn1MelIvgUAgEAheBeTc30IKSmzNxojhA7l2JYzUlOscD91J82aNhVM4je4TzsI7ra3N+PwLf9r59Of11l04cER7EiK1Ws13K9fzps8HeLTryeAxftyOuafZfu9BHNMDgujU52M82vWks+8gvlv1o+RxFoSXlyc7tgdz6+ZpMtLv4uPTSW+XoWIUzqJT0juImkRh4+7du6Smpr6Un5mZyZEjRyQ/n6+vDwsD/ZkzdxHNPTsTFX2B3b9vxNm5nHAK5ysV46vsNFMoqF2zOlMnfJbr9jUbt7LxlxBmTBrFppWLsbG2Ztj4aaSn53QwvHk7BrVKzYxJo/h1w3Imjx7Gll93U84x/xkqpb52pdKW6OgLjBkzTa/jiyPGku4UFB2jTlf+4MEDevbsSUREBAqFgg8++IBly5ZRunRpIGcxmPLly+s8XblFqQr5bj8eupPw01GMGZvz5VYoFNy6Ec7SZcEsCNRvYhPhLHlOOcT4qjv/GY3yeusufBswnQ5tWgE5tRrtevZnYL93GPRBHwCepKbRtsf7zJ06nq4d38zVt2bjL8z/diV37j6VNM7CTleekX6XPr6fEhKyt8B98xt9ZSrPRy7OrIx7ueZLycLK0kxXPvFO0aYrNxZGrdmYMmUKZmZmnDx5kj179nDhwgXatWvH48ePNftIXRaytLSkadOGHDj4YsicWq3mwMFQWrTwEE7hfGViLMnOu/djSXj0mJbNmmjy7EoraVivNlHn8h4ympqWRrYq798cQ1y71Mjh+cjJKRUqhTRJrhi1sPHnn3+yZMkSmjVrRseOHTl27Bju7u60b9+exMREoODpytPT00lJSdFK+RVQnJwcsbCwID5Oe1XB+PiHuLk663UdwlnynHKIsSQ7ExJz/mAp51hWK7+cY1kSHj3O7RDu3L3Ppl9CSHmSWWxxGgI5PB85OQXSYNTCRnJyMmXLvvgxsLKyYvv27VStWpV27doRHx9foCMgIAAHBwetpFY9MWTYAoHgFSPuYQLDxk/j7XbePHmSZexwBK8gooOoEalevTrR0dFaeRYWFmzdupXq1avTvXv3Ah1+fn4kJydrJYWZXZ77JyQkkpWVhYurk1a+i4szsXEP9boO4Sx5TjnEWJKdTv9fo/EoUbsW41HiY5zKadd2xD98xCejptC4QT1mTh5drHEaAjk8Hzk5pUItUZIrRi1sdOnShRUrVryU/0+Bo3HjxgX22bCyssLe3l4r5df0kpmZSWRkNO3beWnyFAoF7dt5ERaW9zot+SGcJc8phxhLsrNieTecypUlLOKsJi81LY3oC5dp9HodTV7cwwQGjZpMvdo1mfvlOMzM8v9JNMS1S40cno+cnAJpMOqkXl999RVPn+be69vCwoJt27Zx7570vYSDvl1J8OogIiKjCQ8/w+hRQ1AqbVi7brNwCucrFeOr7FQo4NKV65rP9+7HcenKdRzs7XB3c+HDvr1Yse5nqlSsQIXyrny38kdcnMrRwTtnxErcwwQGjZxMeTcXJo4czOOkZADMzRVkZ+f9R47U165U2lKzRlXN56pVK9GoYT0SHycRE3NfL6cpPJ9XySkFKlnXSxQdoxY2LCwssLe3z3P7gwcPmDVrFmvWrJH0vFu3huDs5MjMGRNxc3MmKuo83boPID4+oeCDhVM4ZRTjq+y0sjKnz6CRms8L/pdTS9qzS0e+mjaBT/r78uzZc2YuWMKT1FSaNqzP8m/mYGWVM4/GiVNnuHP3Pnfu3qdDrw81nqqVlVy/+fK8P4a6dg+PRvy5f6vm88LAmQCsX7+FwUPG6+U0hefzKjmlQM79LaTAqPNsFERUVBRNmzaVfJ4NgUDwaiCXVV8LO8+GLohVjqWjOObZmF2lvySeGbc3SuIpboxasxESEpLv9hs3bhRTJAKBQCAQCAyFUQsbvXr1QqFQ5NsJtKB5NgQCgUAgMHVKejOKUUejuLu7s337dlQqVa4pMjLSmOEJBAKBQCAJYgZRI+Lh4UFERN7DkQqq9RAIBAKBQGD6GLUZZdKkSaSlpeW5vWbNmhw6dEhnr5Nt3iNc9CXhaYrkzpKMXSkbyZ3Ps/OeZlofMrPFTJJSUsZaKbnTEJ05w1yaS+4cqUqS3Ck1pxOuGjuEVxox9NWIeHvn/0OhVCpp27ZtMUUjEAgEAoFhKNlFDSM3owgEAoFAIHj1MWrNhkAgEAgEJYGSPhpFFDYEAoFAIDAwJb3PhtGbUR49esShQ4dITEwEICEhgfnz5zN79mwuXrxokHO6ubvw3Q/zuXDjBDcfnOHQsd9o1Lh+kb0jhg/k2pUwUlOuczx0J82bNRbOIjJ2/DAep15j3vypRfJMnPgZoaEhxMef5/btCLZsWUGtWtWLHJ9c7qWpO7/wG0VCyhWtdOL0niLHWJQ4y4/vR7O7v2ql+oe/02x36v82tbfOpcnFTTS7+yvm9oXrANvYsyEL181jZ+QvhN0/TJvOXlrbpwdNIez+Ya0UtHFBsTtzw9TfI0M6BUXDqIWNU6dOUaNGDTp06EDNmjWJiIjgjTfeYPXq1axfvx4PDw/J59pwcLBn595NZGVl0b/PUNq26M7MafNJSiraaBNfXx8WBvozZ+4imnt2Jir6Art/34izcznh1JMmTRvw8Sf9OPd30Qud3t6eLF++nrZte9G9+wAsLCzZtetHbG31HxUjl3spF+fFC1eoV7OVJnV7+329XVLF+ezSbc42+ViTLvf202wzs7Yi+XAkD777RaeYbGytuXr+Ogu/XJznPicOnqRro3c0acZns4vd+V/k8h4Z6veoqIgl5o3I1KlT8fX1JTk5mS+//JJevXrRoUMHrly5wrVr1+jXrx9z5syR9Jwjxw7m3t0HjP18Kmci/+bO7Xv8deg4t2/FFMk7bswQVq3exLr1W7h48SqffT6Fp0+fMejjfsKpB0qlLStWL2LMyKlFLggC9Ow5kA0bfuHixav8/fdFhg6dQOXKFWnSpIHeTrncS7k4s7KyiY9P0KTExMd6u6SKU52tIuth0ov0+IlmW/zqncQu3U5a5BWdYjpx6BQ/LFjNX3tC89wnIyOTxIeJmvQkOe+F4Qzl/C9yeY8M4ZQClURJrhi1sBEREcH48eOxs7NjzJgx3L9/nyFDhmi2jxw5kvDwcEnP2alLO6LOnmfl2iDOXQ1l/5Ft9P/It0hOS0tLmjZtyIGDLxaFUqvVHDgYSosWHsKpB4GLZrJv72H+Onxcb0d+2NvbAfD4cZJex8vlXsrFCVC9RhXOXT7K6agDLF+1kAoV3fV2SRWnVTV3Gp5eQ4Njy6n2v3GUKu9UpJgKS9OWjdkdvYPNR9fzRcA47MsWfe6gojjl8h4Z6t2UAhVqSZJcMWoH0YyMDGxscqqxLS0tsbW1xcnpxZfZycmJR48e5etIT08nPT1dK0+tVqFQ5F6Oqly1EgM/6ccPS9fy7aIVNG7yOnPnf0lmZgZbfvpNr+twcnLEwsKC+DjtJYzj4x9Sp3YN4dSRd/p0o1Hj+rRv01uv4wtCoVAQGOjP8ePhXLig21+l/yCXeykXZ8TpKEaNmMK1qzdxdXNm0pSR7NqzCe8W3UlNzXviP0PGmXrmCs/GLeH5jXtYupSl/Lh+1N4+j/MdRqNKe65XTIXhxOFTHP7jCPfvPKBC1QqMmDKYoA3zGdLjc1Qq/f62LapTLu+RIZwCaTBqYaNSpUrcuHGDqlWrAvDzzz/j7v7ir5kHDx5oFT5yIyAggFmzZmnlKa3KUdraOdf9zcwURJ05T8CcxQCci75InXq1+GhQP70LGwLpqFDBnYAF03mnx0DS0zMMco7Fi+dQv/5rdOjQxyB+ge4c2H9E8/8Xzl8m4nQUZ88dpmfvLmz8Ubc+EVKRcuhFf7FnF2+TduYqDcJW4NjDi4Sf/zTYef/87aDm/69fusm1C9fZHvYTTVs15nSofn3YDOEU6IZ86ySkwajNKP369SM+Pl7zuVu3bpqaDshZgv6NN97I1+Hn50dycrJWUlrl3REoPi6BK5eva+VdvXyjSFW2CQmJZGVl4eKqXTBycXEmNu6hcOpAoyb1cXFx4vCx33iYdImHSZfw8vZk2IiBPEy6hJlZ0V7ZoKDZdO3agU6d3ufevVi9PXK4l3Jy/peU5Cdcv36LatWr6O2QOs7slDTSb9zHqqqb3jHpw/07D3j8KImKVSsYzSmX96g43k19EX02jIi/vz/9+uXdaWfq1Kls2rQpX4eVlRX29vZaKa8mFIBTYZHUqFlVK696zarcjbmvU+z/JjMzk8jIaNq3ezHcTKFQ0L6dF2FheS80J5wvc+TwCVq90YU2rXpoUmRENFs3h9CmVQ+9q5Ehp6Dh49OJzp3f5/btonUIlsO9lJPzvyiVtlStVom4uPiCd84DqeM0s7XGqqobmfFF77iqC87uzjiUtedRfP5NyoZ0yuU9Ko53U6AfJj2p16NHj/D392fNmjWSOVcsW8fOfZsYPX4oITv20MSjAR8O9GXiWP8ieYO+XUnw6iAiIqMJDz/D6FFDUCptWLtus3DqQGpqGhcvaC8I9fTpMxITH7+UrwuLF8/lvfd88PUdQmpqGq6uOc1syckpPH+eXsDRuWPq91JOzllzJ7P3j4PExNzHzc2FyV+OJjtbxfatu/SOsahxVpz2MUl/hpNx9yGWrmWpMOF91NkqEn/N6Xxo4VwGS+eympoOmzpVyE59Rsb9h2Qn5T3Sw8bWhorVXtQolK/kRq36NUlJSiHl8RM+nTCQQ78fITE+kQpVyzNy2jDu3rxH2OG8O8sbwvlf5PAeGcopBeoS3pBi0oWNxMRE1q1bJ2lh4+yZc3wyYDRfzhjH+C8+487tu0z3+7rIP2pbt4bg7OTIzBkTcXNzJirqPN26DyA+PqHgg4XT4Awb9iEA+/dv0cofMmQCGzbo1ydALvdSDs7yFdxYsWYRZR3L8ighkZNhEXTu4MujR0WrRShKnKXcy1H9uwlYlLUjKzGZ1FMXueQzmazEnKHYLh92pvz4FzWzdbbPA+DmuCU82nowVydA3Ua1WbZtsebz2FkjAfh98x4W+C2iZt3qdPXthJ19aRLiHnHyr3BWLFhDZkbeqxobwvlf5PAeGcopBXJuApEChVqtNlpxKyQkJN/tN27cYMKECWRnZ+vkdStTtyhh5YpYYl5axBLzJQ9DLDGf9Fy/kSr5IZaYL3lkZdwz+DlGVn1PEs93t4xbQ6MvRq3Z6NWrFwqFgvzKOwqFohgjEggEAoFAeuQ8R4YUGLWDqLu7O9u3b0elUuWapJ6qXCAQCAQCYyCmKzciHh4eRETk3UO4oFoPgUAgEAgEpo9Rm1EmTZpEWlreba41a9bk0KFDOnvN8xn6+qpjZoBmJ5UBCnxPMp5J7hSYNoboX2EIuqdeltx585cxkjvtukq7bpTAsJT0ZhSjFja8vb3z3a5UKmnbtm0xRSMQCAQCgWEo6aNRTHroq0AgEAgErwIlfZ6NktveIBAIBAKBoFgQNRsCgUAgEBiYkt6MUuJqNsKi9nHv8fmX0leB04rsHjF8INeuhJGacp3joTtp3qyxSTm9vDzZsT2YWzdPk5F+Fx+fTkWOzxBxyskphxiFU1qnm7sL3/0wnws3TnDzwRkOHfuNRo3rF+rYbJWKpTuP03XGajzHLqG7/xpW/BGmNeruUUoa09fv5a0vV9Bi7P/47Lvt3NZjPRY53Es5OYuKWqL/5IpJFjaqV6/O1auGmc2ua/v3aFy7rSb16/UpALt+3Vskr6+vDwsD/ZkzdxHNPTsTFX2B3b9vxNk57xVoi9upVNoSHX2BMWOKXrAyZJxyccohRuGU1ungYM/OvZvIysqif5+htG3RnZnT5pOUVLgZhoP3nWbr0Sim9G3H9ukDGdPTi7X7T/PT4bMAqNVqxq3Yyb2EZIKG+fCzX3/cHe0ZvmQbz9ILP0OuHO6lnJyComPU6cqXLFmSa/748eP54osvcHPLWeBo9OjROnkrlC3cXxkAs+ZNoUOntnh5dMl3v7i0pHy3Hw/dSfjpKMaMzfmHXKFQcOtGOEuXBbMgcGmh4ymqs7BDXzPS79LH91NCQgouZBU09NVUrr24nXKIUTh1dzrZ2ufpm+o/nuaeTejV9UOd4vhn6Ouo73+lnJ0tMwe8rdk2YeVOrCwtmPdxF27HPabn7LX8MvVDapbPWSZdpVLTwe8HRvm05p3WDTTH5Tf01VTupVycxTFd+cCq70riWXdrmySe4saoNRtjx44lMDCQoKAgraRSqVi/fj1BQUEsXrzYYOe3tLTknb7d2bxxe5E9TZs25MDBo5o8tVrNgYOhtGjhYTJOQyCXa5faKYcYhVN6Z6cu7Yg6e56Va4M4dzWU/Ue20f8j30If36haeU5ejuF2XE6zyOW7Dzlz/T6t61UFICMrZx0oK8sX3enMzBSUsjDnzPX7hTqHXO6lXJxSoVKrJUlyxaiFjaFDh+Lk5MTu3bu5efOmJpmbm7Nv3z5u3rzJjRs38nWkp6eTkpKildTqwnXF6dytPfYOdmzZ9GuRrsPJyRELCwvi47RXFYyPf4jb/y9lbgpOQyCXa5faKYcYhVN6Z+WqlRj4ST9uXL9Nv3eHsG71z8yd/yV93+9ZqOM/ebs5nT1eo9ectTQb9S39vt5A/3ZN6PZGzuKRVd3K4l7WjiW/hZLy9DmZWdkE7wsnLimVhJTCTYoml3spF6dAGow6GmX58uXs2LGDTp068cUXXzBy5EidHQEBAcyaNUsrr7SVE/Y2LgUe22/Auxz6M5S42Ic6n1cgEJQ8zMwURJ05T8CcxQCci75InXq1+GhQP7b89FuBx++LvMLu8EsEfNyVGu7luHw3nsBtf+HsoMSnRX0szc35ZmgPZm7YT5tJ32NupsCzdmVNzYdAvsi3TkIajD70tXfv3rzxxht89NFH/P777wQHB+t0vJ+fH+PHj9fKq1PZs8DjKlRyx/vNFgz+sOjTCCckJJKVlYWLq5NWvouLM7Fx+hVkDOE0BHK5dqmdcohROKV3xsclcOXyda28q5dv0K3H23kcoU3QjiMMers5nZvVBqBWBSceJD5hzb5wfFrk9DWrV9mVLV8O4MmzdDKzsnG0s2XAgp+oV8W1UOeQy72Ui1MqSvp05SYxGqVChQr8+eeftGnThiZNmui0+JqVlRX29vZaSVGItVHe+6A3CQ8TObDvSFFCByAzM5PIyGjat/PS5CkUCtq38yIsLO+F5orbaQjkcu1SO+UQo3BK7zwVFkmNmlW18qrXrMrdmML1p3iemfVSJ24zM0WubfF2NlY42tlyO/4xF+7E8WbDGoU6h1zupVycAmkwes3GPygUCvz8/Hj77bcJDQ3F3d3doOd6r39vtv78G9nZ2ZI4g75dSfDqICIiowkPP8PoUUNQKm1Yu26zyTiVSltq1qiq+Vy1aiUaNaxH4uMkYgr5Y1kcccrFKYcYhVNa54pl69i5bxOjxw8lZMcemng04MOBvkwc61+o49u8Xp1Ve0/h5miX04wS85ANByPp2fLFCLp9kVcoW9oGd0c7rt57xIJfDtOuUQ1a1a1S6DjlcC/l5JQCOc+RIQUmU9j4Bw8PDzw8cnoNx8TE4O/vz5o1ayQ9h/ebLalYqTybNxRtFMq/2bo1BGcnR2bOmIibmzNRUefp1n0A8fEJBR9cTE4Pj0b8uX+r5vPCwJkArF+/hcFDxudxVPHHKRenHGIUTmmdZ8+c45MBo/lyxjjGf/EZd27fZbrf12zfuqtQx0/p246lu44T8PNBElOf4uxQmne9GjCsSwvNPgnJaXyz7S8ePXmKs72S7p71GNql4KbhfyOHeyknpxSU9BlEjTrPRkFERUXRtGlTnWsfdJlno7AUNM+GqSCXJeYFAlMlv3k29EUsMW/aFMc8G75VCjdiqSC23i64I7IpYtSajZCQkHy3FzTsVSAQCAQCgelj1MJGr169UCgU+XYIVRjgL3WBQCAQCIqTkt5nw6ijUdzd3dm+fTsqlSrXFBkZaczwBAKBQCCQBJVESa4YtbDh4eFBRETew5EKqvUQCAQCgUBg+hi1GWXSpEmkpeU9BW/NmjU5dOiQzl65dOY0BHLpzGmIjqxSI5d7KRe6uTWR3Pl77BnJnYnPnkjudOg2V3Lnk82jJPXZvfc/SX0A1halJHdmZBd+9VtToqT/4WzUwoa3t3e+25VKJW3bti2maAQCgUAgMAxiBlGBQCAQCASvHAEBATRv3hw7OztcXFzo1asXly9f1trn+fPnfP7555QrV47SpUvz7rvvEhcXJ3ksorAhEAgEAoGBMUYH0b/++ovPP/+csLAw9u/fT2ZmJm+//bZW94Vx48axc+dOtm7dyl9//cX9+/d55513inStuWFyM4gKBAKBQPCqIdXQ1/T0dNLT07XyrKyssLKyemnfPXv2aH1eu3YtLi4uRERE0KZNG5KTk1m9ejWbNm2iffv2AAQHB1O3bl3CwsJo0aLFS059MamaDbVazaFDh1i5ciW7du0iM9NwHYFGDB/ItSthpKZc53joTpo3ayycJuL08vJkx/Zgbt08TUb6XXx8OhU5PkM4wfTvpak63/3cl4U7F/HzhS2si9yA38qpVKheQWuftz/oxNzNAfx0fgu/3dmF0l5Z7HH+F1N8N7NVKpbui6Tr/K14TltP9wW/sOLA2Zc6JN6IT2LMuj/x8t9Ai+k/8sF3O3mQlKrTuaS8l4OH9Cfs5B/cj43mfmw0Bw5t4623i95Hz1DfdVMhICAABwcHrRQQEFCoY5OTkwFwdHQEICIigszMTDp27KjZp06dOlSuXJkTJ05IGrdRCxtdu3bVXHxiYiItW7akQ4cOTJ06lZ49e9KwYUMePpR+WWBfXx8WBvozZ+4imnt2Jir6Art/34izcznhNAGnUmlLdPQFxoyZpndMxeGUw700Vefrnq+ze93vTOo1Ef/+07GwsGDmhjlY2bz468zKxoozf0Xwy9ItRovzv5jiuxn8199sDbvElJ4t2D6+N2O6NGPtX3/z0/GLmn1iHqUwaPluqjo7sGpoF7aO7cnQ9o2wsjAv9Hmkvpf37sUyY8Z8vFv70MarJ0f+OsHmLSuoW7eWXr5/MMQzkgIVakmSn58fycnJWsnPz6/g86tUjB07ltatW/P6668DEBsbS6lSpShTpozWvq6ursTGxkp6/UZdG8XMzIzY2FhcXFz47LPP+Ouvv9i1axfVqlXj7t279OrVi+bNm/P999/r5LUoVSHf7cdDdxJ+OooxY3NeRoVCwa0b4SxdFsyCwKV6XYtw6uYs7NDXjPS79PH9lJCQvXrFVhRnQUNfTeVeysWZ39BXe0d7fjy7Cb8+k7lw6rzWttdbNOCrLQF88Pp7pKVoD5UvaOjrq/puAiT/PBKAUWv3U660DTP7vFhWfcKPB7GyNGdev5yagsmbDmNhbsZX77XJ01fQ0Fd97qWuQ1/v3D3DtKkBrF+XdwFTl6Gvhb2fGel3C+3Uly6Vukji+SPmD72OGzFiBH/88QehoaFUrFgRgE2bNjFo0KCXmmXeeOMN2rVrx/z584sc7z+YTDPKwYMHCQgIoFq1agBUrFiR+fPns3evdF9kAEtLS5o2bciBg0c1eWq1mgMHQ2nRwkM4TcApB+RyL+XitLXLaSJJ1bFaPz9KyrvZqIoLJ6894PbDnFriy/cTOXM7jta1c/5BUanUHL0UQxUne0as3ku7OT8xYOlODp6/XehzGPpempmZ0adPd5RKG06dfDVnjjbmDKIjR45k165dHDp0SFPQAHBzcyMjI4OkpCSt/ePi4nBzc9PzbLlj9MLGP2ufPH78mBo1amhtq1mzJvfv38/3+PT0dFJSUrRSfpU1Tk6OWFhYEB+nvdxwfPxD3Fyd9boG4ZTWKQfkci/l4FQoFAyeOYQL4ee5c6Xw/wAWREl5Nz9p25DOjarRa9F2mn25ln7/+43+revTrUnO72li2jOeZmSx5vDftHqtIt9/+jbt61dhwoaDnL5RuKpyQ93L+vVrExt/jsSkyyxe8hXv9xvOpUvX9PYJtFGr1YwcOZIdO3Zw8OBBzR/z/+Dh4YGlpSUHDhzQ5F2+fJk7d+7QsmVLSWMx+miUjz/+GCsrKzIzM7l58yb1679YHj42NvaltqT/EhAQwKxZs7TyFGalUZhLv0y0QCCQnmFzR1D5tSr4vfuFsUORJfv+vsnus9cJ6NeWGq5luHw/kcBdp3C2t8HHoxaq///b6816lfnQO+f3tU75ckTdjueXk5doVl3av2B14cqVG7Rq0Q17Bzt69erCihUL6dyp3ytZ4DDGQmyff/45mzZt4rfffsPOzk7TD8PBwQEbGxscHBz49NNPGT9+PI6Ojtjb2zNq1Chatmwp6UgUMHJhY+DAgZr/79mzJ0+fPtXavm3bNho3bpyvw8/Pj/Hjx2vllS1XJ8/9ExISycrKwsXVSSvfxcWZ2Dj9OqMKp7ROOSCXe2nqzqGzh9O8Q3P8fKfwKPaRXvHkRUl5N4N2hzPozYZ0blQdgFpujjx4nMqaw3/j41GLsrZWWJgpqOHioHVcNRcHztyKL9Q5DHUvMzMzuXEjpzbr7JlzeHg05LPPBzF61FS9naaKMWYQ/ae/45tvvqmVHxwczMcffwxAUFAQZmZmvPvuu6Snp9OpUyeWLVsmeSxGbUYJDg7WSn379tXa7u/vz6+//pqvw8rKCnt7e62U37L0mZmZREZG077di85UCoWC9u28CAvLe1G4/BBOaZ1yQC730pSdQ2cPp0XnlkzrN5X4GOlnLCwp7+bzzOyXOrWamZlpOjhbWphTr6ITtxJStPa5/TAF9zKlC3WO4rqXZmZmlCol/XoqJRW1Wp1r+qegAWBtbc3SpUtJTEwkLS2N7du3S95fA0ygGSU/EhMT8ff3Z82aNZJ6g75dSfDqICIiowkPP8PoUUNQKm1Yu26zcJqAU6m0pWaNqprPVatWolHDeiQ+TiImJv8+PMXplMO9NFXnsLkjaNOzLfMGz+VZ2lPKOJcB4GnKUzLSMwAo41yGss5lca/qDkCVOlV5lvqUh/cekppcuI6kJeHdbFOnEqsORuFWRkkNl5xmlA2h5+jZ7MUQ0o/bNOCLnw7TtJorzau7c/zKXY5cimHV0MKPkJD6Xs6cNYn9+/4iJuYednal8e3rg3ebFvT0GVjwwflgiGckBSV9ITajDn0tiKioKJo2bUp2drZOxxU09BXgsxEfM2H8CNzcnImKOs/YcTM4FV60FSSFs/DO/IYXtmnTkj/3b30pf/36LQweMj6XIwpGH2dhVn01hXspF+e/h77+dmdXrvt8Oz6Ig7/kdFbrN+4D3h/3Qb77FGbV11fx3YQXQ1/T0jNZui+SQ+dvk5j6HGd7Wzo3qsawDo2x/Nc8Gr+GX2H14Wjik59SxdmBER0b065+Fc32wqz6quu9zG/o69Lvv+bNN1vj5uZMSvITzp27xKJFP3DoYGi+MRQ09FWf+1kcQ1/bVXxLEs+hu/sl8RQ3Ri1shISE5Lv9xo0bTJgwwSCFDYFxEUvMlzzkssS8HN5NeFHYkIqSvMS8KGwYHqM2o/Tq1QuFQpFv9VJ+/S8EAoFAIJADxhiNYkoYtYOou7s727dvR6VS5ZoiI1/NyV0EAoFAULJQqdWSJLli1MKGh4cHERF592QuqNZDIBAIBAKB6WPUZpRJkyaRlpaW5/aaNWty6NAhnb2W5tJfVrZKt34jhUHOpdSiUpKvvaRiiP4VhkAu76bUfSySJkg7iRNAmW/CJHfKFXm8VYbDqIUNb2/vfLcrlUrati36ksMCgUAgEBgTY0zqZUqY9DwbAoFAIBC8CpT0wobRF2ITCAQCgUDwaiNqNgQCgUAgMDAlfbBDiavZmDjxM0JDQ4iPP8/t2xFs2bKCWrWqF9nr5eXJju3B3Lp5moz0u/j4dJIgWhgxfCDXroSRmnKd46E7ad6ssXCakFMOMQpnyXQWxWf7xfeUDtj2UirlMxgAq17DsJ24FOXsTSinrsH6w8konPWbTFEO91IKVKglSXLFqIWNu3fvkpCQoPl89OhR+vfvj7e3NwMGDODEiROSn9Pb25Ply9fTtm0vuncfgIWFJbt2/YitrU2RvEqlLdHRFxgzZppEkYKvrw8LA/2ZM3cRzT07ExV9gd2/b8TZuZxwmoBTDjEKZ8l0FtX3dOlk0r76VJOerZoFQPbfOb/J2fdu8PyXpTxdNIZnwXMABTafTAeFbv+kyOFeCqTBqNOVe3p6Mn36dLp3785vv/3GO++8Q/fu3albty5Xrlxh165dbN++ne7du+vktbGpUvBO/4+TkyMxMWfo2NGXY8dO5bmfLkNfM9Lv0sf3U0JC9ua7X0FD7I6H7iT8dBRjxuYUYBQKBbduhLN0WTALApcWOh7hNIxTDjEKZ8l06uPLb+hrqe6DsKjjwdOFuU+RbuZWBdsxi0gL/Ax14osVfAsa+moq9zIr455e59KF5uXbSOIJv39EEk9xY9SajfPnz1O/fn0AAgICmDdvHr/99htff/0127dvZ9GiRcyYMcOgMdjb2wHw+HGSQc+jK5aWljRt2pADB49q8tRqNQcOhtKihYdwGtkphxiFs2Q6JY/R3ALLxm3IPH0wjxNaYeHRDlViHOrkR8aL00BOqchruXddk1wxamHDwsKCJ0+eAHDz5k26dNFe7rhLly5cvnw5X0d6ejopKSlaqbAPRKFQEBjoz/Hj4Vy4cEW/izAQTk6OWFhYEB+XoJUfH/8QN1dn4TSyUw4xCmfJdErts6j3BlgryYrQnmDRokUnlDM3UHr2Jixea8qz1bMgO8tocRrKKZAGoxY22rZty08//QRAkyZNOHz4sNb2Q4cOUaFC/p2OAgICcHBw0EpZWcmFOv/ixXOoX/81PvpI2tUTBQKB4FXBolkHsq+cQf3ksVZ+1pmjPP3fJJ7+MB1Vwn2sP5gAFpZGitL0KekdRI069PXrr7/G29ub+/fv4+XlxdSpUwkPD6du3bpcvnyZzZs3s3z58nwdfn5+jB8/XivPxeX1As8dFDSbrl070LFjX+7diy3SdRiChIREsrKycHF10sp3cXEmNu6hcBrZKYcYhbNkOqX0Kco4Y16zAc83BL68Mf0p6vSnqB894HnMFZQz1mFR35OsqNBij9OQTqmQcxOIFBi1ZqNu3bqcPHmSjIwMFixYQFpaGhs3bmTmzJlcu3aNn3/+mY8//jhfh5WVFfb29lqpoGXpg4Jm4+PTic6d3+f27RgJr0g6MjMziYyMpn07L02eQqGgfTsvwsLyXrxOOIvHKYcYhbNkOqX0WXq0Q52aQvblwhynAPPC12zI4V4KpMPok3rVqFGDn376CbVaTXx8PCqVCicnJywtDVMdt3jxXN57zwdf3yGkpqbh+v/teMnJKTx/nq63V6m0pWaNqprPVatWolHDeiQ+TiIm5r5ezqBvVxK8OoiIyGjCw88wetQQlEob1q7brHecwimdUw4xCmfJdEriUyiw8GhPVuRhUKleZJd1xaJhK7KvRqFOS0HhUI5SbXtDVkYhCyUSx1kMTimQcxOIFBi9sPEPCoUCV1dXrbyYmBj8/f1Zs2aNZOcZNuxDAPbv36KVP2TIBDZs+EVvr4dHI/7cv1XzeWHgTADWr9/C4CHj8zgqf7ZuDcHZyZGZMybi5uZMVNR5unUfQHx8QsEHC6fBnXKIUThLplMKn3nNhpiVdSYz4oD2hqwMzKvVw7J1dxQ2StSpyWTfusDT779EnZZS7HEWh1MK1CW8sGHUeTYKIioqiqZNm5Kdrdvy7rrMs1FYxBLzAoHgVaYkLzFfHPNsvO4qzf09FyePe/pfjFqzERISku/2GzduFFMkAoFAIBAIDIVRCxu9evVCoVDk20u3oM6eAoFAIBCYOiW9GcWoo1Hc3d3Zvn07KpUq1xQZGWnM8AQCgUAgkASVWi1JkitGLWx4eHgQEZF37+WCaj0EAoFAIBCYPkZtRpk0aRJpaWl5bq9ZsyaHDh3Kc3teZOowZa4xsTQ3mcFA+eJi6yC5896Twq+hUFjsShVt5d7/8iTjmaQ+OWFmgOZLufxVZohrNwRS309DdOZMHFTwBIu64hh8TnJncVDSm1GM+q+dt7d3vtuVSiVt27YtpmgEAoFAIDAMcilsGwqjNqMIBAKBQCB49ZFHPb5AIBAIBDKmpDejlNiajRHDB3LtShipKdc5HrqT5s0am5Rz4sTPCA0NIT7+PLdvR7Blywpq1apepPgM4TQzM2OC3+eERv7B5bunOHL6d0ZPGFok5z8Y4hkBjB0/jMep15g3f2qRXab+HhnK6eXlyY7twdy6eZqM9Lv4+HQqcoyGiFNqpyGuuyTdS0WZclh/8gWlv9mK3f9CUM5YjlmVWi+225XBeuAESs/fhN3/fsN29FeYuZQv9jgNgRiNYkS++eYbbt++Xezn9fX1YWGgP3PmLqK5Z2eioi+w+/eNODuXMxmnt7cny5evp23bXnTvPgALC0t27foRW1v9O0EawjlizCcMGNSXGZPn0aFlL76etZhhowfx8dAP9HaCYZ4RQJOmDfj4k36c+/tikTyGilEuTqXSlujoC4wZM01vR3HEKbXTENddYu6lbWmUkxZBdjZP/zeN1JlDeL51Beq0VM0uNp/5Y+bsztNlM0mb+zmqR3HYjv0aSlkVX5wCg2DU6crNzMwwMzOjXbt2DB48mN69e1OqVKkiey1KVch3+/HQnYSfjmLM2Jwvt0Kh4NaNcJYuC2ZB4FK9zqmPU5fRKE5OjsTEnKFjR1+OHTulV4z6OvMbjbJm0/9IePiIL8bM1OQtX7uI58+fM3b4l3keV9BoFH3uZ0GjUZRKWw6H/sbEcf5MnPw5f0df4MvJX+W5f0GjUUzlPTKEU5cRGRnpd+nj+ykhIXvz3a+gv8rkdu2FvW5d0MWZ3/00lXv579EoVr0/wbxGfZ4unJDrvmYuFSg9Zw2pM4eievD/f4QqFJRe8DPpvwaTeWwPUPBoFH3iLI7pyqs7NZHEcyPhjCSe4sbozSirVq1CqVTy4YcfUr58ecaOHcu5c4Yb2mRpaUnTpg05cPCoJk+tVnPgYCgtWniYjPO/2NvbAfD4cZIkPqmcEeFRtGrjSbUaOevR1K3/Gs08m3D4z1C9nYa6n4GLZrJv72H+Onxcb4chY5SL0xCU5GuXGlO9lxYNW5B9+wo2Q6dSOnAzyqlLsfTq8q8dclb6VmdmvMhTqyErE/Oa9YstTkOhVqskSXLF6IWNrl278uuvv3L37l2++OIL9u7dS6NGjXjjjTdYuXIlT548kfR8Tk6OWFhYEB+nvQJgfPxD3P5/uXlTcP4bhUJBYKA/x4+Hc+HClSL7pHQuW7yanTv2cDDsN67FRrD78BbW/LCBX3/ZrbfTEPfznT7daNS4PrP9A/WO69/I5T0y9LspFSX52qXGVO+lmbM7pdp2RxV/n6dLviTjyC6s3xuBZYuOAKhiY1A9isO69ydgWxrMLSjVqS9mjs6YOTgWW5yGQoVakiRXjF7Y+AcXFxe++OILLl68yOHDh6lXrx7jxo3D3d093+PS09NJSUnRSq/arKOLF8+hfv3X+OijkSbn7N6rE736dGP00Cl0a9eP8Z9PY+jnA3m3n49EkRadChXcCVgwnaGfjCc9PaPgAwQCgfQoFGTfuUb6r8GoYq6TefQPMkL/wLJtt5ztqmyeLp+NmWsF7IO2Yfe/ECxqNyLz71M5NRwCWWPUoa95LbLm7e2Nt7c3S5YsYfPmzfk6AgICmDVrlrbXrDQKc/tc909ISCQrK+v/2rv3uBjT/g/gn+kw0zQqOjeoEJVWUWhzSvRUeJRlaW1sVtoHtcoh1volp5XDOq8tbMJai91V22axsQqbY8lh2SgkKjlGUcbM9fvDY3ZH5znUzNP3va95vXbuuecz3/ue+8411324YG5hKjPd3NwMJffuN6J61Wa+sWbNIgwdOhje3mNw926JQlmqyPx84QzErXvduwEAuVevo117K0yNDMFPu+se1bc2yl6fLj2cYG5uivQ/fpZO09HRQZ++vRD6n/GwMO4KiaRx3ZOash2pcttUppa87MqmruuSlT36+1yM/5IUF0K3R7+/n9/OQ8WSqYCePjg6umDlZRB8tg7igob1vqrzd/6/9iO4sZq1Z6O+lW9oaIjQ0NA655k7dy7KyspkHhwtg1rnF4lEyM6+iEFef2/gHA4Hg7z64dSp2sdpqYsqMoHXjQJ/f1/4+Y1FQUGh3DmqzOTz9SCRyH6PYrFEoVs+K3t9Hks/iT69h2BAn+HSR3bWRfywJwUD+gxvdENDFTVqUqYqtORlVzZ1XZfi/CvQsmgvM03Loi0kj0qrz1z5HKy8DFrmQmjZdIYo52ST1akqLf0wSrP2bMjzR/5tPB4PPJ7sZVH1DUu/Zt0WJCasQVb2RZw9ex7TPg2FQMDHtu1196I0ZebatUsQGOiP0aNDUV5eAYv/Hm8sK3uKysoqtck8fCgD4TNCUXSnGNf+yoeTswMmTRmPvbuS5cp7Q5nrs7y8AlevXJeZ9vz5Czx69Lja9OaqUdMyBQJ92HWylT63tW0PF+euePT4CQoLi9SmTmVnqmK5W8q6rDq8D4I5a8Ad8gFE545B29Ye3P5D8WLnWuk8Oq79wcrLIHlUCu22HaA3ZjJe5ZyE+GrDRwBXxbITxan1HUQLCwsRExODrVu3KjX3hx9SYGZqjAXzZ8HS0gwXLvyJYf8eh9LSB/W/uYky//Of8QCAtLS9MtNDQ2di584f1SYz5rNYzJwbjsUr58HU1Bj3Su5j1/YfsW5lvFx5b6jiO1I2TdiOVJXp5uaCw2k/SJ9/uXIBAGDHjr2YFDpDbepUdqYqlrulrEtJwTW8iFsE3nsfgzcsCJIHJajcG49XZ/4ebFPLyBjc0f8Bx7A1WNkjiE4dRtX+XU1ap6q09MMozXqfjfpcuHABrq6uEIvFjXpffffZUBc06qty0aivykOjvqo/TVifmjLqa1PcZ8OqdVel5BQ/uaKUnKbWrP/apaTUfQLhjRs3mqgSQgghhKhKszY2RowYAQ6HU2f3Un3nXxBCCCHqjgZia0ZWVlbYt28fJBJJjY/s7IafFEQIIYSoK8aYUh6aqlkbG25ubsjKqv1ypPp6PQghhBCi/pr1MEpUVBQqKipqfd3Ozg5Hjx6t9fXa6OkoPpjb2ypfKf/OkyLxK6VnquLktuLyR0rPVIWWekKnppzMqSl1amtpKz1TU/Z1ZVPFyZwV53coPbMpaPI9MpShWRsb/fv3r/N1gUAAT0/PJqqGEEIIUY2W3kuvGddeEkIIIRpMEy5VViW1GYiNEEIIIf+bqGeDEEIIUbGWfhilxfVsTAoNwqnTB1BUchFFJRdx5OhP+JePcs4LmTI5GHnXTqH8aT4yT/yCXj27q1Vmv37uSNqXiFs3z+Fl1R34+/sqXJ8qMt9Q9/WpKTWq6jtqiXXOmjUVJ06koLT0TxQUZGHv3s3o3Lmj2tWpSfu6ostd8aISyxN+gO8n/4deH0Rg/NyVuHz9lsw8N+4U49OlcegzbgZ6j43E2KhlKL7ftCe+t/SB2Jq9sZGamor58+fjjz/+AAD8/vvvGDp0KPz8/LB582alf97duyWYP385+vf1x4B+ATiWcRJ79m6Go2NnhXJHj/bHlytjsHjJavRy98OFi1fw6/7vYGZmojaZAoE+Ll68goiI/5O7pqbIBDRjfWpCjYBqvqOWWmf//u6Ij98BT88R+Pe/x0FHRxepqd9CX1+xW+W31H1dGcu9YONOnLr4F76ICMZPa+bBw8URnyxcj3sPnwAACkvuI/jz1ejQzgIJi6bjpzXz8MnooeDq6iptOUj9mnVslE2bNiE8PBwuLi64fv06Nm7ciKlTpyIwMBDa2trYsWMHYmNjERER0ajcVvodGjX/7Tvn8X/zYrFj+95a56nv0tfME7/g7LkLiIh8vSNyOBzcunEWG79OxIqVGxtVjyKZDb0c7mXVHbw/OgQpKYfkqk3RzPpOllKX9akJNTbmEsiGfkeq+H40pc7GjFlkamqMwsLz8PYejT/+OFPrfPVd+vq/uq+r4vv556WvlVUv4RE0A+s++w8G9OwmnR44Kxb9XJ3w6Yf+mL0qATo62lgaMaHWOnhOg+usUxkMBcrpAXtaoZnDeDRrz8b69evx9ddf49y5c0hOTkZoaCiWLVuGLVu2ID4+Hl9//TU2bdqkss/X0tLC++//GwIBH2dOy3+3Ul1dXbi6OuPI78el0xhjOPL7Cbz7rpvaZGoKTVifmlCjqlCdfzM0NAAAPH78RO4MTVmfyqaM5RZLJBBLJOByZXsp9LhcnL+aD4lEgmNZl2FjZY7JizbAc8JsfDhnBX4/naPMRWkQCWNKeWiqZm1s3Lx5E76+r4/7eXl5QSwWY8CAAdLXBw4ciIKCgjozqqqq8PTpU5lHfZ01Tk72KCm9jEdPcrF2/RcY+8Fk/PVXntzLYWpqDB0dHZTekx3CuLT0PiwtzNQmU1NowvrUhBpVhep8jcPhYOXKGGRmnsWVK9fkztGU9alsylhuAV8PLvYdsPmHAyh99ARisQSpGadx4doN3H9chkdlz/C8sgoJSb+hb4+u2BTzKQa7u2D6ii0496f83xlpvGZtbJiYmEgbE0VFRXj16hVu374tfb2goADGxsZ1ZsTGxsLIyEjmIXr1pM73XLt2A33eHYaBnu/hmy07sXnzl3BwsFN4eQghLcfatYvh5NQFH30U3tyltGhLIyaAMQbvSZ+jZ+A07NqfjiH9ekKLw5H2BHj1dsb44YPh0KE9Qkb6YoDbO9h76EST1smU9J+matZLXwMCAhASEoLg4GCkpKTgo48+wsyZM6GlpQUOh4OoqCj4+PjUmTF37lzMmDFDZpqVhXOd7xGJRLhx43UjJ+f8Zbi5OWNq2MeY9uk8uZbjwYNHePXqFcwtTGWmm5uboeTefbXJ1BSasD41oUZVoTqBNWsWYejQwfD2HoO7d0sUytKU9alsylru9pZmSFwyA88rq1DxvBJmxkaI+vIbtLMwRRuDVtDR1kKndlYy7+nYzhLnr+YrZTkaSpMPgShDs/ZsLF++HAMHDsTu3bvRvXt3bN68GSEhIQgICMCQIUNgYmKC2NjYOjN4PB4MDQ1lHo0dll5LSwtcrvzjqYhEImRnX8Qgr37SaRwOB4O8+uHUqdoHmmvqTE2hCetTE2pUlZZe55o1i+Dv7ws/v7EoKChU2zrVnbKXW1+PBzNjIzwtf47MnKvw6u0CXV0dONnZ4FbRPZl5C4pKYWVed685Ua5m7dkQCATVLm+dNWsWwsPDIRKJYGBgoPTPXLAwCmm/ZaCw8C4MDFph9Bh/9B/wLgL8gxXKXbNuCxIT1iAr+yLOnj2PaZ+GQiDgY9v2PWqTKRDow66TrfS5rW17uDh3xaPHT1BYWKQ2mYBmrE9NqBFQzXfUUutcu3YJAgP9MXp0KMrLK2Dx33MLysqeorKySq5MVdSpKfu6Mpb7j/NXwBiDbVsLFBbfx+odSbBta4GAQR4AgAkB/0LU6gS4drVD73e64I/zV5Bx7hISFkfKVbO8WvpNvZr10tf6FBYWIiYmBlu3bm3U++q69HVj3DIMHNgXlpZmeFr2DJcv/4XVqzfh6O91H79ryKivU6dMwMwZU2BpaYYLF/5E5PT5OHP2fKNqVzSzrsvhBgzwwOG0H6pN37FjLyaFzqjhHfWTN7MhXYrqsD41ocb6LoGU5ztSxfejKXXWdenrixc1n7AeGjoTO3f+WOv7GjLq6//ivq6K7+ftUV8P/ZGFdTt/xr2HT2DUSh/eHj3w6Yf+MBD8fe+TpCOZSNh3CPcePoGt0AJTPxgGr94u0teb4tJXnl57peRUVSrem9Yc1LqxceHCBbi6ukIsFjfqfY29z0ZDqGKIeVXQhGGnATp+qUyaMnS7ptTZmPtsNFRLHWJeFd+PKoaYb4rGBpfXTik5L6vuKCWnqTXrYZSUlJQ6X79xQzNvXkIIIYSoi40bN2LlypUoKSmBi4sLNmzYgN69ezdpDc3a2BgxYgQ4HE6dx7Iae7InIYQQom6a6yDCnj17MGPGDMTHx8Pd3R1r166Fr68vcnNzYW5u3mR1NOvVKFZWVti3bx8kEkmNj+xs+e/qSQghhKgLpqRHY61evRqhoaH4+OOP0bVrV8THx0NfX7/R50IqqlkbG25ubsjKqv0Sp/p6PQghhJCWpKa7ZldV1Xwl1MuXL5GVlQVvb2/pNC0tLXh7e+PkyZNNVfJrrBkdO3aMHThwoNbXy8vLWXp6uso+v7KyksXExLDKykq1zdSEGimTMml7p8yWkKkOYmJiqnV4xMTE1Djv3bt3GQCWmZkpMz0qKor17t27Car9m1pfjaJqT58+hZGREcrKymBoaKiWmZpQI2VSJm3vlNkSMtVBVVVVtZ4MHo8HHo9Xbd6ioiK0bdsWmZmZ8PDwkE6fPXs2MjIycPr0aZXX+0azniBKCCGEkIarrWFRE1NTU2hra+PePdk7qN67dw+WlpaqKK9WzXrOBiGEEEJUg8vlws3NDUeOHJFOk0gkOHLkiExPR1Ogng1CCCHkf9SMGTMQHByMnj17onfv3li7di0qKirw8ccfN2kdLbqxwePxEBMT0+AuqebI1IQaKZMyaXunzJaQqYkCAwNx//59zJ8/HyUlJejevTsOHjwICwuLJq2jRZ8gSgghhBDVo3M2CCGEEKJS1NgghBBCiEpRY4MQQgghKkWNDUIIIYSoVIttbGzcuBG2trbQ09ODu7s7zpw5o1DesWPHMHz4cAiFQnA4HCQnJyuUFxsbi169esHAwADm5uYYMWIEcnNzFcqMi4uDs7MzDA0NYWhoCA8PDxw4cEChzLctW7YMHA4HkZGRcmcsWLAAHA5H5uHg4KBwbXfv3sW4ceNgYmICPp+Pbt264dy5c3Ln2draVquTw+EgLCxMrjyxWIzo6Gh06NABfD4fnTp1wuLFixUeH+jZs2eIjIyEjY0N+Hw++vTpg7Nnzzb4/fVt24wxzJ8/H1ZWVuDz+fD29sb169cVyty3bx98fHxgYmICDoeDnJwcheoUiUSYM2cOunXrBoFAAKFQiI8++ghFRUUK1blgwQI4ODhAIBCgTZs28Pb2rveujI35WzF58mRwOBysXbtWocwJEyZU2079/PwUrvPq1avw9/eHkZERBAIBevXqhdu3b8udWdP+xOFwsHLlSrnyysvLER4ejnbt2oHP50sHIiNNr0U2Nt4MuRsTE4Ps7Gy4uLjA19cXpaWlcmdWVFTAxcUFGzduVEqNGRkZCAsLw6lTp5CWlgaRSAQfHx9UVFTIndmuXTssW7YMWVlZOHfuHAYNGoSAgAD8+eefSqn57Nmz2LRpE5ydnRXOcnJyQnFxsfRx4sQJhfIeP36Mvn37QldXFwcOHMCVK1ewatUqtGnTRu7Ms2fPytSYlpYGABg9erRcecuXL0dcXBy++uorXL16FcuXL8eKFSuwYcMGuWsEgEmTJiEtLQ3ffvstLl26BB8fH3h7e+Pu3bsNen992/aKFSuwfv16xMfH4/Tp0xAIBPD19UVlZaXcmRUVFejXrx+WL1/eoBrry3z+/Dmys7MRHR2N7Oxs7Nu3D7m5ufD395c7EwC6dOmCr776CpcuXcKJEydga2sLHx8f3L9/X+7MN5KSknDq1CkIhcI652topp+fn8z2+v333yuUmZ+fj379+sHBwQHp6em4ePEioqOjoaenJ3fmP+srLi7G1q1bweFwMGrUKLnyZsyYgYMHD2Lnzp24evUqIiMjER4ejpSUlDqXnahAk47EoiZ69+7NwsLCpM/FYjETCoUsNjZWKfkAWFJSklKy3igtLWUAWEZGhlJz27Rpw7755huFc549e8Y6d+7M0tLSmKenJ4uIiJA7KyYmhrm4uChc0z/NmTOH9evXT6mZb4uIiGCdOnViEolErvcPGzaMTZw4UWbayJEjWVBQkNw1PX/+nGlra7PU1FSZ6a6urmzevHmNznt725ZIJMzS0pKtXLlSOu3JkyeMx+Ox77//Xq7Mf7p58yYDwM6fP69QnTU5c+YMA8AKCgqUlllWVsYAsMOHDyuUeefOHda2bVt2+fJlZmNjw9asWdOgvNoyg4ODWUBAQIMzGpIZGBjIxo0bp9TMtwUEBLBBgwbJnefk5MQWLVokM03ebZ8opsX1bKjVkLuNUFZWBgAwNjZWSp5YLMbu3btRUVGhlNvWhoWFYdiwYTLrVRHXr1+HUChEx44dERQUVGfXbEOkpKSgZ8+eGD16NMzNzdGjRw9s2bJFKbUCr7ernTt3YuLEieBwOHJl9OnTB0eOHMG1a9cAABcuXMCJEycwZMgQuet69eoVxGJxtV+bfD5f4d4iALh58yZKSkpkvncjIyO4u7ur9f4EvN6nOBwOWrdurZS8ly9fYvPmzTAyMoKLi4vcORKJBOPHj0dUVBScnJyUUhsApKenw9zcHPb29pgyZQoePnyoUI379+9Hly5d4OvrC3Nzc7i7uyt8+Pif7t27h/379yMkJETujD59+iAlJQV3794FYwxHjx7FtWvX4OPjo7Q6ScO0uMbGgwcPIBaLq909zcLCAiUlJc1UVd0kEgkiIyPRt29fvPPOOwplXbp0Ca1atQKPx8PkyZORlJSErl27KpS5e/duZGdnIzY2VqGcN9zd3bFt2zYcPHgQcXFxuHnzJvr3749nz57JnXnjxg3ExcWhc+fOOHToEKZMmYJp06Zh+/btSqk5OTkZT548wYQJE+TO+Oyzz/DBBx/AwcEBurq66NGjByIjIxEUFCR3poGBATw8PLB48WIUFRVBLBZj586dOHnyJIqLi+XOfePNPqNJ+xMAVFZWYs6cORg7dqzCI4KmpqaiVatW0NPTw5o1a5CWlgZTU1O585YvXw4dHR1MmzZNobr+yc/PDzt27MCRI0ewfPlyZGRkYMiQIRCLxXLllZaWory8HMuWLYOfnx9+++03vPfeexg5ciQyMjKUUvP27dthYGCAkSNHyp2xYcMGdO3aFe3atQOXy4Wfnx82btyIAQMGKKVG0nAt+nblmiIsLAyXL19Wyi9Re3t75OTkoKysDD/++COCg4ORkZEhd4OjsLAQERERSEtLq/NYbWP885e8s7Mz3N3dYWNjg71798r9K0cikaBnz55YunQpAKBHjx64fPky4uPjERwcrHDNCQkJGDJkSIOOr9dm7969+O6777Br1y44OTkhJycHkZGREAqFCtX47bffYuLEiWjbti20tbXh6uqKsWPHIisrS+5MTSYSiTBmzBgwxhAXF6dwnpeXF3JycvDgwQNs2bIFY8aMwenTp2Fubt7orKysLKxbtw7Z2dly95DV5IMPPpD+f7du3eDs7IxOnTohPT0dgwcPbnSeRCIBAAQEBGD69OkAgO7duyMzMxPx8fHw9PRUuOatW7ciKChIob8rGzZswKlTp5CSkgIbGxscO3YMYWFhEAqFSuuFJQ3T4no21GnI3YYIDw9Hamoqjh49inbt2imcx+VyYWdnBzc3N8TGxsLFxQXr1q2TOy8rKwulpaVwdXWFjo4OdHR0kJGRgfXr10NHR0fuX07/1Lp1a3Tp0gV5eXlyZ1hZWVVrUDk6Oip8eAYACgoKcPjwYUyaNEmhnKioKGnvRrdu3TB+/HhMnz5d4R6jTp06ISMjA+Xl5SgsLMSZM2cgEonQsWNHhXIBSPcZTdmf3jQ0CgoKkJaWpnCvBgAIBALY2dnh3XffRUJCAnR0dJCQkCBX1vHjx1FaWgpra2vp/lRQUICZM2fC1tZW4Vrf6NixI0xNTeXep0xNTaGjo6Oyfer48ePIzc1VaJ968eIFPv/8c6xevRrDhw+Hs7MzwsPDERgYiC+//FLhGknjtLjGhjoNuVsXxhjCw8ORlJSE33//HR06dFDJ50gkElRVVcn9/sGDB+PSpUvIycmRPnr27ImgoCDk5ORAW1tb4RrLy8uRn58PKysruTP69u1b7dLha9euwcbGRtHykJiYCHNzcwwbNkyhnOfPn0NLS3aX1NbWlv6KVJRAIICVlRUeP36MQ4cOISAgQOHMDh06wNLSUmZ/evr0KU6fPq1W+xPwd0Pj+vXrOHz4MExMTFTyOYrsU+PHj8fFixdl9iehUIioqCgcOnRIaTXeuXMHDx8+lHuf4nK56NWrl8r2qYSEBLi5uSl07otIJIJIJFLpPkUarkUeRlHFkLvl5eUyvxJu3ryJnJwcGBsbw9rautF5YWFh2LVrF37++WcYGBhIj38bGRmBz+fLVePcuXMxZMgQWFtb49mzZ9i1axfS09MV+iNmYGBQ7TwSgUAAExMTuc8vmTVrFoYPHw4bGxsUFRUhJiYG2traGDt2rNx1Tp8+HX369MHSpUsxZswYnDlzBps3b8bmzZvlzgRe/8OSmJiI4OBg6OgotjsNHz4cX3zxBaytreHk5ITz589j9erVmDhxokK5hw4dAmMM9vb2yMvLQ1RUFBwcHBq8vde3bUdGRmLJkiXo3LkzOnTogOjoaAiFQowYMULuzEePHuH27dvS+2C8+UfN0tKy1h6TujKtrKzw/vvvIzs7G6mpqRCLxdJ9ytjYGFwut9GZJiYm+OKLL+Dv7w8rKys8ePAAGzduxN27d+u8/Lm+ZX+7EaSrqwtLS0vY29vLlWlsbIyFCxdi1KhRsLS0RH5+PmbPng07Ozv4+vrKXWdUVBQCAwMxYMAAeHl54eDBg/jll1+Qnp4udybwurH6ww8/YNWqVbXmNDTP09MTUVFR4PP5sLGxQUZGBnbs2IHVq1fXm02UrHkvhmk+GzZsYNbW1ozL5bLevXuzU6dOKZR39OhRBqDaIzg4WK68mrIAsMTERLlrnDhxIrOxsWFcLpeZmZmxwYMHs99++03uvNooeulrYGAgs7KyYlwul7Vt25YFBgayvLw8hev65Zdf2DvvvMN4PB5zcHBgmzdvVjjz0KFDDADLzc1VOOvp06csIiKCWVtbMz09PdaxY0c2b948VlVVpVDunj17WMeOHRmXy2WWlpYsLCyMPXnypMHvr2/blkgkLDo6mllYWDAej8cGDx5c7/qoLzMxMbHG12NiYuTKfHMJbU2Po0ePypX54sUL9t577zGhUMi4XC6zsrJi/v7+7MyZMwot+9saculrXZnPnz9nPj4+zMzMjOnq6jIbGxsWGhrKSkpKFK4zISGB2dnZMT09Pebi4sKSk5MVzty0aRPj8/kN2kbryysuLmYTJkxgQqGQ6enpMXt7e7Zq1Sq5L08n8qMh5gkhhBCiUi3unA1CCCGENC1qbBBCCCFEpaixQQghhBCVosYGIYQQQlSKGhuEEEIIUSlqbBBCCCFEpaixQQghhBCVosYGIYQQQlSKGhuENKMJEybI3NZ74MCBiIyMbPI60tPTweFw8OTJk1rn4XA4SE5ObnDmggUL0L17d4XqunXrFjgcDnJychTKIYQ0L2psEPKWCRMmgMPhgMPhSEfJXbRoEV69eqXyz963bx8WL17coHkb0kAghBB10CIHYiOkPn5+fkhMTERVVRV+/fVXhIWFQVdXF3Pnzq0278uXL2sdyKuxjI2NlZJDCCHqhHo2CKkBj8eDpaUlbGxsMGXKFHh7eyMlJQXA34c+vvjiCwiFQulonIWFhRgzZgxat24NY2NjBAQE4NatW9JMsViMGTNmoHXr1jAxMcHs2bPx9tBEbx9Gqaqqwpw5c9C+fXvweDzY2dkhISEBt27dgpeXFwCgTZs24HA4mDBhAoDXI9HGxsaiQ4cO4PP5cHFxwY8//ijzOb/++iu6dOkCPp8PLy8vmTobas6cOejSpQv09fXRsWNHREdHQyQSVZtv06ZNaN++PfT19TFmzBiUlZXJvP7NN9/A0dERenp6cHBwwNdff13rZz5+/BhBQUEwMzMDn89H586dkZiY2OjaCSFNi3o2CGkAPp+Phw8fSp8fOXIEhoaGSEtLAwCIRCL4+vrCw8MDx48fh46ODpYsWQI/Pz9cvHgRXC4Xq1atwrZt27B161Y4Ojpi1apVSEpKwqBBg2r93I8++ggnT57E+vXr4eLigps3b+LBgwdo3749fvrpJ4waNQq5ubkwNDQEn88HAMTGxmLnzp2Ij49H586dcezYMYwbNw5mZmbw9PREYWEhRo4cibCwMHzyySc4d+4cZs6c2eh1YmBggG3btkEoFOLSpUsIDQ2FgYEBZs+eLZ0nLy8Pe/fuxS+//IKnT58iJCQEU6dOxXfffQcA+O677zB//nx89dVX6NGjB86fP4/Q0FAIBAIEBwdX+8zo6GhcuXIFBw4cgKmpKfLy8vDixYtG104IaWLNPOosIWonODiYBQQEMMZeD5+elpbGeDwemzVrlvR1CwsLmaHfv/32W2Zvby8zdHVVVRXj8/ns0KFDjDHGrKys2IoVK6Svi0Qi1q5dO+lnMcaYp6cni4iIYIwxlpubywCwtLS0Gut8M7z248ePpdMqKyuZvr4+y8zMlJk3JCSEjR07ljHG2Ny5c1nXrl1lXp8zZ061rLcBYElJSbW+vnLlSubm5iZ9HhMTw7S1tdmdO3ek0w4cOMC0tLRYcXExY4yxTp06sV27dsnkLF68mHl4eDDGmHRo+PPnzzPGGBs+fDj7+OOPa62BEKKeqGeDkBqkpqaiVatWEIlEkEgk+PDDD7FgwQLp6926dZM5T+PChQvIy8uDgYGBTE5lZSXy8/NRVlaG4uJiuLu7S1/T0dFBz549qx1KeSMnJwfa2trw9PRscN15eXl4/vw5/vWvf8lMf/nyJXr06AEAuHr1qkwdAODh4dHgz3hjz549WL9+PfLz81FeXo5Xr17B0NBQZh5ra2u0bdtW5nMkEglyc3NhYGCA/Px8hISEIDQ0VDrPq1evYGRkVONnTpkyBaNGjUJ2djZ8fHwwYsQI9OnTp9G1E0KaFjU2CKmBl5cX4uLiwOVyIRQKoaMju6sIBAKZ5+Xl5XBzc5MeHvgnMzMzuWp4c1ikMcrLywEA+/fvl/lHHnh9HoqynDx5EkFBQVi4cCF8fX1hZGSE3bt3Y9WqVY2udcuWLdUaP9ra2jW+Z8iQISgoKMCvv/6KtLQ0DB48GGFhYfjyyy/lXxhCiMpRY4OQGggEAtjZ2TV4fldXV+zZswfm5ubVft2/YWVlhdOnT2PAgAEAXv+Cz8rKgqura43zd+vWDRKJBBkZGfD29q72+pueFbFYLJ3WtWtX8Hg83L59u9YeEUdHR+nJrm+cOnWq/oX8h8zMTNjY2GDevHnSaQUFBdXmu337NoqKiiAUCqWfo6WlBXt7e1hYWEAoFOLGjRsICgpq8GebmZkhODgYwcHB6N+/P6KioqixQYiao6tRCFGCoKAgmJqaIiAgAMePH8fNmzeRnp6OadOm4c6dOwCAiIgILFu2DMnJyfjrr78wderUOu+RYWtri+DgYEycOBHJycnSzL179wIAbGxswOFwkJqaivv376O8vBwGBgaYNWsWpk+fju3btyM/Px/Z2dnYsGEDtm/fDgCYPHkyrl+/jqioKOTm5mLXrl3Ytm1bo5a3c+fOuH37Nnbv3o38/HysX78eSUlJ1ebT09NDcHAwLly4gOPHj2PatGkYM2YMLC0tAQALFy5EbGws1q9fj2vXruHSpUtITEzE6tWra/zc+fPn4+eff0ZeXh7+/PNPpKamwtHRsVG1E0KaHjU2CFECfX19HDt2DNbW1hg5ciQcHR0REhKCyspKaU/HzJkzMX78eAQHB8PDwwMGBgZ477336syNi4vD+++/j6lTp8LBwQGhoaGoqKgAALRt2xYLFy7EZ599BgsLC4SHhwMAFi9ejOjoaMTGxsLR0RF+fn7Yv38/OnToAOD1eRQ//fQTkpOT4eLigvj4eCxdurRRy+vv74/p06cjPDwc3bt3R2ZmJqKjo6vNZ2dnh5EjR2Lo0KHw8fGBs7OzzKWtkyZNwjfffIPExER069YNnp6e2LZtm7TWt3G5XMydOxfOzs4YMGAAtLW1sXv37kbVTghpehxW29lphBBCCCFKQD0bhBBCCFEpamwQQgghRKWosUEIIYQQlaLGBiGEEEJUihobhBBCCFEpamwQQgghRKWosUEIIYQQlaLGBiGEEEJUihobhBBCCFEpamwQQgghRKWosUEIIYQQlfp/x6UVgXBDCrQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nur um die Confusionmatrix einmal richtig zu erzeugen\n",
    "# Define the CNN model\n",
    "def create_model(layer,opti_str):# Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), padding='same', input_shape=(256,128,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    for x in range(layer):\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(16, (3,3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    optimizer=RMSprop(learning_rate=0.001)\n",
    "    if opti_str == 'ADAM':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for layer, opti_str in [(2,'ADAM')]:\n",
    "        for bs in range(2,3):\n",
    "            title_appendix = f'CNN_layer={layer+1}_optimizer={opti_str}_batchsize={pow(2,4+bs)}'\n",
    "            print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer+1, opti_str, pow(2,4+bs)))\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            lss = []\n",
    "            acc = []\n",
    "            # Training with cross-validation\n",
    "            for train, test in kfold.split(X_train, y_train):\n",
    "                model = create_model(layer,opti_str)\n",
    "                early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "                model.fit(X_train[train], y_train[train], epochs=100, batch_size=pow(2,4+bs), verbose=0, callbacks=[early_stopping])\n",
    "                score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "                lss.append(score[0])\n",
    "                acc.append(score[1])\n",
    "                print(f'Test loss: {score[0]}, Test accuracy: {score[1]}')\n",
    "            print(f'Average Model Performance: Loss: {sum(lss)/len(lss)}; Accuracy: {sum(acc)/len(acc)}')\n",
    "            \n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print(f'Validation Performance: Loss: {loss}; Accuracy: {accuracy}')\n",
    "\n",
    "            generate_confusion_matrix(X_test, y_test, title_appendix)\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f22afd-41e8-45f1-8009-e50cb16f39d5",
   "metadata": {},
   "source": [
    "Interessehalber wird die Modelperformance auch für Inception-Layer anstelle von Convolution-MaxPooling-Layer-Paaren getestet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcefc563-5171-45c9-9218-493963e6ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 16:31:25.135131: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-18 16:31:25.135244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7024 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-18 16:31:28.754841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-18 16:31:30.540105: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f073c044b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-18 16:31:30.540139: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-18 16:31:30.599495: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-18 16:31:30.941813: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.7598 - accuracy: 0.8280\n",
      "Number of layers: 1; Optimizer: RMSP; Batchsize: 32\n",
      "Final Training Loss: 0.040205974131822586\n",
      "Final Training Accuracy: 0.9881075620651245\n",
      "Final Validation Loss: 0.7812396287918091\n",
      "Final Validation Accuracy: 0.8242591023445129\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.7473 - accuracy: 0.8252\n",
      "Number of layers: 1; Optimizer: RMSP; Batchsize: 64\n",
      "Final Training Loss: 0.06978782266378403\n",
      "Final Training Accuracy: 0.9827645421028137\n",
      "Final Validation Loss: 0.6727607250213623\n",
      "Final Validation Accuracy: 0.8290833830833435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 16:36:57.861289: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1835008000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 57016320/8510832640\n",
      "2024-01-18 16:36:57.861314: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7365263360\n",
      "InUse:                      3932494948\n",
      "MaxInUse:                   6815291700\n",
      "NumAllocs:                      404712\n",
      "MaxAllocSize:               1835008000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-18 16:36:57.861325: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-18 16:36:57.861330: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 30\n",
      "2024-01-18 16:36:57.861334: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 8\n",
      "2024-01-18 16:36:57.861338: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1\n",
      "2024-01-18 16:36:57.861341: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 64, 15\n",
      "2024-01-18 16:36:57.861344: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 2\n",
      "2024-01-18 16:36:57.861347: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 576, 3\n",
      "2024-01-18 16:36:57.861351: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1024, 3\n",
      "2024-01-18 16:36:57.861354: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-18 16:36:57.861357: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2000, 2\n",
      "2024-01-18 16:36:57.861361: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4096, 1\n",
      "2024-01-18 16:36:57.861364: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 3\n",
      "2024-01-18 16:36:57.861367: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 25600, 3\n",
      "2024-01-18 16:36:57.861370: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 36864, 1\n",
      "2024-01-18 16:36:57.861374: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 38000, 2\n",
      "2024-01-18 16:36:57.861377: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 102400, 1\n",
      "2024-01-18 16:36:57.861380: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1048576000, 2\n",
      "2024-01-18 16:36:57.861384: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1835008000, 1\n",
      "2024-01-18 16:36:57.861389: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7348420608\n",
      "2024-01-18 16:36:57.861392: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 3932494948\n",
      "2024-01-18 16:36:57.861396: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7348420608\n",
      "2024-01-18 16:36:57.861399: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6815291700\n",
      "2024-01-18 16:36:57.861416: W tensorflow/core/framework/op_kernel.cc:1816] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(InceptionModule2(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m19\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2101\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2110\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2111\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2115\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for opti_str in ['RMSP','ADAM']:\n",
    "        for layer in range(1,7):\n",
    "            for bs in range(2):\n",
    "                title_appendix = f'CNN_nr_of_inception_layers={layer}_optimizer={opti_str}_batchsize={pow(2,5+bs)}'\n",
    "                # Build the model\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "                for x in range(layer):\n",
    "                    model.add(Dropout(0.2))\n",
    "                    model.add(InceptionModule2(filters=16))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(500, activation='relu'))\n",
    "                model.add(Dense(19, activation='softmax'))\n",
    "        \n",
    "                \n",
    "                optimizer=RMSprop(learning_rate=0.001)\n",
    "                if opti_str == 'ADAM':\n",
    "                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=optimizer,\n",
    "                              metrics=['accuracy'])\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "                history = model.fit(X_train, y_train, epochs=50, batch_size=pow(2,5+bs), verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
    "                \n",
    "                # Evaluate the model\n",
    "                loss, accuracy = model.evaluate(X_test, y_test)\n",
    "                print('Number of layers: %.i; Optimizer: %s; Batchsize: %.i' % (layer, opti_str, pow(2,5+bs)))\n",
    "                print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "                print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "                print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "                print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "                visualize_history(history, f'{title_appendix}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "                \n",
    "                # Clear Keras session\n",
    "                K.clear_session()\n",
    "                del model\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16931f9d-b251-4861-a5f6-55fe80992cd9",
   "metadata": {},
   "source": [
    "Das scheint die Grafikkarte nicht mitzumachen. Daher nochmal für nur einen Layer aber dafür mit Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7cf1ed-b23f-44a1-8565-ba43a2bfeded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: RMSP; Batchsize: 32\n",
      "Epoch 25: early stopping\n",
      "Test loss: 1.5782508850097656, Test accuracy: 0.8332184553146362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 20:10:15.847706: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1048576000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 253820928/8510832640\n",
      "2024-01-18 20:10:15.847752: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7313817600\n",
      "InUse:                      6053400276\n",
      "MaxInUse:                   6053400280\n",
      "NumAllocs:                    51481695\n",
      "MaxAllocSize:               1234183168\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-18 20:10:15.847785: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-18 20:10:15.847798: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 36\n",
      "2024-01-18 20:10:15.847806: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 14\n",
      "2024-01-18 20:10:15.847814: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1\n",
      "2024-01-18 20:10:15.847821: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 64, 49\n",
      "2024-01-18 20:10:15.847828: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 6\n",
      "2024-01-18 20:10:15.847835: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 576, 7\n",
      "2024-01-18 20:10:15.847842: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1024, 4\n",
      "2024-01-18 20:10:15.847850: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-18 20:10:15.847857: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2000, 6\n",
      "2024-01-18 20:10:15.847865: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 10\n",
      "2024-01-18 20:10:15.847872: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 25600, 4\n",
      "2024-01-18 20:10:15.847879: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 38000, 6\n",
      "2024-01-18 20:10:15.847885: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440952, 1\n",
      "2024-01-18 20:10:15.847890: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384000, 3\n",
      "2024-01-18 20:10:15.847894: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760479744, 1\n",
      "2024-01-18 20:10:15.847899: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1048576000, 5\n",
      "2024-01-18 20:10:15.847906: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7281311744\n",
      "2024-01-18 20:10:15.847911: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 6053400276\n",
      "2024-01-18 20:10:15.847916: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-18 20:10:15.847921: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6053400280\n",
      "2024-01-18 20:10:15.847949: W tensorflow/core/framework/op_kernel.cc:1816] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Training with cross-validation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(X_train, y_train):\n\u001b[0;32m---> 32\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopti_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train[train], y_train[train], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m+\u001b[39mbs), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(opti_str)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(InceptionModule2(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m19\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2101\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2110\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2111\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2115\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "def create_model(opti_str):\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(InceptionModule2(filters=16))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    optimizer=RMSprop(learning_rate=0.001)\n",
    "    if opti_str == 'ADAM':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Dropout-Layers after MaxPooling-Layers seem to be a known issue currently: https://github.com/tensorflow/tensorflow/issues/61324\n",
    "# The following line is to suppress the error.\n",
    "with options({\"layout_optimizer\": False}):\n",
    "    for opti_str in ['RMSP','ADAM']:\n",
    "        for bs in range(2):\n",
    "            title_appendix = f'inception_CNN_optimizer={opti_str}_batchsize={pow(2,4+bs)}'\n",
    "            # Evaluate the model\n",
    "            print('Optimizer: %s; Batchsize: %.i' % (opti_str, pow(2,5+bs)))\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            lss = []\n",
    "            acc = []\n",
    "            # Training with cross-validation\n",
    "            for train, test in kfold.split(X_train, y_train):\n",
    "                model = create_model(opti_str)\n",
    "                early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "                model.fit(X_train[train], y_train[train], epochs=50, batch_size=pow(2,4+bs), verbose=0, callbacks=[early_stopping])\n",
    "                score = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "                lss.append(score[0])\n",
    "                acc.append(score[1])\n",
    "                print(f'Test loss: {score[0]}, Test accuracy: {score[1]}')\n",
    "            print(f'Average Model Performance: Loss: {sum(lss)/len(lss)}; Accuracy: {sum(acc)/len(acc)}')\n",
    "            \n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print(f'Validation Performance: Loss: {loss}; Accuracy: {accuracy}')\n",
    "\n",
    "            generate_confusion_matrix(X_test, y_test, title_appendix)\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d69898-fb6e-4234-ae83-ce8e744db00d",
   "metadata": {},
   "source": [
    "hätt ich mir denken können ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42751acd-1bb5-4b4a-8bbc-2309005084f9",
   "metadata": {},
   "source": [
    "## Rumgespiele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e5180d-10bb-4084-895d-7f2fb1ba1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 09:02:32.438164: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-15 09:02:32.438269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-15 09:02:36.384578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-15 09:02:38.257146: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f795287b550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-15 09:02:38.257176: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-15 09:02:38.313372: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-15 09:02:38.650217: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5251 - accuracy: 0.8501\n",
      "Number of additional convolution layers in between: 2 (RMSP)\n",
      "Final Training Loss: 0.02549000270664692\n",
      "Final Training Accuracy: 0.9915516972541809\n",
      "Final Validation Loss: 1.3266136646270752\n",
      "Final Validation Accuracy: 0.8579310178756714\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 2.9403 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 3 (RMSP)\n",
      "Final Training Loss: 2.937068223953247\n",
      "Final Training Accuracy: 0.053793102502822876\n",
      "Final Validation Loss: 2.9395830631256104\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9402 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 4 (RMSP)\n",
      "Final Training Loss: 2.937070608139038\n",
      "Final Training Accuracy: 0.05344827473163605\n",
      "Final Validation Loss: 2.9395265579223633\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9402 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 5 (RMSP)\n",
      "Final Training Loss: 2.9370641708374023\n",
      "Final Training Accuracy: 0.052931033074855804\n",
      "Final Validation Loss: 2.939509630203247\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.8866 - accuracy: 0.8302\n",
      "Number of additional convolution layers in between: 2 (ADAM)\n",
      "Final Training Loss: 0.07068878412246704\n",
      "Final Training Accuracy: 0.9743103384971619\n",
      "Final Validation Loss: 0.9282426238059998\n",
      "Final Validation Accuracy: 0.8372413516044617\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.9656 - accuracy: 0.8401\n",
      "Number of additional convolution layers in between: 3 (ADAM)\n",
      "Final Training Loss: 0.031694453209638596\n",
      "Final Training Accuracy: 0.9891379475593567\n",
      "Final Validation Loss: 0.8907752633094788\n",
      "Final Validation Accuracy: 0.8379310369491577\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8001 - accuracy: 0.8241\n",
      "Number of additional convolution layers in between: 4 (ADAM)\n",
      "Final Training Loss: 0.0650148019194603\n",
      "Final Training Accuracy: 0.9796551465988159\n",
      "Final Validation Loss: 0.8136645555496216\n",
      "Final Validation Accuracy: 0.8186206817626953\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.0824 - accuracy: 0.8341\n",
      "Number of additional convolution layers in between: 5 (ADAM)\n",
      "Final Training Loss: 0.05831792205572128\n",
      "Final Training Accuracy: 0.9784482717514038\n",
      "Final Validation Loss: 0.9478310942649841\n",
      "Final Validation Accuracy: 0.838620662689209\n"
     ]
    }
   ],
   "source": [
    "#del\n",
    "for opti_str in ['RMSP','ADAM']:\n",
    "    title_appendix = f'{opti_str}_e100_d250'\n",
    "    for i in range(2,6):\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        for x in range(5):\n",
    "            for y in range(i):\n",
    "                model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "        # Compile the model\n",
    "        opti = RMSprop(learning_rate=0.001)\n",
    "        if opti_str == 'ADAM':\n",
    "            opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=opti,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=100, batch_size=50, verbose=0, validation_split=0.2)\n",
    "    \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print('Number of additional convolution layers in between: %.i (%s)' % (i, opti_str))\n",
    "        print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "        print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "        print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        visualize_history(history, f'{title_appendix}_sublayercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Clear Keras session\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed4383-0d63-4272-99cf-8927ed922583",
   "metadata": {},
   "source": [
    "Für RMS-Propagation scheint das Model mit mehr als 2 Convolutional Sublayern instabil zu werden (fransiger Plot + unterirdische Genauigkeit).\n",
    "\n",
    "Beim Adam-Optimizer higegen scheint dass erhöhen der Convolutional Sublayer die Performace des Models nicht direkt zu beeinflussen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41187816-77cb-484b-8420-c129ff73f4f5",
   "metadata": {},
   "source": [
    "Kann eine andere Batchsize hier zu besseren Ergebnissen führen? Um Overfitting zu verhindern und Ressourcen zu sparen wird außerdem zusätzlich Early Stopping eingebaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348f95d7-cf6d-4407-9a9e-1a993333c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 11:26:52.340578: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-15 11:26:52.340650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6889 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-15 11:26:54.990222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-15 11:26:55.535946: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f80c9a61280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-15 11:26:55.535967: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-15 11:26:55.539887: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-15 11:26:55.632802: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5720 - accuracy: 0.8037\n",
      "Number of additional convolution layers in between: 1 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.4000765383243561\n",
      "Final Training Accuracy: 0.8641379475593567\n",
      "Final Validation Loss: 0.5638492703437805\n",
      "Final Validation Accuracy: 0.8103448152542114\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 2.9412 - accuracy: 0.0419\n",
      "Number of additional convolution layers in between: 2 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 2.9378814697265625\n",
      "Final Training Accuracy: 0.05206896737217903\n",
      "Final Validation Loss: 2.939415454864502\n",
      "Final Validation Accuracy: 0.06275861710309982\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9409 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 3 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 2.938035011291504\n",
      "Final Training Accuracy: 0.05000000074505806\n",
      "Final Validation Loss: 2.939582109451294\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9411 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 4 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 2.9379093647003174\n",
      "Final Training Accuracy: 0.05034482851624489\n",
      "Final Validation Loss: 2.939556121826172\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9411 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 5 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 2.9379260540008545\n",
      "Final Training Accuracy: 0.05275861918926239\n",
      "Final Validation Loss: 2.9394493103027344\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 13: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5631 - accuracy: 0.8225\n",
      "Number of additional convolution layers in between: 1 (RMSP, batchsize: 10)\n",
      "Final Training Loss: 0.24396353960037231\n",
      "Final Training Accuracy: 0.9108620882034302\n",
      "Final Validation Loss: 0.5485295653343201\n",
      "Final Validation Accuracy: 0.8358620405197144\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.7115 - accuracy: 0.8434\n",
      "Number of additional convolution layers in between: 2 (RMSP, batchsize: 10)\n",
      "Final Training Loss: 0.2329184114933014\n",
      "Final Training Accuracy: 0.9222413897514343\n",
      "Final Validation Loss: 0.5776548385620117\n",
      "Final Validation Accuracy: 0.839310348033905\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9407 - accuracy: 0.0419\n",
      "Number of additional convolution layers in between: 3 (RMSP, batchsize: 10)\n",
      "Final Training Loss: 2.937790632247925\n",
      "Final Training Accuracy: 0.04965517297387123\n",
      "Final Validation Loss: 2.9394538402557373\n",
      "Final Validation Accuracy: 0.06275861710309982\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9406 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 4 (RMSP, batchsize: 10)\n",
      "Final Training Loss: 2.9377987384796143\n",
      "Final Training Accuracy: 0.05000000074505806\n",
      "Final Validation Loss: 2.939518690109253\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 8: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9407 - accuracy: 0.0540\n",
      "Number of additional convolution layers in between: 5 (RMSP, batchsize: 10)\n",
      "Final Training Loss: 2.93789005279541\n",
      "Final Training Accuracy: 0.04793103411793709\n",
      "Final Validation Loss: 2.9397027492523193\n",
      "Final Validation Accuracy: 0.0468965508043766\n",
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.6460 - accuracy: 0.8495\n",
      "Number of additional convolution layers in between: 1 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.12930789589881897\n",
      "Final Training Accuracy: 0.9534482955932617\n",
      "Final Validation Loss: 0.6144660115242004\n",
      "Final Validation Accuracy: 0.8551723957061768\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5406 - accuracy: 0.8523\n",
      "Number of additional convolution layers in between: 2 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.16435062885284424\n",
      "Final Training Accuracy: 0.9437931180000305\n",
      "Final Validation Loss: 0.5295000076293945\n",
      "Final Validation Accuracy: 0.8572413921356201\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9404 - accuracy: 0.0419\n",
      "Number of additional convolution layers in between: 3 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 2.9377684593200684\n",
      "Final Training Accuracy: 0.04827586188912392\n",
      "Final Validation Loss: 2.9392342567443848\n",
      "Final Validation Accuracy: 0.06275861710309982\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9404 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 4 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 2.9377615451812744\n",
      "Final Training Accuracy: 0.04758620634675026\n",
      "Final Validation Loss: 2.939375400543213\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9404 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 5 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 2.937713146209717\n",
      "Final Training Accuracy: 0.05155172571539879\n",
      "Final Validation Loss: 2.939263105392456\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.6119 - accuracy: 0.8401\n",
      "Number of additional convolution layers in between: 1 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.13958145678043365\n",
      "Final Training Accuracy: 0.9481034278869629\n",
      "Final Validation Loss: 0.5385950803756714\n",
      "Final Validation Accuracy: 0.8379310369491577\n",
      "Epoch 23: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.6265 - accuracy: 0.8363\n",
      "Number of additional convolution layers in between: 2 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.14491823315620422\n",
      "Final Training Accuracy: 0.9458620548248291\n",
      "Final Validation Loss: 0.5597926378250122\n",
      "Final Validation Accuracy: 0.8406896591186523\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5609 - accuracy: 0.8264\n",
      "Number of additional convolution layers in between: 3 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.22881732881069183\n",
      "Final Training Accuracy: 0.9129310250282288\n",
      "Final Validation Loss: 0.5572965741157532\n",
      "Final Validation Accuracy: 0.8206896781921387\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9403 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 4 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 2.9376189708709717\n",
      "Final Training Accuracy: 0.051034484058618546\n",
      "Final Validation Loss: 2.939444065093994\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9403 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 5 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 2.9377124309539795\n",
      "Final Training Accuracy: 0.05068965628743172\n",
      "Final Validation Loss: 2.9393908977508545\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.6658 - accuracy: 0.8203\n",
      "Number of additional convolution layers in between: 1 (RMSP, batchsize: 100)\n",
      "Final Training Loss: 0.22784081101417542\n",
      "Final Training Accuracy: 0.9148275852203369\n",
      "Final Validation Loss: 0.5703153014183044\n",
      "Final Validation Accuracy: 0.8151724338531494\n",
      "Epoch 23: early stopping\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.6010 - accuracy: 0.8313\n",
      "Number of additional convolution layers in between: 2 (RMSP, batchsize: 100)\n",
      "Final Training Loss: 0.2088249921798706\n",
      "Final Training Accuracy: 0.9220689535140991\n",
      "Final Validation Loss: 0.49771034717559814\n",
      "Final Validation Accuracy: 0.8468965291976929\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 2.9401 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 3 (RMSP, batchsize: 100)\n",
      "Final Training Loss: 2.937220811843872\n",
      "Final Training Accuracy: 0.052931033074855804\n",
      "Final Validation Loss: 2.9393932819366455\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 11: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 2.9402 - accuracy: 0.0485\n",
      "Number of additional convolution layers in between: 4 (RMSP, batchsize: 100)\n",
      "Final Training Loss: 2.9377691745758057\n",
      "Final Training Accuracy: 0.05000000074505806\n",
      "Final Validation Loss: 2.939478635787964\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9401 - accuracy: 0.0496\n",
      "Number of additional convolution layers in between: 5 (RMSP, batchsize: 100)\n",
      "Final Training Loss: 2.9375970363616943\n",
      "Final Training Accuracy: 0.05206896737217903\n",
      "Final Validation Loss: 2.939595937728882\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Epoch 33: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5408 - accuracy: 0.8319\n",
      "Number of additional convolution layers in between: 1 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.19623006880283356\n",
      "Final Training Accuracy: 0.9286206960678101\n",
      "Final Validation Loss: 0.5735440850257874\n",
      "Final Validation Accuracy: 0.8331034779548645\n",
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.5060 - accuracy: 0.8401\n",
      "Number of additional convolution layers in between: 2 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.22000664472579956\n",
      "Final Training Accuracy: 0.9179310202598572\n",
      "Final Validation Loss: 0.49796876311302185\n",
      "Final Validation Accuracy: 0.8462069034576416\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6535 - accuracy: 0.8434\n",
      "Number of additional convolution layers in between: 3 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.1592620462179184\n",
      "Final Training Accuracy: 0.9434483051300049\n",
      "Final Validation Loss: 0.5271924138069153\n",
      "Final Validation Accuracy: 0.8496551513671875\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5979 - accuracy: 0.8054\n",
      "Number of additional convolution layers in between: 4 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.26602622866630554\n",
      "Final Training Accuracy: 0.9029310345649719\n",
      "Final Validation Loss: 0.525749683380127\n",
      "Final Validation Accuracy: 0.8296551704406738\n",
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9401 - accuracy: 0.0540\n",
      "Number of additional convolution layers in between: 5 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 2.9376893043518066\n",
      "Final Training Accuracy: 0.048620689660310745\n",
      "Final Validation Loss: 2.9394218921661377\n",
      "Final Validation Accuracy: 0.0468965508043766\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5493 - accuracy: 0.8308\n",
      "Number of additional convolution layers in between: 1 (ADAM, batchsize: 10)\n",
      "Final Training Loss: 0.29005831480026245\n",
      "Final Training Accuracy: 0.8931034207344055\n",
      "Final Validation Loss: 0.5641157031059265\n",
      "Final Validation Accuracy: 0.8144827485084534\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.6073 - accuracy: 0.8071\n",
      "Number of additional convolution layers in between: 2 (ADAM, batchsize: 10)\n",
      "Final Training Loss: 0.34180930256843567\n",
      "Final Training Accuracy: 0.8770689368247986\n",
      "Final Validation Loss: 0.545391321182251\n",
      "Final Validation Accuracy: 0.8199999928474426\n",
      "Epoch 23: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5914 - accuracy: 0.8258\n",
      "Number of additional convolution layers in between: 3 (ADAM, batchsize: 10)\n",
      "Final Training Loss: 0.2239580899477005\n",
      "Final Training Accuracy: 0.9165517091751099\n",
      "Final Validation Loss: 0.5160976648330688\n",
      "Final Validation Accuracy: 0.8303448557853699\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6839 - accuracy: 0.8186\n",
      "Number of additional convolution layers in between: 4 (ADAM, batchsize: 10)\n",
      "Final Training Loss: 0.21274256706237793\n",
      "Final Training Accuracy: 0.9177586436271667\n",
      "Final Validation Loss: 0.6165984869003296\n",
      "Final Validation Accuracy: 0.822068989276886\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.6404 - accuracy: 0.8115\n",
      "Number of additional convolution layers in between: 5 (ADAM, batchsize: 10)\n",
      "Final Training Loss: 0.2970540523529053\n",
      "Final Training Accuracy: 0.886896550655365\n",
      "Final Validation Loss: 0.5928335785865784\n",
      "Final Validation Accuracy: 0.817241370677948\n",
      "Epoch 42: early stopping\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.5937 - accuracy: 0.8142\n",
      "Number of additional convolution layers in between: 1 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.3045993745326996\n",
      "Final Training Accuracy: 0.8924137949943542\n",
      "Final Validation Loss: 0.5701876878738403\n",
      "Final Validation Accuracy: 0.8110345005989075\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.5982 - accuracy: 0.8032\n",
      "Number of additional convolution layers in between: 2 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.3421308100223541\n",
      "Final Training Accuracy: 0.8772413730621338\n",
      "Final Validation Loss: 0.585189163684845\n",
      "Final Validation Accuracy: 0.8055172562599182\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7281 - accuracy: 0.7949\n",
      "Number of additional convolution layers in between: 3 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.2778193950653076\n",
      "Final Training Accuracy: 0.9010344743728638\n",
      "Final Validation Loss: 0.7100542187690735\n",
      "Final Validation Accuracy: 0.7848275899887085\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5740 - accuracy: 0.8186\n",
      "Number of additional convolution layers in between: 4 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.3355327248573303\n",
      "Final Training Accuracy: 0.8820689916610718\n",
      "Final Validation Loss: 0.5484637022018433\n",
      "Final Validation Accuracy: 0.8137931227684021\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.7054 - accuracy: 0.7729\n",
      "Number of additional convolution layers in between: 5 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.3622906804084778\n",
      "Final Training Accuracy: 0.867241382598877\n",
      "Final Validation Loss: 0.6356187462806702\n",
      "Final Validation Accuracy: 0.7931034564971924\n",
      "Epoch 47: early stopping\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.6195 - accuracy: 0.7900\n",
      "Number of additional convolution layers in between: 1 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.46042051911354065\n",
      "Final Training Accuracy: 0.8415517210960388\n",
      "Final Validation Loss: 0.5823294520378113\n",
      "Final Validation Accuracy: 0.7986207008361816\n",
      "Epoch 47: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.6365 - accuracy: 0.8087\n",
      "Number of additional convolution layers in between: 2 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.2374497801065445\n",
      "Final Training Accuracy: 0.9120689630508423\n",
      "Final Validation Loss: 0.5568759441375732\n",
      "Final Validation Accuracy: 0.8358620405197144\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.6191 - accuracy: 0.8076\n",
      "Number of additional convolution layers in between: 3 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.3686636984348297\n",
      "Final Training Accuracy: 0.8660345077514648\n",
      "Final Validation Loss: 0.567862331867218\n",
      "Final Validation Accuracy: 0.8089655041694641\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5955 - accuracy: 0.7944\n",
      "Number of additional convolution layers in between: 4 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.3525010943412781\n",
      "Final Training Accuracy: 0.8755172491073608\n",
      "Final Validation Loss: 0.5610929727554321\n",
      "Final Validation Accuracy: 0.8193103671073914\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.7019 - accuracy: 0.7789\n",
      "Number of additional convolution layers in between: 5 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.37775588035583496\n",
      "Final Training Accuracy: 0.8627586364746094\n",
      "Final Validation Loss: 0.6204414367675781\n",
      "Final Validation Accuracy: 0.7924137711524963\n",
      "Epoch 62: early stopping\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.7147 - accuracy: 0.7541\n",
      "Number of additional convolution layers in between: 1 (ADAM, batchsize: 100)\n",
      "Final Training Loss: 0.5553405284881592\n",
      "Final Training Accuracy: 0.8093103170394897\n",
      "Final Validation Loss: 0.6688649654388428\n",
      "Final Validation Accuracy: 0.7634482979774475\n",
      "Epoch 42: early stopping\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.6046 - accuracy: 0.7993\n",
      "Number of additional convolution layers in between: 2 (ADAM, batchsize: 100)\n",
      "Final Training Loss: 0.406250536441803\n",
      "Final Training Accuracy: 0.855344831943512\n",
      "Final Validation Loss: 0.5781531929969788\n",
      "Final Validation Accuracy: 0.800000011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 13:53:33.632860: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 419430400 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 175439872/8510832640\n",
      "2024-01-15 13:53:33.632915: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7223967744\n",
      "InUse:                      6364151756\n",
      "MaxInUse:                   6965338016\n",
      "NumAllocs:                    88047801\n",
      "MaxAllocSize:               2248708096\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-15 13:53:33.632969: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-15 13:53:33.632983: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 59\n",
      "2024-01-15 13:53:33.632995: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 28\n",
      "2024-01-15 13:53:33.633006: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 21\n",
      "2024-01-15 13:53:33.633016: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 322\n",
      "2024-01-15 13:53:33.633027: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 400, 1\n",
      "2024-01-15 13:53:33.633037: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 21\n",
      "2024-01-15 13:53:33.633047: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-15 13:53:33.633057: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1152, 21\n",
      "2024-01-15 13:53:33.633066: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 21\n",
      "2024-01-15 13:53:33.633077: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 36864, 301\n",
      "2024-01-15 13:53:33.633087: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 110200, 6\n",
      "2024-01-15 13:53:33.633096: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256000, 21\n",
      "2024-01-15 13:53:33.633117: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 6\n",
      "2024-01-15 13:53:33.633126: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 13107200, 1\n",
      "2024-01-15 13:53:33.633136: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 104857600, 2\n",
      "2024-01-15 13:53:33.633145: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 190054400, 6\n",
      "2024-01-15 13:53:33.633155: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 419430400, 1\n",
      "2024-01-15 13:53:33.633165: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 6\n",
      "2024-01-15 13:53:33.633178: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7214202880\n",
      "2024-01-15 13:53:33.633189: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 6364151756\n",
      "2024-01-15 13:53:33.633200: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7314866176\n",
      "2024-01-15 13:53:33.633211: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6965338016\n",
      "2024-01-15 13:53:33.633238: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at pooling_ops_common.cc:856 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[100,32,256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_28615/1778880752.py\", line 30, in <module>\n      history = model.fit(X_train, y_train,\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1084, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad'\nOOM when allocating tensor with shape[100,32,256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0\n\t [[{{node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2179997]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_28615/1778880752.py\", line 30, in <module>\n      history = model.fit(X_train, y_train,\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1084, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/firefly-rgb/.local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad'\nOOM when allocating tensor with shape[100,32,256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0\n\t [[{{node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2179997]"
     ]
    }
   ],
   "source": [
    "for opti_str in ['RMSP','ADAM']:\n",
    "    for bs in [5, 10, 25, 50, 100 ]:\n",
    "        title_appendix = f'{opti_str}_ bs{bs}_e100_d250'\n",
    "        for i in range(1,6):\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(5):\n",
    "                for y in range(i):\n",
    "                    model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(250, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "        \n",
    "            # Compile the model\n",
    "            opti = RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=opti,\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "            # Fit the model\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, \n",
    "                                epochs=100, \n",
    "                                batch_size=bs, \n",
    "                                verbose=0, \n",
    "                                validation_split=0.2, \n",
    "                                callbacks=[early_stopping])\n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of additional convolution layers in between: %.i (%s, batchsize: %.i)' % (i, opti_str, bs))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'{title_appendix}_sublayercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "            \n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c18057-da62-476b-bdad-51ea741d30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 19:01:49.310907: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-15 19:01:49.310971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6866 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-15 19:01:51.814664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-15 19:01:52.339485: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f05191e7500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-15 19:01:52.339510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-15 19:01:52.343442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-15 19:01:52.431046: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: early stopping\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 1.7996 - accuracy: 0.7845\n",
      "Number of inception layers: 1 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.017035244032740593\n",
      "Final Training Accuracy: 0.9950000047683716\n",
      "Final Validation Loss: 1.8507038354873657\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Epoch 12: early stopping\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 1.9841 - accuracy: 0.8104\n",
      "Number of inception layers: 2 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.02317635715007782\n",
      "Final Training Accuracy: 0.9929310083389282\n",
      "Final Validation Loss: 1.8488867282867432\n",
      "Final Validation Accuracy: 0.8227586150169373\n",
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.6534 - accuracy: 0.8247\n",
      "Number of inception layers: 3 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.05556110665202141\n",
      "Final Training Accuracy: 0.9837930798530579\n",
      "Final Validation Loss: 1.4771275520324707\n",
      "Final Validation Accuracy: 0.8282758593559265\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.7967 - accuracy: 0.8644\n",
      "Number of inception layers: 4 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.11330492794513702\n",
      "Final Training Accuracy: 0.964310348033905\n",
      "Final Validation Loss: 0.8341147899627686\n",
      "Final Validation Accuracy: 0.8482758402824402\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 1.1648 - accuracy: 0.7580\n",
      "Number of inception layers: 5 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.3665923476219177\n",
      "Final Training Accuracy: 0.8843103647232056\n",
      "Final Validation Loss: 0.9826512932777405\n",
      "Final Validation Accuracy: 0.7634482979774475\n",
      "Epoch 12: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 1.0050 - accuracy: 0.8071\n",
      "Number of inception layers: 1 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.021674569696187973\n",
      "Final Training Accuracy: 0.9937931299209595\n",
      "Final Validation Loss: 1.0313470363616943\n",
      "Final Validation Accuracy: 0.7979310154914856\n",
      "Epoch 12: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.8124 - accuracy: 0.8572\n",
      "Number of inception layers: 2 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.0309658944606781\n",
      "Final Training Accuracy: 0.9917241334915161\n",
      "Final Validation Loss: 0.8589001297950745\n",
      "Final Validation Accuracy: 0.8462069034576416\n",
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.7923 - accuracy: 0.8693\n",
      "Number of inception layers: 3 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.03394568711519241\n",
      "Final Training Accuracy: 0.988103449344635\n",
      "Final Validation Loss: 0.7517362833023071\n",
      "Final Validation Accuracy: 0.865517258644104\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.6619 - accuracy: 0.8600\n",
      "Number of inception layers: 4 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.05688304454088211\n",
      "Final Training Accuracy: 0.9800000190734863\n",
      "Final Validation Loss: 0.6116185188293457\n",
      "Final Validation Accuracy: 0.8600000143051147\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.5571 - accuracy: 0.8798\n",
      "Number of inception layers: 5 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.11165802925825119\n",
      "Final Training Accuracy: 0.9613792896270752\n",
      "Final Validation Loss: 0.5196184515953064\n",
      "Final Validation Accuracy: 0.8668965697288513\n",
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.9848 - accuracy: 0.7745\n",
      "Number of inception layers: 1 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.04241448640823364\n",
      "Final Training Accuracy: 0.9939655065536499\n",
      "Final Validation Loss: 0.9399096965789795\n",
      "Final Validation Accuracy: 0.7689655423164368\n",
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.8023 - accuracy: 0.8313\n",
      "Number of inception layers: 2 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.04167532920837402\n",
      "Final Training Accuracy: 0.9894827604293823\n",
      "Final Validation Loss: 0.748903751373291\n",
      "Final Validation Accuracy: 0.8399999737739563\n",
      "Epoch 14: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.7955 - accuracy: 0.8512\n",
      "Number of inception layers: 3 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.03276493027806282\n",
      "Final Training Accuracy: 0.9900000095367432\n",
      "Final Validation Loss: 0.8155484199523926\n",
      "Final Validation Accuracy: 0.8427585959434509\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.5150 - accuracy: 0.8578\n",
      "Number of inception layers: 4 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.09253179281949997\n",
      "Final Training Accuracy: 0.966724157333374\n",
      "Final Validation Loss: 0.49019476771354675\n",
      "Final Validation Accuracy: 0.8627586364746094\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.6048 - accuracy: 0.8655\n",
      "Number of inception layers: 5 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.05284413695335388\n",
      "Final Training Accuracy: 0.9820689558982849\n",
      "Final Validation Loss: 0.58658367395401\n",
      "Final Validation Accuracy: 0.8662068843841553\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.6100 - accuracy: 0.8186\n",
      "Number of inception layers: 1 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.06944885849952698\n",
      "Final Training Accuracy: 0.9918965697288513\n",
      "Final Validation Loss: 0.6482841372489929\n",
      "Final Validation Accuracy: 0.8020689487457275\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.6282 - accuracy: 0.8275\n",
      "Number of inception layers: 2 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.08393058180809021\n",
      "Final Training Accuracy: 0.9791379570960999\n",
      "Final Validation Loss: 0.594811201095581\n",
      "Final Validation Accuracy: 0.8324137926101685\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.5762 - accuracy: 0.8407\n",
      "Number of inception layers: 3 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.12208041548728943\n",
      "Final Training Accuracy: 0.9598276019096375\n",
      "Final Validation Loss: 0.5424432754516602\n",
      "Final Validation Accuracy: 0.8517241477966309\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.5611 - accuracy: 0.8655\n",
      "Number of inception layers: 4 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.1014336496591568\n",
      "Final Training Accuracy: 0.9681034684181213\n",
      "Final Validation Loss: 0.5407701134681702\n",
      "Final Validation Accuracy: 0.8634482622146606\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.5278 - accuracy: 0.8578\n",
      "Number of inception layers: 5 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.10069594532251358\n",
      "Final Training Accuracy: 0.9637930989265442\n",
      "Final Validation Loss: 0.4965728223323822\n",
      "Final Validation Accuracy: 0.8579310178756714\n"
     ]
    }
   ],
   "source": [
    "# Varrie the number of layers\n",
    "# convolutional filters: 32; epochs: 100; density: 250, rms_propagation learning rate:\n",
    "for opti_str in ['RMSP','ADAM']:\n",
    "    for bs in [5, 25]:\n",
    "        title_appendix = f'{opti_str}_ bs{bs}_e100_d250'\n",
    "        for i in range(1,6):\n",
    "            # Build the model\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            for x in range(i):\n",
    "                model.add(InceptionModule(filters=32))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(250, activation='relu'))\n",
    "            model.add(Dense(19, activation='softmax'))\n",
    "        \n",
    "            # Compile the model\n",
    "            opti = RMSprop(learning_rate=0.001)\n",
    "            if opti_str == 'ADAM':\n",
    "                opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=opti,\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "            # Fit the model\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=1, mode='auto')\n",
    "            history = model.fit(X_train, y_train, \n",
    "                                epochs=100, \n",
    "                                batch_size=bs, \n",
    "                                verbose=0, \n",
    "                                validation_split=0.2, \n",
    "                                callbacks=[early_stopping])\n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "            print('Number of inception layers: %.i (%s, batchsize: %.i)' % (i, opti_str, bs))\n",
    "            print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "            print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "            print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "            print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "            visualize_history(history, f'inception_{title_appendix}_sublayercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "\n",
    "            # Clear Keras session\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674f2621-4105-4763-b921-00ed3d7ddbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 09:55:33.299535: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-16 09:55:33.299599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7020 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-16 09:55:35.776631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-16 09:55:36.280123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc7fc02d0b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 09:55:36.280145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-16 09:55:36.283985: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-16 09:55:36.370591: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 1.6217 - accuracy: 0.8418\n",
      "Number of inception layers: 1 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 2.764451437542448e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6036280393600464\n",
      "Final Validation Accuracy: 0.839310348033905\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.0767 - accuracy: 0.8390\n",
      "Number of inception layers: 2 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.013805396854877472\n",
      "Final Training Accuracy: 0.997586190700531\n",
      "Final Validation Loss: 1.9938232898712158\n",
      "Final Validation Accuracy: 0.838620662689209\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.5897 - accuracy: 0.8286\n",
      "Number of inception layers: 3 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.04686996340751648\n",
      "Final Training Accuracy: 0.9891379475593567\n",
      "Final Validation Loss: 1.4806262254714966\n",
      "Final Validation Accuracy: 0.8413792848587036\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.9859 - accuracy: 0.8126\n",
      "Number of inception layers: 4 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.14658603072166443\n",
      "Final Training Accuracy: 0.9536206722259521\n",
      "Final Validation Loss: 0.8594504594802856\n",
      "Final Validation Accuracy: 0.8262069225311279\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.6391 - accuracy: 0.8049\n",
      "Number of inception layers: 5 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.3373463451862335\n",
      "Final Training Accuracy: 0.8917241096496582\n",
      "Final Validation Loss: 0.5683172941207886\n",
      "Final Validation Accuracy: 0.8317241668701172\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.8811 - accuracy: 0.8280\n",
      "Number of inception layers: 1 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.02146887592971325\n",
      "Final Training Accuracy: 0.994655191898346\n",
      "Final Validation Loss: 0.8470926880836487\n",
      "Final Validation Accuracy: 0.8337931036949158\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.0206 - accuracy: 0.8462\n",
      "Number of inception layers: 2 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.021923882886767387\n",
      "Final Training Accuracy: 0.994655191898346\n",
      "Final Validation Loss: 0.987531304359436\n",
      "Final Validation Accuracy: 0.8337931036949158\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.0629 - accuracy: 0.8451\n",
      "Number of inception layers: 3 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.025728551670908928\n",
      "Final Training Accuracy: 0.9917241334915161\n",
      "Final Validation Loss: 0.9741343855857849\n",
      "Final Validation Accuracy: 0.8489655256271362\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.8403 - accuracy: 0.8479\n",
      "Number of inception layers: 4 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.051185667514801025\n",
      "Final Training Accuracy: 0.9832758903503418\n",
      "Final Validation Loss: 0.8573512434959412\n",
      "Final Validation Accuracy: 0.8600000143051147\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.8225 - accuracy: 0.8473\n",
      "Number of inception layers: 5 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.06691379845142365\n",
      "Final Training Accuracy: 0.9770689606666565\n",
      "Final Validation Loss: 0.7821918725967407\n",
      "Final Validation Accuracy: 0.8427585959434509\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 1.4504 - accuracy: 0.7536\n",
      "Number of inception layers: 1 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.010335440747439861\n",
      "Final Training Accuracy: 0.9984482526779175\n",
      "Final Validation Loss: 1.4028143882751465\n",
      "Final Validation Accuracy: 0.7634482979774475\n",
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8839 - accuracy: 0.8506\n",
      "Number of inception layers: 2 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.012631249614059925\n",
      "Final Training Accuracy: 0.9965517520904541\n",
      "Final Validation Loss: 0.7974684238433838\n",
      "Final Validation Accuracy: 0.8544827699661255\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.7619 - accuracy: 0.8649\n",
      "Number of inception layers: 3 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.031949732452631\n",
      "Final Training Accuracy: 0.9908620715141296\n",
      "Final Validation Loss: 0.7623605728149414\n",
      "Final Validation Accuracy: 0.8668965697288513\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.6250 - accuracy: 0.8627\n",
      "Number of inception layers: 4 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.0672222301363945\n",
      "Final Training Accuracy: 0.9768965244293213\n",
      "Final Validation Loss: 0.6104110479354858\n",
      "Final Validation Accuracy: 0.8565517067909241\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.0496 - accuracy: 0.8275\n",
      "Number of inception layers: 5 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.07033362239599228\n",
      "Final Training Accuracy: 0.9760344624519348\n",
      "Final Validation Loss: 0.9283121228218079\n",
      "Final Validation Accuracy: 0.8413792848587036\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 3s 34ms/step - loss: 1.6883 - accuracy: 0.7767\n",
      "Number of inception layers: 1 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.034521885216236115\n",
      "Final Training Accuracy: 0.9927586317062378\n",
      "Final Validation Loss: 1.7014156579971313\n",
      "Final Validation Accuracy: 0.7779310345649719\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 1.9631 - accuracy: 0.8434\n",
      "Number of inception layers: 2 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.019730279222130775\n",
      "Final Training Accuracy: 0.9955172538757324\n",
      "Final Validation Loss: 2.0883235931396484\n",
      "Final Validation Accuracy: 0.8358620405197144\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 1.1794 - accuracy: 0.8445\n",
      "Number of inception layers: 3 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.05763247236609459\n",
      "Final Training Accuracy: 0.982758641242981\n",
      "Final Validation Loss: 1.1127879619598389\n",
      "Final Validation Accuracy: 0.8475862145423889\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 1.4943 - accuracy: 0.8644\n",
      "Number of inception layers: 4 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.1019328162074089\n",
      "Final Training Accuracy: 0.9768965244293213\n",
      "Final Validation Loss: 1.5110210180282593\n",
      "Final Validation Accuracy: 0.8572413921356201\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 1.8436 - accuracy: 0.8462\n",
      "Number of inception layers: 5 (RMSP, batchsize: 5)\n",
      "Final Training Loss: 0.29740437865257263\n",
      "Final Training Accuracy: 0.918448269367218\n",
      "Final Validation Loss: 1.8472342491149902\n",
      "Final Validation Accuracy: 0.8427585959434509\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 2.0701 - accuracy: 0.7448\n",
      "Number of inception layers: 1 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.01804344914853573\n",
      "Final Training Accuracy: 0.9958620667457581\n",
      "Final Validation Loss: 1.9289171695709229\n",
      "Final Validation Accuracy: 0.7599999904632568\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.9718 - accuracy: 0.8335\n",
      "Number of inception layers: 2 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.019667640328407288\n",
      "Final Training Accuracy: 0.9943103194236755\n",
      "Final Validation Loss: 0.9214721918106079\n",
      "Final Validation Accuracy: 0.8399999737739563\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8529 - accuracy: 0.8412\n",
      "Number of inception layers: 3 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.024459058418869972\n",
      "Final Training Accuracy: 0.9929310083389282\n",
      "Final Validation Loss: 0.7987802028656006\n",
      "Final Validation Accuracy: 0.8558620810508728\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8587 - accuracy: 0.8666\n",
      "Number of inception layers: 4 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.04056617617607117\n",
      "Final Training Accuracy: 0.9875862002372742\n",
      "Final Validation Loss: 0.8351250886917114\n",
      "Final Validation Accuracy: 0.8558620810508728\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8609 - accuracy: 0.8782\n",
      "Number of inception layers: 5 (RMSP, batchsize: 25)\n",
      "Final Training Loss: 0.05540398508310318\n",
      "Final Training Accuracy: 0.9825862050056458\n",
      "Final Validation Loss: 0.8337509036064148\n",
      "Final Validation Accuracy: 0.865517258644104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 11:37:38.742880: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 3872276480 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 172163072/8510832640\n",
      "2024-01-16 11:37:38.742907: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7361527808\n",
      "InUse:                      2533783028\n",
      "MaxInUse:                   6300677620\n",
      "NumAllocs:                    42541048\n",
      "MaxAllocSize:               3872014336\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 11:37:38.742922: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 11:37:38.742932: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 56\n",
      "2024-01-16 11:37:38.742936: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 10\n",
      "2024-01-16 11:37:38.742939: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 11:37:38.742942: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 11:37:38.742945: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 40\n",
      "2024-01-16 11:37:38.742948: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 400, 1\n",
      "2024-01-16 11:37:38.742951: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 11:37:38.742954: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 11:37:38.742956: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 4\n",
      "2024-01-16 11:37:38.742959: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3800, 1\n",
      "2024-01-16 11:37:38.742962: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 4\n",
      "2024-01-16 11:37:38.742965: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 11:37:38.742968: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 49152, 8\n",
      "2024-01-16 11:37:38.742971: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 4\n",
      "2024-01-16 11:37:38.742974: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 5\n",
      "2024-01-16 11:37:38.742977: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 11:37:38.742980: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 442368, 8\n",
      "2024-01-16 11:37:38.742982: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1228800, 8\n",
      "2024-01-16 11:37:38.742985: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1536000, 2\n",
      "2024-01-16 11:37:38.742988: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 11:37:38.742991: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 104857600, 4\n",
      "2024-01-16 11:37:38.742994: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 121634816, 1\n",
      "2024-01-16 11:37:38.742997: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 393216000, 2\n",
      "2024-01-16 11:37:38.743000: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 419430400, 1\n",
      "2024-01-16 11:37:38.743002: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 11:37:38.743008: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7348420608\n",
      "2024-01-16 11:37:38.743011: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2533783028\n",
      "2024-01-16 11:37:38.743015: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 11:37:38.743018: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6300677620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.8372 - accuracy: 0.8225\n",
      "Number of inception layers: 1 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.030646998435258865\n",
      "Final Training Accuracy: 0.9948275685310364\n",
      "Final Validation Loss: 0.7750986814498901\n",
      "Final Validation Accuracy: 0.8289655447006226\n",
      "Epoch 19: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.1205 - accuracy: 0.8396\n",
      "Number of inception layers: 2 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.016368580982089043\n",
      "Final Training Accuracy: 0.994655191898346\n",
      "Final Validation Loss: 1.0896378755569458\n",
      "Final Validation Accuracy: 0.8303448557853699\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.0342 - accuracy: 0.8506\n",
      "Number of inception layers: 3 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.025272224098443985\n",
      "Final Training Accuracy: 0.9932758808135986\n",
      "Final Validation Loss: 0.8827308416366577\n",
      "Final Validation Accuracy: 0.8537930846214294\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6041 - accuracy: 0.8771\n",
      "Number of inception layers: 4 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.04839091747999191\n",
      "Final Training Accuracy: 0.9860345125198364\n",
      "Final Validation Loss: 0.5875909924507141\n",
      "Final Validation Accuracy: 0.8744827508926392\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5966 - accuracy: 0.8831\n",
      "Number of inception layers: 5 (RMSP, batchsize: 50)\n",
      "Final Training Loss: 0.057620011270046234\n",
      "Final Training Accuracy: 0.9843103289604187\n",
      "Final Validation Loss: 0.592026948928833\n",
      "Final Validation Accuracy: 0.8689655065536499\n",
      "Epoch 30: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.9378 - accuracy: 0.7949\n",
      "Number of inception layers: 1 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.017838256433606148\n",
      "Final Training Accuracy: 0.9972413778305054\n",
      "Final Validation Loss: 0.9019713997840881\n",
      "Final Validation Accuracy: 0.8041379451751709\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.9083 - accuracy: 0.8313\n",
      "Number of inception layers: 2 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.0048604197800159454\n",
      "Final Training Accuracy: 0.9998275637626648\n",
      "Final Validation Loss: 0.789341390132904\n",
      "Final Validation Accuracy: 0.8372413516044617\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.8213 - accuracy: 0.8523\n",
      "Number of inception layers: 3 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.0027783329132944345\n",
      "Final Training Accuracy: 0.9996551871299744\n",
      "Final Validation Loss: 0.7467943429946899\n",
      "Final Validation Accuracy: 0.8586207032203674\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.7065 - accuracy: 0.8429\n",
      "Number of inception layers: 4 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.05354785919189453\n",
      "Final Training Accuracy: 0.9844827651977539\n",
      "Final Validation Loss: 0.6861289739608765\n",
      "Final Validation Accuracy: 0.834482729434967\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.6034 - accuracy: 0.8418\n",
      "Number of inception layers: 5 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.11437029391527176\n",
      "Final Training Accuracy: 0.9584482908248901\n",
      "Final Validation Loss: 0.5548103451728821\n",
      "Final Validation Accuracy: 0.8524137735366821\n",
      "Epoch 44: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.7112 - accuracy: 0.8082\n",
      "Number of inception layers: 1 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.0379146970808506\n",
      "Final Training Accuracy: 0.9987931251525879\n",
      "Final Validation Loss: 0.7056901454925537\n",
      "Final Validation Accuracy: 0.8062068819999695\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.7565 - accuracy: 0.8186\n",
      "Number of inception layers: 2 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.045466601848602295\n",
      "Final Training Accuracy: 0.9910345077514648\n",
      "Final Validation Loss: 0.6859081387519836\n",
      "Final Validation Accuracy: 0.8365517258644104\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5910 - accuracy: 0.8484\n",
      "Number of inception layers: 3 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.11168460547924042\n",
      "Final Training Accuracy: 0.9637930989265442\n",
      "Final Validation Loss: 0.5305395722389221\n",
      "Final Validation Accuracy: 0.8510344624519348\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5638 - accuracy: 0.8523\n",
      "Number of inception layers: 4 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.10427835583686829\n",
      "Final Training Accuracy: 0.9681034684181213\n",
      "Final Validation Loss: 0.5379769802093506\n",
      "Final Validation Accuracy: 0.8413792848587036\n",
      "Epoch 53: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.6148 - accuracy: 0.8484\n",
      "Number of inception layers: 5 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.11390137672424316\n",
      "Final Training Accuracy: 0.9584482908248901\n",
      "Final Validation Loss: 0.5532671809196472\n",
      "Final Validation Accuracy: 0.843448281288147\n",
      "Epoch 46: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5932 - accuracy: 0.8225\n",
      "Number of inception layers: 1 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.04536674916744232\n",
      "Final Training Accuracy: 0.9993103742599487\n",
      "Final Validation Loss: 0.5884968638420105\n",
      "Final Validation Accuracy: 0.8213793039321899\n",
      "Epoch 44: early stopping\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.6861 - accuracy: 0.8286\n",
      "Number of inception layers: 2 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.05050107091665268\n",
      "Final Training Accuracy: 0.9920689463615417\n",
      "Final Validation Loss: 0.6182072162628174\n",
      "Final Validation Accuracy: 0.8303448557853699\n",
      "Epoch 38: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.5578 - accuracy: 0.8495\n",
      "Number of inception layers: 3 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.10742121189832687\n",
      "Final Training Accuracy: 0.9653448462486267\n",
      "Final Validation Loss: 0.5418640375137329\n",
      "Final Validation Accuracy: 0.8537930846214294\n",
      "Epoch 47: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.6082 - accuracy: 0.8412\n",
      "Number of inception layers: 4 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.1070331260561943\n",
      "Final Training Accuracy: 0.963620662689209\n",
      "Final Validation Loss: 0.5620838403701782\n",
      "Final Validation Accuracy: 0.8406896591186523\n",
      "Epoch 60: early stopping\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.6470 - accuracy: 0.8115\n",
      "Number of inception layers: 5 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.17271281778812408\n",
      "Final Training Accuracy: 0.9418965578079224\n",
      "Final Validation Loss: 0.6118879914283752\n",
      "Final Validation Accuracy: 0.817241370677948\n",
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.8089 - accuracy: 0.8175\n",
      "Number of inception layers: 1 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.009855571202933788\n",
      "Final Training Accuracy: 0.9989655017852783\n",
      "Final Validation Loss: 0.787357747554779\n",
      "Final Validation Accuracy: 0.813103437423706\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8927 - accuracy: 0.8523\n",
      "Number of inception layers: 2 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.01768774539232254\n",
      "Final Training Accuracy: 0.9950000047683716\n",
      "Final Validation Loss: 0.8672950267791748\n",
      "Final Validation Accuracy: 0.8427585959434509\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.7311 - accuracy: 0.8407\n",
      "Number of inception layers: 3 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.02463584393262863\n",
      "Final Training Accuracy: 0.9939655065536499\n",
      "Final Validation Loss: 0.7400733232498169\n",
      "Final Validation Accuracy: 0.8531034588813782\n",
      "Epoch 17: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5230 - accuracy: 0.8771\n",
      "Number of inception layers: 4 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.04786916822195053\n",
      "Final Training Accuracy: 0.9858620762825012\n",
      "Final Validation Loss: 0.47833096981048584\n",
      "Final Validation Accuracy: 0.8813793063163757\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.6506 - accuracy: 0.8578\n",
      "Number of inception layers: 5 (ADAM, batchsize: 5)\n",
      "Final Training Loss: 0.03833983466029167\n",
      "Final Training Accuracy: 0.9893103241920471\n",
      "Final Validation Loss: 0.5997804999351501\n",
      "Final Validation Accuracy: 0.8689655065536499\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.7182 - accuracy: 0.8170\n",
      "Number of inception layers: 1 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.017551351338624954\n",
      "Final Training Accuracy: 0.9996551871299744\n",
      "Final Validation Loss: 0.6882728934288025\n",
      "Final Validation Accuracy: 0.817241370677948\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8084 - accuracy: 0.8396\n",
      "Number of inception layers: 2 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.009846094995737076\n",
      "Final Training Accuracy: 0.9996551871299744\n",
      "Final Validation Loss: 0.7883447408676147\n",
      "Final Validation Accuracy: 0.8420689702033997\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.7317 - accuracy: 0.8467\n",
      "Number of inception layers: 3 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.039236899465322495\n",
      "Final Training Accuracy: 0.9912068843841553\n",
      "Final Validation Loss: 0.6866586208343506\n",
      "Final Validation Accuracy: 0.8406896591186523\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6020 - accuracy: 0.8677\n",
      "Number of inception layers: 4 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.04148484766483307\n",
      "Final Training Accuracy: 0.9901723861694336\n",
      "Final Validation Loss: 0.5929313898086548\n",
      "Final Validation Accuracy: 0.8717241287231445\n",
      "Epoch 23: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.6583 - accuracy: 0.8545\n",
      "Number of inception layers: 5 (ADAM, batchsize: 25)\n",
      "Final Training Loss: 0.058970216661691666\n",
      "Final Training Accuracy: 0.9817241430282593\n",
      "Final Validation Loss: 0.5247541666030884\n",
      "Final Validation Accuracy: 0.8558620810508728\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.6225 - accuracy: 0.8324\n",
      "Number of inception layers: 1 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.019217474386096\n",
      "Final Training Accuracy: 0.9998275637626648\n",
      "Final Validation Loss: 0.6277771592140198\n",
      "Final Validation Accuracy: 0.8199999928474426\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.7799 - accuracy: 0.8115\n",
      "Number of inception layers: 2 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.057434771209955215\n",
      "Final Training Accuracy: 0.9844827651977539\n",
      "Final Validation Loss: 0.6833822131156921\n",
      "Final Validation Accuracy: 0.8310344815254211\n",
      "Epoch 23: early stopping\n",
      "57/57 [==============================] - 2s 25ms/step - loss: 0.7527 - accuracy: 0.8550\n",
      "Number of inception layers: 3 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.029221711680293083\n",
      "Final Training Accuracy: 0.994655191898346\n",
      "Final Validation Loss: 0.6748066544532776\n",
      "Final Validation Accuracy: 0.8593103289604187\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5685 - accuracy: 0.8688\n",
      "Number of inception layers: 4 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.057011086493730545\n",
      "Final Training Accuracy: 0.9846552014350891\n",
      "Final Validation Loss: 0.5289248824119568\n",
      "Final Validation Accuracy: 0.8703448176383972\n",
      "Epoch 32: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6381 - accuracy: 0.8616\n",
      "Number of inception layers: 5 (ADAM, batchsize: 50)\n",
      "Final Training Loss: 0.027819564566016197\n",
      "Final Training Accuracy: 0.992241382598877\n",
      "Final Validation Loss: 0.6511240601539612\n",
      "Final Validation Accuracy: 0.8551723957061768\n"
     ]
    }
   ],
   "source": [
    "# Varrie the number of layers\n",
    "# convolutional filters: 32; epochs: 100; density: 250, rms_propagation learning rate:\n",
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for fs in [16,64]:\n",
    "        for bs in [5,25,50]:\n",
    "            title_appendix = f'{opti_str}_ fs{fs}_bs{bs}_e100_d250'\n",
    "            for i in range(1,6):\n",
    "                # Build the model\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(fs, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "                for x in range(i):\n",
    "                    model.add(InceptionModule(filters=fs))\n",
    "                    model.add(MaxPooling2D(2, 2))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(250, activation='relu'))\n",
    "                model.add(Dense(19, activation='softmax'))\n",
    "            \n",
    "                # Compile the model\n",
    "                opti = RMSprop(learning_rate=0.001)\n",
    "                if opti_str == 'ADAM':\n",
    "                    opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=opti,\n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "                # Fit the model\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "                history = model.fit(X_train, y_train, \n",
    "                                    epochs=100, \n",
    "                                    batch_size=bs, \n",
    "                                    verbose=0, \n",
    "                                    validation_split=0.2, \n",
    "                                    callbacks=[early_stopping])\n",
    "                # Evaluate the model\n",
    "                loss, accuracy = model.evaluate(X_test, y_test)\n",
    "                print('Number of inception layers: %.i (%s, batchsize: %.i)' % (i, opti_str, bs))\n",
    "                print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "                print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "                print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "                print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "                visualize_history(history, f'inception_{title_appendix}_sublayercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "                # Clear Keras session\n",
    "                K.clear_session()\n",
    "                del model\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0121da2d-cf14-41d0-b534-25b65ba8166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 16:19:03.012352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-16 16:19:03.923259: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1f7825e520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 16:19:03.923283: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-16 16:19:03.927377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-16 16:19:04.026918: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: early stopping\n",
      "57/57 [==============================] - 4s 41ms/step - loss: 0.8639 - accuracy: 0.7861\n",
      "Number of inception layers: 6 (RMSP, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.5793085694313049\n",
      "Final Training Accuracy: 0.8322413563728333\n",
      "Final Validation Loss: 0.6257495880126953\n",
      "Final Validation Accuracy: 0.8124138116836548\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 1.4180 - accuracy: 0.7933\n",
      "Number of inception layers: 7 (RMSP, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.6847610473632812\n",
      "Final Training Accuracy: 0.7908620834350586\n",
      "Final Validation Loss: 1.0126256942749023\n",
      "Final Validation Accuracy: 0.795862078666687\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.9070 - accuracy: 0.7078\n",
      "Number of inception layers: 8 (RMSP, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.8827313184738159\n",
      "Final Training Accuracy: 0.7336207032203674\n",
      "Final Validation Loss: 0.8492476344108582\n",
      "Final Validation Accuracy: 0.7096551656723022\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.2500 - accuracy: 0.7106\n",
      "Number of inception layers: 9 (RMSP, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.9730051755905151\n",
      "Final Training Accuracy: 0.7144827842712402\n",
      "Final Validation Loss: 0.9144446849822998\n",
      "Final Validation Accuracy: 0.708965539932251\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 1.3875 - accuracy: 0.6847\n",
      "Number of inception layers: 10 (RMSP, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 1.1024044752120972\n",
      "Final Training Accuracy: 0.6853448152542114\n",
      "Final Validation Loss: 1.3158410787582397\n",
      "Final Validation Accuracy: 0.6965517401695251\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.7155 - accuracy: 0.8655\n",
      "Number of inception layers: 6 (RMSP, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.09814345836639404\n",
      "Final Training Accuracy: 0.9670689702033997\n",
      "Final Validation Loss: 0.7162286043167114\n",
      "Final Validation Accuracy: 0.869655191898346\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8917 - accuracy: 0.8589\n",
      "Number of inception layers: 7 (RMSP, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.10747111588716507\n",
      "Final Training Accuracy: 0.9696551561355591\n",
      "Final Validation Loss: 0.6526616215705872\n",
      "Final Validation Accuracy: 0.8593103289604187\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.1918 - accuracy: 0.8600\n",
      "Number of inception layers: 8 (RMSP, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.12554657459259033\n",
      "Final Training Accuracy: 0.9681034684181213\n",
      "Final Validation Loss: 0.9435098171234131\n",
      "Final Validation Accuracy: 0.8634482622146606\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8699 - accuracy: 0.8479\n",
      "Number of inception layers: 9 (RMSP, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.19723950326442719\n",
      "Final Training Accuracy: 0.943965494632721\n",
      "Final Validation Loss: 0.7846373915672302\n",
      "Final Validation Accuracy: 0.8310344815254211\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.9312 - accuracy: 0.8490\n",
      "Number of inception layers: 10 (RMSP, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.20635348558425903\n",
      "Final Training Accuracy: 0.9468965530395508\n",
      "Final Validation Loss: 0.7135934233665466\n",
      "Final Validation Accuracy: 0.8600000143051147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 17:58:45.524095: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 3872276480 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 189399040/8510832640\n",
      "2024-01-16 17:58:45.524122: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      1780429468\n",
      "MaxInUse:                   5547324060\n",
      "NumAllocs:                    52222838\n",
      "MaxAllocSize:               3872014336\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 17:58:45.524139: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 17:58:45.524145: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 34\n",
      "2024-01-16 17:58:45.524148: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 10\n",
      "2024-01-16 17:58:45.524151: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 17:58:45.524154: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 17:58:45.524158: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 104\n",
      "2024-01-16 17:58:45.524161: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 400, 1\n",
      "2024-01-16 17:58:45.524164: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 17:58:45.524167: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 17:58:45.524170: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 5\n",
      "2024-01-16 17:58:45.524173: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3800, 1\n",
      "2024-01-16 17:58:45.524176: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 5\n",
      "2024-01-16 17:58:45.524179: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 17:58:45.524182: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 49152, 28\n",
      "2024-01-16 17:58:45.524185: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 5\n",
      "2024-01-16 17:58:45.524188: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 192000, 2\n",
      "2024-01-16 17:58:45.524191: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 17:58:45.524194: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 6\n",
      "2024-01-16 17:58:45.524197: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 17:58:45.524200: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 442368, 28\n",
      "2024-01-16 17:58:45.524203: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1228800, 28\n",
      "2024-01-16 17:58:45.524206: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 17:58:45.524209: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 104857600, 4\n",
      "2024-01-16 17:58:45.524212: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 121634816, 1\n",
      "2024-01-16 17:58:45.524215: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 419430400, 1\n",
      "2024-01-16 17:58:45.524218: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 17:58:45.524223: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 17:58:45.524227: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 1780429468\n",
      "2024-01-16 17:58:45.524230: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7449083904\n",
      "2024-01-16 17:58:45.524235: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5547324060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 2s 25ms/step - loss: 0.6056 - accuracy: 0.8600\n",
      "Number of inception layers: 6 (RMSP, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.07514721155166626\n",
      "Final Training Accuracy: 0.9743103384971619\n",
      "Final Validation Loss: 0.5782197117805481\n",
      "Final Validation Accuracy: 0.8455172181129456\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.8576 - accuracy: 0.8760\n",
      "Number of inception layers: 7 (RMSP, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.06016116961836815\n",
      "Final Training Accuracy: 0.9824137687683105\n",
      "Final Validation Loss: 0.7358242869377136\n",
      "Final Validation Accuracy: 0.869655191898346\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.8205 - accuracy: 0.8705\n",
      "Number of inception layers: 8 (RMSP, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.07935906946659088\n",
      "Final Training Accuracy: 0.9751724004745483\n",
      "Final Validation Loss: 0.7537447214126587\n",
      "Final Validation Accuracy: 0.8682758808135986\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6686 - accuracy: 0.8534\n",
      "Number of inception layers: 9 (RMSP, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.10426536947488785\n",
      "Final Training Accuracy: 0.966724157333374\n",
      "Final Validation Loss: 0.6113303303718567\n",
      "Final Validation Accuracy: 0.8496551513671875\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.7396 - accuracy: 0.8655\n",
      "Number of inception layers: 10 (RMSP, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.1207370012998581\n",
      "Final Training Accuracy: 0.9641379117965698\n",
      "Final Validation Loss: 0.7008507251739502\n",
      "Final Validation Accuracy: 0.8641379475593567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 18:41:53.874966: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 6565134336 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 92667904/8510832640\n",
      "2024-01-16 18:41:53.874994: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      1186059704\n",
      "MaxInUse:                   5862581572\n",
      "NumAllocs:                    59270678\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 18:41:53.875013: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 18:41:53.875018: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 18:41:53.875022: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 18:41:53.875025: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 20, 1\n",
      "2024-01-16 18:41:53.875029: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 18:41:53.875032: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 66\n",
      "2024-01-16 18:41:53.875035: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 40\n",
      "2024-01-16 18:41:53.875039: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 18:41:53.875042: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 18:41:53.875045: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 3\n",
      "2024-01-16 18:41:53.875048: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 2\n",
      "2024-01-16 18:41:53.875052: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 3\n",
      "2024-01-16 18:41:53.875055: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 18:41:53.875058: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 49152, 18\n",
      "2024-01-16 18:41:53.875061: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 2\n",
      "2024-01-16 18:41:53.875065: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 3\n",
      "2024-01-16 18:41:53.875068: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 192000, 2\n",
      "2024-01-16 18:41:53.875071: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 11\n",
      "2024-01-16 18:41:53.875074: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 3\n",
      "2024-01-16 18:41:53.875078: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 18:41:53.875083: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 442368, 18\n",
      "2024-01-16 18:41:53.875089: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 2\n",
      "2024-01-16 18:41:53.875094: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 655360, 1\n",
      "2024-01-16 18:41:53.875099: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 18:41:53.875102: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1228800, 18\n",
      "2024-01-16 18:41:53.875105: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 2\n",
      "2024-01-16 18:41:53.875108: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 11\n",
      "2024-01-16 18:41:53.875111: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 12\n",
      "2024-01-16 18:41:53.875114: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 5242880, 1\n",
      "2024-01-16 18:41:53.875117: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 15728640, 3\n",
      "2024-01-16 18:41:53.875122: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 20971520, 4\n",
      "2024-01-16 18:41:53.875125: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 21692416, 1\n",
      "2024-01-16 18:41:53.875128: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 62914560, 1\n",
      "2024-01-16 18:41:53.875131: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 83886080, 1\n",
      "2024-01-16 18:41:53.875134: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 18:41:53.875139: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 18:41:53.875143: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 1186059704\n",
      "2024-01-16 18:41:53.875146: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7449083904\n",
      "2024-01-16 18:41:53.875149: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5862581572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 7s 83ms/step - loss: 0.9079 - accuracy: 0.7370\n",
      "Number of inception layers: 6 (RMSP, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.6113438606262207\n",
      "Final Training Accuracy: 0.8286206722259521\n",
      "Final Validation Loss: 0.8049278259277344\n",
      "Final Validation Accuracy: 0.7220689654350281\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 4s 61ms/step - loss: 1.0353 - accuracy: 0.7740\n",
      "Number of inception layers: 7 (RMSP, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.7488276958465576\n",
      "Final Training Accuracy: 0.7865517139434814\n",
      "Final Validation Loss: 0.80855393409729\n",
      "Final Validation Accuracy: 0.7875862121582031\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 4s 61ms/step - loss: 1.7018 - accuracy: 0.6792\n",
      "Number of inception layers: 8 (RMSP, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.9560306668281555\n",
      "Final Training Accuracy: 0.7268965244293213\n",
      "Final Validation Loss: 1.1788352727890015\n",
      "Final Validation Accuracy: 0.6848275661468506\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 1.3390 - accuracy: 0.6764\n",
      "Number of inception layers: 9 (RMSP, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 1.0231821537017822\n",
      "Final Training Accuracy: 0.6994827389717102\n",
      "Final Validation Loss: 1.053723692893982\n",
      "Final Validation Accuracy: 0.6737930774688721\n",
      "Epoch 15: early stopping\n",
      "57/57 [==============================] - 4s 61ms/step - loss: 2.9409 - accuracy: 0.0540\n",
      "Number of inception layers: 10 (RMSP, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 2.9377758502960205\n",
      "Final Training Accuracy: 0.05637931078672409\n",
      "Final Validation Loss: 2.9394285678863525\n",
      "Final Validation Accuracy: 0.0468965508043766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 20:36:11.520738: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 6586105856 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 101318656/8510832640\n",
      "2024-01-16 20:36:11.520763: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      2354249736\n",
      "MaxInUse:                   5862581572\n",
      "NumAllocs:                    98919251\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 20:36:11.520780: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 20:36:11.520784: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 20:36:11.520788: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 20:36:11.520791: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 20:36:11.520794: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 100, 1\n",
      "2024-01-16 20:36:11.520797: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 20:36:11.520800: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 101\n",
      "2024-01-16 20:36:11.520803: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 20:36:11.520807: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 20:36:11.520810: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 20:36:11.520813: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 20:36:11.520816: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 20:36:11.520819: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 20:36:11.520822: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 20:36:11.520825: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 20:36:11.520828: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 20:36:11.520831: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 20:36:11.520834: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 20:36:11.520837: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 20:36:11.520840: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 20:36:11.520843: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 20:36:11.520846: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 20:36:11.520849: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 30\n",
      "2024-01-16 20:36:11.520852: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3276800, 1\n",
      "2024-01-16 20:36:11.520855: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 28\n",
      "2024-01-16 20:36:11.520858: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 18546688, 1\n",
      "2024-01-16 20:36:11.520861: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 26214400, 2\n",
      "2024-01-16 20:36:11.520864: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 78643200, 2\n",
      "2024-01-16 20:36:11.520867: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 104857600, 4\n",
      "2024-01-16 20:36:11.520870: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 314572800, 1\n",
      "2024-01-16 20:36:11.520875: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 419430400, 1\n",
      "2024-01-16 20:36:11.520878: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 20:36:11.520883: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 20:36:11.520886: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2354249736\n",
      "2024-01-16 20:36:11.520890: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7449083904\n",
      "2024-01-16 20:36:11.520893: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5862581572\n",
      "2024-01-16 20:36:11.917102: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 6586105856 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 101515264/8510832640\n",
      "2024-01-16 20:36:11.917130: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      2417885704\n",
      "MaxInUse:                   5862581572\n",
      "NumAllocs:                    98919284\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 20:36:11.917148: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 20:36:11.917154: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 20:36:11.917158: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 20:36:11.917161: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 20:36:11.917165: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 100, 1\n",
      "2024-01-16 20:36:11.917168: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 20:36:11.917171: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 102\n",
      "2024-01-16 20:36:11.917174: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 20:36:11.917178: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 20:36:11.917181: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 20:36:11.917184: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 20:36:11.917187: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 20:36:11.917190: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 20:36:11.917194: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 20:36:11.917197: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 20:36:11.917200: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 20:36:11.917203: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 20:36:11.917207: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 20:36:11.917210: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 20:36:11.917213: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 20:36:11.917216: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 20:36:11.917219: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 20:36:11.917222: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 29\n",
      "2024-01-16 20:36:11.917225: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3276800, 1\n",
      "2024-01-16 20:36:11.917229: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 30\n",
      "2024-01-16 20:36:11.917232: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 21692416, 1\n",
      "2024-01-16 20:36:11.917235: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 26214400, 1\n",
      "2024-01-16 20:36:11.917238: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 78643200, 3\n",
      "2024-01-16 20:36:11.917241: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 104857600, 4\n",
      "2024-01-16 20:36:11.917244: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 314572800, 1\n",
      "2024-01-16 20:36:11.917249: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 419430400, 1\n",
      "2024-01-16 20:36:11.917253: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 20:36:11.917258: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 20:36:11.917262: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2417885704\n",
      "2024-01-16 20:36:11.917265: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7449083904\n",
      "2024-01-16 20:36:11.917268: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5862581572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 4s 59ms/step - loss: 0.8162 - accuracy: 0.8567\n",
      "Number of inception layers: 6 (RMSP, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.10968565940856934\n",
      "Final Training Accuracy: 0.9653448462486267\n",
      "Final Validation Loss: 0.6077407598495483\n",
      "Final Validation Accuracy: 0.8510344624519348\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 4s 59ms/step - loss: 0.9809 - accuracy: 0.8512\n",
      "Number of inception layers: 7 (RMSP, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.15752045810222626\n",
      "Final Training Accuracy: 0.9498276114463806\n",
      "Final Validation Loss: 0.7261333465576172\n",
      "Final Validation Accuracy: 0.8544827699661255\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 4s 60ms/step - loss: 0.6437 - accuracy: 0.8335\n",
      "Number of inception layers: 8 (RMSP, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.21454960107803345\n",
      "Final Training Accuracy: 0.9356896281242371\n",
      "Final Validation Loss: 0.5281261205673218\n",
      "Final Validation Accuracy: 0.8351724147796631\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 4s 60ms/step - loss: 1.1315 - accuracy: 0.8561\n",
      "Number of inception layers: 9 (RMSP, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.21087105572223663\n",
      "Final Training Accuracy: 0.9413793087005615\n",
      "Final Validation Loss: 0.7615461945533752\n",
      "Final Validation Accuracy: 0.8510344624519348\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 4s 60ms/step - loss: 0.8436 - accuracy: 0.8671\n",
      "Number of inception layers: 10 (RMSP, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.27265724539756775\n",
      "Final Training Accuracy: 0.9267241358757019\n",
      "Final Validation Loss: 0.718923032283783\n",
      "Final Validation Accuracy: 0.8613793253898621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 22:01:02.317823: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 3948937216 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 202702848/8510832640\n",
      "2024-01-16 22:01:02.317850: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      3667660484\n",
      "MaxInUse:                   5884808900\n",
      "NumAllocs:                   109880800\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:02.317869: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:02.317874: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 44\n",
      "2024-01-16 22:01:02.317878: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 10\n",
      "2024-01-16 22:01:02.317882: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:02.317885: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:02.317888: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:02.317892: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 400, 1\n",
      "2024-01-16 22:01:02.317895: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 100\n",
      "2024-01-16 22:01:02.317898: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:02.317901: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:02.317905: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:02.317908: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3800, 1\n",
      "2024-01-16 22:01:02.317911: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:02.317914: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:02.317918: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:02.317921: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:02.317924: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:02.317932: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 28\n",
      "2024-01-16 22:01:02.317936: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:02.317939: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:02.317942: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:02.317945: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:02.317948: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:02.317952: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:02.317955: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 28\n",
      "2024-01-16 22:01:02.317958: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 29\n",
      "2024-01-16 22:01:02.317961: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:02.317965: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 3\n",
      "2024-01-16 22:01:02.317968: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 69206016, 1\n",
      "2024-01-16 22:01:02.317971: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 1\n",
      "2024-01-16 22:01:02.317976: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:02.317979: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:02.317982: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:02.317986: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:02.317991: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:02.317995: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 3667660484\n",
      "2024-01-16 22:01:02.317998: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:02.318001: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5884808900\n",
      "2024-01-16 22:01:02.324891: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 3948937216 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 202702848/8510832640\n",
      "2024-01-16 22:01:02.324905: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      3667660484\n",
      "MaxInUse:                   5884808900\n",
      "NumAllocs:                   109880800\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:02.324919: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:02.324924: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 44\n",
      "2024-01-16 22:01:02.324934: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 10\n",
      "2024-01-16 22:01:02.324937: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:02.324941: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:02.324944: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:02.324947: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 400, 1\n",
      "2024-01-16 22:01:02.324950: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 100\n",
      "2024-01-16 22:01:02.324954: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:02.324957: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:02.324960: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:02.324963: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3800, 1\n",
      "2024-01-16 22:01:02.324967: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:02.324970: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:02.324973: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:02.324976: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:02.324980: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:02.324983: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 28\n",
      "2024-01-16 22:01:02.324986: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:02.324989: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:02.324992: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:02.324996: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:02.324999: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:02.325002: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:02.325005: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 28\n",
      "2024-01-16 22:01:02.325009: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 29\n",
      "2024-01-16 22:01:02.325012: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:02.325015: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 3\n",
      "2024-01-16 22:01:02.325018: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 69206016, 1\n",
      "2024-01-16 22:01:02.325022: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 1\n",
      "2024-01-16 22:01:02.325025: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:02.325028: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:02.325031: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:02.325035: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:02.325039: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:02.325043: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 3667660484\n",
      "2024-01-16 22:01:02.325046: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:02.325050: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 5884808900\n",
      "2024-01-16 22:01:06.044278: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 6612320256 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 198770688/8510832640\n",
      "2024-01-16 22:01:06.044307: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      3720675436\n",
      "MaxInUse:                   6195711684\n",
      "NumAllocs:                   109881537\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:06.044326: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:06.044332: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 22:01:06.044336: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 22:01:06.044339: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:06.044343: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:06.044346: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:06.044349: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 101\n",
      "2024-01-16 22:01:06.044353: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:06.044356: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:06.044359: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:06.044363: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:06.044366: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:06.044370: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:06.044373: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:06.044376: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:06.044380: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 22:01:06.044385: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:06.044391: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:06.044396: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:06.044402: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:06.044407: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:06.044410: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:06.044414: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 30\n",
      "2024-01-16 22:01:06.044417: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 28\n",
      "2024-01-16 22:01:06.044420: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:06.044424: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 18546688, 1\n",
      "2024-01-16 22:01:06.044427: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 2\n",
      "2024-01-16 22:01:06.044431: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 2\n",
      "2024-01-16 22:01:06.044434: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:06.044438: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:06.044443: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:06.044446: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:06.044452: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:06.044455: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 3720675436\n",
      "2024-01-16 22:01:06.044459: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:06.044462: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6195711684\n",
      "2024-01-16 22:01:06.070837: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 7413628928 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 198770688/8510832640\n",
      "2024-01-16 22:01:06.070866: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      4190765676\n",
      "MaxInUse:                   6195711684\n",
      "NumAllocs:                   109881548\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:06.070891: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:06.070900: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 22:01:06.070905: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 22:01:06.070910: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:06.070915: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:06.070920: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:06.070930: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 102\n",
      "2024-01-16 22:01:06.070937: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:06.070943: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:06.070948: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:06.070954: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:06.070957: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:06.070960: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:06.070965: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:06.070970: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:06.070976: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 22:01:06.070979: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:06.070982: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:06.070985: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:06.070989: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:06.070992: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:06.070995: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:06.070999: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 30\n",
      "2024-01-16 22:01:06.071004: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 28\n",
      "2024-01-16 22:01:06.071010: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:06.071015: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 2\n",
      "2024-01-16 22:01:06.071020: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 4\n",
      "2024-01-16 22:01:06.071024: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 174063616, 1\n",
      "2024-01-16 22:01:06.071030: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:06.071034: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:06.071039: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:06.071044: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:06.071051: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:06.071057: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 4190765676\n",
      "2024-01-16 22:01:06.071062: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:06.071068: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6195711684\n",
      "2024-01-16 22:01:06.866626: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 6612320256 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 198770688/8510832640\n",
      "2024-01-16 22:01:06.866654: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      3836740204\n",
      "MaxInUse:                   6195711684\n",
      "NumAllocs:                   109881571\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:06.866671: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:06.866677: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 22:01:06.866681: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 22:01:06.866684: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:06.866687: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:06.866691: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:06.866694: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 102\n",
      "2024-01-16 22:01:06.866697: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:06.866701: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:06.866704: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:06.866707: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:06.866710: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:06.866713: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:06.866717: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:06.866720: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:06.866723: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 22:01:06.866726: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:06.866729: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:06.866733: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:06.866736: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:06.866739: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:06.866742: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:06.866747: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 29\n",
      "2024-01-16 22:01:06.866753: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 30\n",
      "2024-01-16 22:01:06.866763: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:06.866769: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 21692416, 1\n",
      "2024-01-16 22:01:06.866774: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 1\n",
      "2024-01-16 22:01:06.866779: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 3\n",
      "2024-01-16 22:01:06.866784: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:06.866789: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:06.866795: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:06.866800: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:06.866807: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:06.866813: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 3836740204\n",
      "2024-01-16 22:01:06.866818: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:06.866824: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6195711684\n",
      "2024-01-16 22:01:06.906003: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 7416774656 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 198770688/8510832640\n",
      "2024-01-16 22:01:06.906034: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7343308800\n",
      "InUse:                      4303684716\n",
      "MaxInUse:                   6195711684\n",
      "NumAllocs:                   109881581\n",
      "MaxAllocSize:               3872276480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-16 22:01:06.906052: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-16 22:01:06.906057: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 46\n",
      "2024-01-16 22:01:06.906061: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-16 22:01:06.906064: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-16 22:01:06.906067: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 200, 1\n",
      "2024-01-16 22:01:06.906070: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 4\n",
      "2024-01-16 22:01:06.906073: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 103\n",
      "2024-01-16 22:01:06.906076: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1000, 4\n",
      "2024-01-16 22:01:06.906079: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-16 22:01:06.906082: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 1\n",
      "2024-01-16 22:01:06.906085: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4608, 4\n",
      "2024-01-16 22:01:06.906088: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16384, 1\n",
      "2024-01-16 22:01:06.906091: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19000, 4\n",
      "2024-01-16 22:01:06.906094: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 4\n",
      "2024-01-16 22:01:06.906097: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 1\n",
      "2024-01-16 22:01:06.906100: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 29\n",
      "2024-01-16 22:01:06.906103: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 384000, 2\n",
      "2024-01-16 22:01:06.906108: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 409600, 1\n",
      "2024-01-16 22:01:06.906113: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-16 22:01:06.906118: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 4\n",
      "2024-01-16 22:01:06.906122: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 768000, 2\n",
      "2024-01-16 22:01:06.906127: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1638400, 4\n",
      "2024-01-16 22:01:06.906132: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1769472, 29\n",
      "2024-01-16 22:01:06.906137: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 30\n",
      "2024-01-16 22:01:06.906142: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6553600, 1\n",
      "2024-01-16 22:01:06.906146: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 52428800, 1\n",
      "2024-01-16 22:01:06.906151: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 157286400, 5\n",
      "2024-01-16 22:01:06.906156: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 174063616, 1\n",
      "2024-01-16 22:01:06.906161: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 209715200, 4\n",
      "2024-01-16 22:01:06.906166: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 629145600, 1\n",
      "2024-01-16 22:01:06.906173: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-16 22:01:06.906178: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 838860800, 1\n",
      "2024-01-16 22:01:06.906186: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 7314866176\n",
      "2024-01-16 22:01:06.906192: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 4303684716\n",
      "2024-01-16 22:01:06.906197: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 7482638336\n",
      "2024-01-16 22:01:06.906203: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6195711684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.5605 - accuracy: 0.8804\n",
      "Number of inception layers: 6 (RMSP, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.06856274604797363\n",
      "Final Training Accuracy: 0.978620707988739\n",
      "Final Validation Loss: 0.5663509368896484\n",
      "Final Validation Accuracy: 0.8627586364746094\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.8798 - accuracy: 0.8418\n",
      "Number of inception layers: 7 (RMSP, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.11138313263654709\n",
      "Final Training Accuracy: 0.966724157333374\n",
      "Final Validation Loss: 0.7684637904167175\n",
      "Final Validation Accuracy: 0.8489655256271362\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 4s 57ms/step - loss: 0.8201 - accuracy: 0.8374\n",
      "Number of inception layers: 8 (RMSP, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.12528003752231598\n",
      "Final Training Accuracy: 0.9625862240791321\n",
      "Final Validation Loss: 0.6158639788627625\n",
      "Final Validation Accuracy: 0.8496551513671875\n",
      "Epoch 35: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.8570 - accuracy: 0.8583\n",
      "Number of inception layers: 9 (RMSP, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.1111065149307251\n",
      "Final Training Accuracy: 0.968448281288147\n",
      "Final Validation Loss: 0.7046611309051514\n",
      "Final Validation Accuracy: 0.8503448367118835\n",
      "Epoch 30: early stopping\n",
      "57/57 [==============================] - 4s 60ms/step - loss: 0.7001 - accuracy: 0.8528\n",
      "Number of inception layers: 10 (RMSP, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.1571866124868393\n",
      "Final Training Accuracy: 0.9506896734237671\n",
      "Final Validation Loss: 0.5292060971260071\n",
      "Final Validation Accuracy: 0.8710345029830933\n",
      "Epoch 21: early stopping\n",
      "57/57 [==============================] - 2s 24ms/step - loss: 0.4810 - accuracy: 0.8771\n",
      "Number of inception layers: 6 (ADAM, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.06435347348451614\n",
      "Final Training Accuracy: 0.9818965792655945\n",
      "Final Validation Loss: 0.4587627053260803\n",
      "Final Validation Accuracy: 0.8820689916610718\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5401 - accuracy: 0.8677\n",
      "Number of inception layers: 7 (ADAM, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.07059171795845032\n",
      "Final Training Accuracy: 0.9746551513671875\n",
      "Final Validation Loss: 0.554827094078064\n",
      "Final Validation Accuracy: 0.8565517067909241\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6339 - accuracy: 0.8638\n",
      "Number of inception layers: 8 (ADAM, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.09572361409664154\n",
      "Final Training Accuracy: 0.9663792848587036\n",
      "Final Validation Loss: 0.5701503753662109\n",
      "Final Validation Accuracy: 0.860689640045166\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5074 - accuracy: 0.8743\n",
      "Number of inception layers: 9 (ADAM, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.11734447628259659\n",
      "Final Training Accuracy: 0.956551730632782\n",
      "Final Validation Loss: 0.4921487271785736\n",
      "Final Validation Accuracy: 0.8634482622146606\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.5830 - accuracy: 0.8649\n",
      "Number of inception layers: 10 (ADAM, batchsize: 5, filtersize: 64)\n",
      "Final Training Loss: 0.07751229405403137\n",
      "Final Training Accuracy: 0.9729310274124146\n",
      "Final Validation Loss: 0.5255815386772156\n",
      "Final Validation Accuracy: 0.865517258644104\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 2s 25ms/step - loss: 0.5235 - accuracy: 0.8644\n",
      "Number of inception layers: 6 (ADAM, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.0753977969288826\n",
      "Final Training Accuracy: 0.9729310274124146\n",
      "Final Validation Loss: 0.5530070662498474\n",
      "Final Validation Accuracy: 0.8524137735366821\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5317 - accuracy: 0.8671\n",
      "Number of inception layers: 7 (ADAM, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.06272148340940475\n",
      "Final Training Accuracy: 0.9793103337287903\n",
      "Final Validation Loss: 0.5022935271263123\n",
      "Final Validation Accuracy: 0.8724138140678406\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6349 - accuracy: 0.8346\n",
      "Number of inception layers: 8 (ADAM, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.12438507378101349\n",
      "Final Training Accuracy: 0.9551724195480347\n",
      "Final Validation Loss: 0.5968068242073059\n",
      "Final Validation Accuracy: 0.8331034779548645\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5409 - accuracy: 0.8655\n",
      "Number of inception layers: 9 (ADAM, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.15340963006019592\n",
      "Final Training Accuracy: 0.9481034278869629\n",
      "Final Validation Loss: 0.4916076958179474\n",
      "Final Validation Accuracy: 0.8668965697288513\n",
      "Epoch 27: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.5423 - accuracy: 0.8523\n",
      "Number of inception layers: 10 (ADAM, batchsize: 25, filtersize: 64)\n",
      "Final Training Loss: 0.209464892745018\n",
      "Final Training Accuracy: 0.9231034517288208\n",
      "Final Validation Loss: 0.5305910706520081\n",
      "Final Validation Accuracy: 0.8296551704406738\n",
      "Epoch 33: early stopping\n",
      "57/57 [==============================] - 2s 25ms/step - loss: 0.5509 - accuracy: 0.8732\n",
      "Number of inception layers: 6 (ADAM, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.08860950171947479\n",
      "Final Training Accuracy: 0.9706896543502808\n",
      "Final Validation Loss: 0.5232139229774475\n",
      "Final Validation Accuracy: 0.8634482622146606\n",
      "Epoch 37: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5221 - accuracy: 0.8534\n",
      "Number of inception layers: 7 (ADAM, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.07727257907390594\n",
      "Final Training Accuracy: 0.9743103384971619\n",
      "Final Validation Loss: 0.5106505155563354\n",
      "Final Validation Accuracy: 0.8675861954689026\n",
      "Epoch 31: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5491 - accuracy: 0.8462\n",
      "Number of inception layers: 8 (ADAM, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.15925052762031555\n",
      "Final Training Accuracy: 0.9410344958305359\n",
      "Final Validation Loss: 0.533951997756958\n",
      "Final Validation Accuracy: 0.8372413516044617\n",
      "Epoch 32: early stopping\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.5929 - accuracy: 0.8418\n",
      "Number of inception layers: 9 (ADAM, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.1711249053478241\n",
      "Final Training Accuracy: 0.936896562576294\n",
      "Final Validation Loss: 0.5550115704536438\n",
      "Final Validation Accuracy: 0.8337931036949158\n",
      "Epoch 32: early stopping\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.5157 - accuracy: 0.8462\n",
      "Number of inception layers: 10 (ADAM, batchsize: 50, filtersize: 64)\n",
      "Final Training Loss: 0.21961480379104614\n",
      "Final Training Accuracy: 0.9170689582824707\n",
      "Final Validation Loss: 0.4834457039833069\n",
      "Final Validation Accuracy: 0.8489655256271362\n",
      "Epoch 18: early stopping\n",
      "57/57 [==============================] - 3s 55ms/step - loss: 0.5587 - accuracy: 0.8501\n",
      "Number of inception layers: 6 (ADAM, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.07145543396472931\n",
      "Final Training Accuracy: 0.9781034588813782\n",
      "Final Validation Loss: 0.5962411761283875\n",
      "Final Validation Accuracy: 0.8475862145423889\n",
      "Epoch 25: early stopping\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.5648 - accuracy: 0.8705\n",
      "Number of inception layers: 7 (ADAM, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.04574752226471901\n",
      "Final Training Accuracy: 0.9860345125198364\n",
      "Final Validation Loss: 0.537516176700592\n",
      "Final Validation Accuracy: 0.869655191898346\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 4s 57ms/step - loss: 0.5569 - accuracy: 0.8627\n",
      "Number of inception layers: 8 (ADAM, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.08294346183538437\n",
      "Final Training Accuracy: 0.9713793396949768\n",
      "Final Validation Loss: 0.5405215620994568\n",
      "Final Validation Accuracy: 0.864827573299408\n",
      "Epoch 24: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.6274 - accuracy: 0.8539\n",
      "Number of inception layers: 9 (ADAM, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.08051316440105438\n",
      "Final Training Accuracy: 0.9718965291976929\n",
      "Final Validation Loss: 0.5792412161827087\n",
      "Final Validation Accuracy: 0.8448275923728943\n",
      "Epoch 30: early stopping\n",
      "57/57 [==============================] - 4s 59ms/step - loss: 0.5371 - accuracy: 0.8688\n",
      "Number of inception layers: 10 (ADAM, batchsize: 5, filtersize: 128)\n",
      "Final Training Loss: 0.09807974100112915\n",
      "Final Training Accuracy: 0.9653448462486267\n",
      "Final Validation Loss: 0.5345739722251892\n",
      "Final Validation Accuracy: 0.8600000143051147\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.5261 - accuracy: 0.8705\n",
      "Number of inception layers: 6 (ADAM, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.06843608617782593\n",
      "Final Training Accuracy: 0.9782758355140686\n",
      "Final Validation Loss: 0.4626723825931549\n",
      "Final Validation Accuracy: 0.8724138140678406\n",
      "Epoch 22: early stopping\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.5133 - accuracy: 0.8655\n",
      "Number of inception layers: 7 (ADAM, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.0717860758304596\n",
      "Final Training Accuracy: 0.9750000238418579\n",
      "Final Validation Loss: 0.5386068224906921\n",
      "Final Validation Accuracy: 0.8558620810508728\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.6680 - accuracy: 0.8561\n",
      "Number of inception layers: 8 (ADAM, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.051993560045957565\n",
      "Final Training Accuracy: 0.9829310178756714\n",
      "Final Validation Loss: 0.6583993434906006\n",
      "Final Validation Accuracy: 0.8448275923728943\n",
      "Epoch 29: early stopping\n",
      "57/57 [==============================] - 4s 59ms/step - loss: 0.5454 - accuracy: 0.8765\n",
      "Number of inception layers: 9 (ADAM, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.09200558066368103\n",
      "Final Training Accuracy: 0.9682758450508118\n",
      "Final Validation Loss: 0.5159106850624084\n",
      "Final Validation Accuracy: 0.8675861954689026\n",
      "Epoch 34: early stopping\n",
      "57/57 [==============================] - 4s 60ms/step - loss: 0.6318 - accuracy: 0.8693\n",
      "Number of inception layers: 10 (ADAM, batchsize: 25, filtersize: 128)\n",
      "Final Training Loss: 0.07746975123882294\n",
      "Final Training Accuracy: 0.9710344672203064\n",
      "Final Validation Loss: 0.6427853107452393\n",
      "Final Validation Accuracy: 0.8537930846214294\n",
      "Epoch 26: early stopping\n",
      "57/57 [==============================] - 3s 56ms/step - loss: 0.5744 - accuracy: 0.8655\n",
      "Number of inception layers: 6 (ADAM, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.08708912134170532\n",
      "Final Training Accuracy: 0.9743103384971619\n",
      "Final Validation Loss: 0.5670775771141052\n",
      "Final Validation Accuracy: 0.8634482622146606\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 3s 57ms/step - loss: 0.5780 - accuracy: 0.8671\n",
      "Number of inception layers: 7 (ADAM, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.05232078209519386\n",
      "Final Training Accuracy: 0.9848275780677795\n",
      "Final Validation Loss: 0.5182580947875977\n",
      "Final Validation Accuracy: 0.864827573299408\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.5647 - accuracy: 0.8594\n",
      "Number of inception layers: 8 (ADAM, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.07981213927268982\n",
      "Final Training Accuracy: 0.9727585911750793\n",
      "Final Validation Loss: 0.5773453712463379\n",
      "Final Validation Accuracy: 0.8475862145423889\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.5160 - accuracy: 0.8671\n",
      "Number of inception layers: 9 (ADAM, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.08685798197984695\n",
      "Final Training Accuracy: 0.9712069034576416\n",
      "Final Validation Loss: 0.504889190196991\n",
      "Final Validation Accuracy: 0.8613793253898621\n",
      "Epoch 28: early stopping\n",
      "57/57 [==============================] - 4s 58ms/step - loss: 0.5236 - accuracy: 0.8600\n",
      "Number of inception layers: 10 (ADAM, batchsize: 50, filtersize: 128)\n",
      "Final Training Loss: 0.14590269327163696\n",
      "Final Training Accuracy: 0.9465517401695251\n",
      "Final Validation Loss: 0.5032649040222168\n",
      "Final Validation Accuracy: 0.8475862145423889\n"
     ]
    }
   ],
   "source": [
    "# Varrie the number of layers\n",
    "# convolutional filters: 32; epochs: 100; density: 250, rms_propagation learning rate:\n",
    "for opti_str in ['RMSP', 'ADAM']:\n",
    "    for fs in [64,128]:\n",
    "        for bs in [5,25,50]:\n",
    "            title_appendix = f'{opti_str}_ fs{fs}_bs{bs}_e100_d250'\n",
    "            for i in range(6,11):\n",
    "                # Build the model\n",
    "                model = Sequential()\n",
    "                model.add(Conv2D(fs, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "                model.add(MaxPooling2D(2, 2))\n",
    "                for x in range(i):\n",
    "                    model.add(InceptionModule(filters=fs))\n",
    "                    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(250, activation='relu'))\n",
    "                model.add(Dense(19, activation='softmax'))\n",
    "            \n",
    "                # Compile the model\n",
    "                opti = RMSprop(learning_rate=0.001)\n",
    "                if opti_str == 'ADAM':\n",
    "                    opti = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=opti,\n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "                # Fit the model\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "                history = model.fit(X_train, y_train, \n",
    "                                    epochs=100, \n",
    "                                    batch_size=bs, \n",
    "                                    verbose=0, \n",
    "                                    validation_split=0.2, \n",
    "                                    callbacks=[early_stopping])\n",
    "                # Evaluate the model\n",
    "                loss, accuracy = model.evaluate(X_test, y_test)\n",
    "                print('Number of inception layers: %.i (%s, batchsize: %.i, filtersize: %.i)' % (i, opti_str, bs, fs))\n",
    "                print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "                print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "                print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "                print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "                visualize_history(history, f'inception_{title_appendix}_sublayercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "                # Clear Keras session\n",
    "                K.clear_session()\n",
    "                del model\n",
    "                gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
