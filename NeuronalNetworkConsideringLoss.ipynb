{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49759a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import gc\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import relu, tanh, linear\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from joblib import Parallel, delayed #Paralleize calculation\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Column, Integer, ARRAY, MetaData, Table, Text\n",
    "from sqlalchemy.dialects.postgresql import ARRAY as PG_ARRAY\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# Set random seed for reproducability\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "# check if tensorflow uses GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851aac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom adapter function for postgre\n",
    "def adapt_numpy_ndarray(numpy_array):\n",
    "    return AsIs(list(numpy_array))\n",
    "# Register the postgre-adapter\n",
    "register_adapter(np.ndarray, adapt_numpy_ndarray)\n",
    "\n",
    "# Database connection parameters and alchemy engine\n",
    "dbname = 'bathunting'\n",
    "user = 'python'\n",
    "password = 'python_password'\n",
    "host = 'localhost'\n",
    "port = '5432' \n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "\n",
    "# Functions to get datatf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "\n",
    "def get_target_data(target, limit=0, no_target=False):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    #query = \"\"\n",
    "    if no_target:\n",
    "        query = f\"SELECT new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    else:\n",
    "        query = f\"SELECT target, new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    if no_target:\n",
    "        df = pd.DataFrame(df['new_arr'].tolist())\n",
    "    return df\n",
    "\n",
    "# messy, i know\n",
    "def get_targets_to_data(limit=0):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    query = f\"SELECT target FROM batcall where 10 < ANY(new_arr) {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df\n",
    "\n",
    "def get_all_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    return all_df\n",
    "\n",
    "def get_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    df = pd.concat(all_df)\n",
    "    return df\n",
    "\n",
    "# try to visualize only the maximum values per species\n",
    "def spectrogram_range(target):\n",
    "    df = get_target_data(target, limit=0, no_target=True)\n",
    "    max_vals = df.max()\n",
    "    min_vals = df.min()\n",
    "    \n",
    "    abs_pos = max_vals.abs()\n",
    "    abs_neg = min_vals.abs()\n",
    "    mask = (abs_pos > abs_neg).astype(bool)\n",
    "    return min_vals.where(mask, max_vals), max_vals.where(mask, min_vals)\n",
    "\n",
    "def get_targets():\n",
    "    #conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    #cursor = conn.cursor()\n",
    "    query = f\"SELECT target, bat FROM batcall group by target, bat order by target\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    #conn.close()\n",
    "    return df\n",
    "\n",
    "def get_shape(nested_list):\n",
    "    try:\n",
    "        # Initialize shape list\n",
    "        shape = []\n",
    "\n",
    "        # Iterate to calculate the shape\n",
    "        while isinstance(nested_list, list) or isinstance(nested_list, np.ndarray):\n",
    "            shape.append(len(nested_list))\n",
    "            nested_list = nested_list[0]\n",
    "\n",
    "        return tuple(shape)\n",
    "    except (TypeError, IndexError) as e:\n",
    "        # In case the nested lists are not uniformly sized\n",
    "        return f\"Irregular shape - nested lists are not of equal size. \\n ERROR: {e}\"\n",
    "    \n",
    "# Get data to work with\n",
    "def get_features_and_targets(limit=100, scaler=StandardScaler(), categorical=True):\n",
    "    data = get_data(limit=limit)\n",
    "\n",
    "    df = pd.DataFrame(data[\"new_arr\"].tolist())\n",
    "    if scaler != None:\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    labels = pd.DataFrame(data[\"target\"])\n",
    "    if categorical:\n",
    "        labels = to_categorical(labels, num_classes=19)\n",
    "    return df, labels\n",
    "\n",
    "def vogl_conversion(df):\n",
    "    data_reshaped = []\n",
    "    for _,data in df.iterrows():\n",
    "        # Normalize\n",
    "        data -= np.mean(data)\n",
    "        data /= np.std(data)\n",
    "        # Realy no idea just assuming prof did it right\n",
    "        # Calculate spectrogram with FFT\n",
    "        stft = np.abs(librosa.stft(np.array(data), n_fft=512, hop_length=32))\n",
    "        stft = 10 * np.log10(stft)\n",
    "        stft = np.nan_to_num(stft)\n",
    "        # Scale between [0,1] and reduce shape if needed\n",
    "        stft = (stft - np.min(stft)) / (np.max(stft) - np.min(stft))\n",
    "        stft = np.reshape(stft, (257, 138, 1))\n",
    "        stft = stft[:256, -128: , :]\n",
    "        data_reshaped.append(stft)\n",
    "    return np.array(data_reshaped)\n",
    "\n",
    "def confusion_matrix(model, X_test, y_test):\n",
    "    # Confusion Matrix\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)  # y_test are the true labels (one-hot encoded)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot the confusion matrix using Seaborn\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_history(history, title_appendix=''):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model accuracy {title_appendix}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/accuracy_{title_appendix}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model loss {title_appendix}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/loss_{title_appendix}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db617238",
   "metadata": {},
   "source": [
    "This should be all about Convolutional Neuronal Networks and lowering the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ce4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2495/927267477.py:101: RuntimeWarning: divide by zero encountered in log10\n",
      "  stft = 10 * np.log10(stft)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7250, 256, 128, 1)\n",
      "(7250, 19)\n"
     ]
    }
   ],
   "source": [
    "df, labels = get_features_and_targets(limit=500, scaler=None)\n",
    "df_reshaped = vogl_conversion(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reshaped, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train[:-3]\n",
    "y_train = y_train[:-3]\n",
    "print(get_shape(X_train))\n",
    "print(get_shape(y_train))\n",
    "# Determine the number of input features\n",
    "input_dim = get_shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aae7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 6ms/step - loss: 2.9404 - accuracy: 0.0485\n",
      "Density: 50\n",
      "Final Training Loss: 2.9372239112854004\n",
      "Final Training Accuracy: 0.05155172571539879\n",
      "Final Validation Loss: 2.9394729137420654\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Test Accuracy: 4.85 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 3.0835 - accuracy: 0.6985\n",
      "Density: 100\n",
      "Final Training Loss: 1.479933757764229e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 2.7108476161956787\n",
      "Final Validation Accuracy: 0.7103448510169983\n",
      "Test Accuracy: 69.85 %\n",
      "Test Loss: 3.08\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 1.7402 - accuracy: 0.7800\n",
      "Density: 150\n",
      "Final Training Loss: 6.372534357979021e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7338581085205078\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.00 %\n",
      "Test Loss: 1.74\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.7007 - accuracy: 0.7778\n",
      "Density: 200\n",
      "Final Training Loss: 5.571984047492151e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5493212938308716\n",
      "Final Validation Accuracy: 0.7765517234802246\n",
      "Test Accuracy: 77.78 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6537 - accuracy: 0.7861\n",
      "Density: 250\n",
      "Final Training Loss: 4.688399428687262e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.555867314338684\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.61 %\n",
      "Test Loss: 1.65\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.6482 - accuracy: 0.7883\n",
      "Density: 300\n",
      "Final Training Loss: 4.449776440651476e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6411774158477783\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.83 %\n",
      "Test Loss: 1.65\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6550 - accuracy: 0.7806\n",
      "Density: 350\n",
      "Final Training Loss: 3.5713347301680187e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5333704948425293\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 78.06 %\n",
      "Test Loss: 1.66\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7135 - accuracy: 0.7900\n",
      "Density: 400\n",
      "Final Training Loss: 4.5552187089015206e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6754086017608643\n",
      "Final Validation Accuracy: 0.7820689678192139\n",
      "Test Accuracy: 79.00 %\n",
      "Test Loss: 1.71\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6616 - accuracy: 0.7966\n",
      "Density: 450\n",
      "Final Training Loss: 4.723961239960772e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6203457117080688\n",
      "Final Validation Accuracy: 0.7889655232429504\n",
      "Test Accuracy: 79.66 %\n",
      "Test Loss: 1.66\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6714 - accuracy: 0.7933\n",
      "Density: 500\n",
      "Final Training Loss: 3.910874397661246e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6596729755401611\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 79.33 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6181 - accuracy: 0.7856\n",
      "Density: 550\n",
      "Final Training Loss: 3.143205162814411e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6524972915649414\n",
      "Final Validation Accuracy: 0.769655168056488\n",
      "Test Accuracy: 78.56 %\n",
      "Test Loss: 1.62\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7515 - accuracy: 0.7773\n",
      "Density: 600\n",
      "Final Training Loss: 3.9116946481954074e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7328212261199951\n",
      "Final Validation Accuracy: 0.7751724123954773\n",
      "Test Accuracy: 77.73 %\n",
      "Test Loss: 1.75\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5878 - accuracy: 0.7938\n",
      "Density: 650\n",
      "Final Training Loss: 3.462193944869796e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5322779417037964\n",
      "Final Validation Accuracy: 0.7924137711524963\n",
      "Test Accuracy: 79.38 %\n",
      "Test Loss: 1.59\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6188 - accuracy: 0.7938\n",
      "Density: 700\n",
      "Final Training Loss: 3.833388859675324e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5643048286437988\n",
      "Final Validation Accuracy: 0.7813792824745178\n",
      "Test Accuracy: 79.38 %\n",
      "Test Loss: 1.62\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5553 - accuracy: 0.7960\n",
      "Density: 750\n",
      "Final Training Loss: 3.8473646668535366e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.496852993965149\n",
      "Final Validation Accuracy: 0.8013793230056763\n",
      "Test Accuracy: 79.60 %\n",
      "Test Loss: 1.56\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5389 - accuracy: 0.7988\n",
      "Density: 800\n",
      "Final Training Loss: 3.255020715187129e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4608345031738281\n",
      "Final Validation Accuracy: 0.791034460067749\n",
      "Test Accuracy: 79.88 %\n",
      "Test Loss: 1.54\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.9138 - accuracy: 0.7751\n",
      "Density: 850\n",
      "Final Training Loss: 3.7550708498201857e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.863211750984192\n",
      "Final Validation Accuracy: 0.7806896567344666\n",
      "Test Accuracy: 77.51 %\n",
      "Test Loss: 1.91\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.8605 - accuracy: 0.7817\n",
      "Density: 900\n",
      "Final Training Loss: 3.210418242360902e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7416468858718872\n",
      "Final Validation Accuracy: 0.7834482789039612\n",
      "Test Accuracy: 78.17 %\n",
      "Test Loss: 1.86\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6717 - accuracy: 0.7993\n",
      "Density: 950\n",
      "Final Training Loss: 2.7500266241986537e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6734187602996826\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 79.93 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6954 - accuracy: 0.8037\n",
      "Density: 1000\n",
      "Final Training Loss: 3.232607070913218e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6210309267044067\n",
      "Final Validation Accuracy: 0.8062068819999695\n",
      "Test Accuracy: 80.37 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7337 - accuracy: 0.7822\n",
      "Density: 1050\n",
      "Final Training Loss: 3.3045392910935334e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7278374433517456\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 78.22 %\n",
      "Test Loss: 1.73\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 1.7008 - accuracy: 0.7905\n",
      "Density: 1100\n",
      "Final Training Loss: 2.906847385020228e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6569880247116089\n",
      "Final Validation Accuracy: 0.7917241454124451\n",
      "Test Accuracy: 79.05 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6665 - accuracy: 0.8054\n",
      "Density: 1150\n",
      "Final Training Loss: 3.6461466379478225e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5800879001617432\n",
      "Final Validation Accuracy: 0.8013793230056763\n",
      "Test Accuracy: 80.54 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 1.7331 - accuracy: 0.7971\n",
      "Density: 1200\n",
      "Final Training Loss: 2.8416948794074415e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7120484113693237\n",
      "Final Validation Accuracy: 0.7889655232429504\n",
      "Test Accuracy: 79.71 %\n",
      "Test Loss: 1.73\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6452 - accuracy: 0.7982\n",
      "Density: 1250\n",
      "Final Training Loss: 3.577287941425311e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.615990161895752\n",
      "Final Validation Accuracy: 0.7903448343276978\n",
      "Test Accuracy: 79.82 %\n",
      "Test Loss: 1.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     17\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1733\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1731\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1733\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m         ):\n\u001b[1;32m   1741\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1401\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1402\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1403\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1406\u001b[0m )\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Density 50 to 5000 -> does not realy matter after 100 and not matter anymore after 500\n",
    "for i in range(50, 5001, 50):\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Density: %.i' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'dens={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b50574-8dad-4ba9-9b15-4973dc4630db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 7ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 20\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 40\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 60\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340591430664\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 80\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 100\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 120\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.7503 - accuracy: 0.7696\n",
      "Density: 140\n",
      "Final Training Loss: 2.9926145543868188e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.715936303138733\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 76.96 %\n",
      "Test Loss: 1.75\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 2.9405 - accuracy: 0.0496\n",
      "Density: 160\n",
      "Final Training Loss: 2.937248706817627\n",
      "Final Training Accuracy: 0.05672413855791092\n",
      "Final Validation Loss: 2.939340353012085\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.6982 - accuracy: 0.7811\n",
      "Density: 180\n",
      "Final Training Loss: 2.40611871049623e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5949324369430542\n",
      "Final Validation Accuracy: 0.7772414088249207\n",
      "Test Accuracy: 78.11 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.6694 - accuracy: 0.7729\n",
      "Density: 200\n",
      "Final Training Loss: 2.4795597255433677e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5844987630844116\n",
      "Final Validation Accuracy: 0.7703448534011841\n",
      "Test Accuracy: 77.29 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.6160 - accuracy: 0.7856\n",
      "Density: 220\n",
      "Final Training Loss: 2.146619635823299e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6360830068588257\n",
      "Final Validation Accuracy: 0.7682758569717407\n",
      "Test Accuracy: 78.56 %\n",
      "Test Loss: 1.62\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5049 - accuracy: 0.7977\n",
      "Density: 240\n",
      "Final Training Loss: 1.5416588894368033e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.445501446723938\n",
      "Final Validation Accuracy: 0.791034460067749\n",
      "Test Accuracy: 79.77 %\n",
      "Test Loss: 1.50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.4280 - accuracy: 0.7916\n",
      "Density: 260\n",
      "Final Training Loss: 4.956538305123104e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4390696287155151\n",
      "Final Validation Accuracy: 0.7882758378982544\n",
      "Test Accuracy: 79.16 %\n",
      "Test Loss: 1.43\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5427 - accuracy: 0.7977\n",
      "Density: 280\n",
      "Final Training Loss: 1.4954131302147289e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4518858194351196\n",
      "Final Validation Accuracy: 0.7965517044067383\n",
      "Test Accuracy: 79.77 %\n",
      "Test Loss: 1.54\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.4441 - accuracy: 0.7729\n",
      "Density: 300\n",
      "Final Training Loss: 0.0030751931481063366\n",
      "Final Training Accuracy: 0.9994827508926392\n",
      "Final Validation Loss: 1.4199838638305664\n",
      "Final Validation Accuracy: 0.7675862312316895\n",
      "Test Accuracy: 77.29 %\n",
      "Test Loss: 1.44\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.4627 - accuracy: 0.7960\n",
      "Density: 320\n",
      "Final Training Loss: 1.6932513972278684e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4565520286560059\n",
      "Final Validation Accuracy: 0.7848275899887085\n",
      "Test Accuracy: 79.60 %\n",
      "Test Loss: 1.46\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5130 - accuracy: 0.7861\n",
      "Density: 340\n",
      "Final Training Loss: 0.002030828967690468\n",
      "Final Training Accuracy: 0.9993103742599487\n",
      "Final Validation Loss: 1.5118376016616821\n",
      "Final Validation Accuracy: 0.7779310345649719\n",
      "Test Accuracy: 78.61 %\n",
      "Test Loss: 1.51\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.4701 - accuracy: 0.7894\n",
      "Density: 360\n",
      "Final Training Loss: 1.8369897816228331e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4869956970214844\n",
      "Final Validation Accuracy: 0.7806896567344666\n",
      "Test Accuracy: 78.94 %\n",
      "Test Loss: 1.47\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5063 - accuracy: 0.7839\n",
      "Density: 380\n",
      "Final Training Loss: 1.1630222616076935e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4147711992263794\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 78.39 %\n",
      "Test Loss: 1.51\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5924 - accuracy: 0.7933\n",
      "Density: 400\n",
      "Final Training Loss: 9.766626135387924e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5522949695587158\n",
      "Final Validation Accuracy: 0.7868965268135071\n",
      "Test Accuracy: 79.33 %\n",
      "Test Loss: 1.59\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.5925 - accuracy: 0.7856\n",
      "Density: 420\n",
      "Final Training Loss: 1.07222631413606e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5459344387054443\n",
      "Final Validation Accuracy: 0.7903448343276978\n",
      "Test Accuracy: 78.56 %\n",
      "Test Loss: 1.59\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.5484 - accuracy: 0.7911\n",
      "Density: 440\n",
      "Final Training Loss: 1.1962202734139282e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5393526554107666\n",
      "Final Validation Accuracy: 0.7579310536384583\n",
      "Test Accuracy: 79.11 %\n",
      "Test Loss: 1.55\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5206 - accuracy: 0.7922\n",
      "Density: 460\n",
      "Final Training Loss: 1.6579087969148532e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5074756145477295\n",
      "Final Validation Accuracy: 0.7875862121582031\n",
      "Test Accuracy: 79.22 %\n",
      "Test Loss: 1.52\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.4849 - accuracy: 0.7729\n",
      "Density: 480\n",
      "Final Training Loss: 0.024050544947385788\n",
      "Final Training Accuracy: 0.9963793158531189\n",
      "Final Validation Loss: 1.4255852699279785\n",
      "Final Validation Accuracy: 0.7648276090621948\n",
      "Test Accuracy: 77.29 %\n",
      "Test Loss: 1.48\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5807 - accuracy: 0.7878\n",
      "Density: 500\n",
      "Final Training Loss: 4.0147851905203424e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5009757280349731\n",
      "Final Validation Accuracy: 0.7813792824745178\n",
      "Test Accuracy: 78.78 %\n",
      "Test Loss: 1.58\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.5376 - accuracy: 0.7734\n",
      "Density: 520\n",
      "Final Training Loss: 8.358761078852694e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4620517492294312\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 77.34 %\n",
      "Test Loss: 1.54\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.5713 - accuracy: 0.7922\n",
      "Density: 540\n",
      "Final Training Loss: 2.225827302027028e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5073959827423096\n",
      "Final Validation Accuracy: 0.7903448343276978\n",
      "Test Accuracy: 79.22 %\n",
      "Test Loss: 1.57\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.5676 - accuracy: 0.7971\n",
      "Density: 560\n",
      "Final Training Loss: 1.7821530491346493e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5182565450668335\n",
      "Final Validation Accuracy: 0.7937930822372437\n",
      "Test Accuracy: 79.71 %\n",
      "Test Loss: 1.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     20\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1733\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1731\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1733\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m         ):\n\u001b[1;32m   1741\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1401\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1402\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1403\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1406\u001b[0m )\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# increase convolutional filters to 32\n",
    "# decrease epochs to 25\n",
    "# Density 20 to 2000\n",
    "# -> accuracy is fine at density about 200, loss is low and neighter accuracy nore loss change significantly afterwards. \n",
    "# Also overfitting seems to happen after about 4 or 5 epochs\n",
    "title_appendix = 'e25_1x_conv32'\n",
    "for i in range(20, 1001, 20):\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=25, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Density: %.i' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_dens={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1004d398-1947-47c2-8bcd-f6686facb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 10ms/step - loss: 1.8074 - accuracy: 0.7541\n",
      "Density: 150\n",
      "Final Training Loss: 0.017362574115395546\n",
      "Final Training Accuracy: 0.9960345029830933\n",
      "Final Validation Loss: 1.6691466569900513\n",
      "Final Validation Accuracy: 0.7558620572090149\n",
      "Test Accuracy: 75.41 %\n",
      "Test Loss: 1.81\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 1.8635 - accuracy: 0.7266\n",
      "Density: 160\n",
      "Final Training Loss: 0.013060218654572964\n",
      "Final Training Accuracy: 0.9987931251525879\n",
      "Final Validation Loss: 1.7736579179763794\n",
      "Final Validation Accuracy: 0.7434482574462891\n",
      "Test Accuracy: 72.66 %\n",
      "Test Loss: 1.86\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 1.6025 - accuracy: 0.7674\n",
      "Density: 170\n",
      "Final Training Loss: 0.02390725165605545\n",
      "Final Training Accuracy: 0.9941379427909851\n",
      "Final Validation Loss: 1.483515739440918\n",
      "Final Validation Accuracy: 0.7655172348022461\n",
      "Test Accuracy: 76.74 %\n",
      "Test Loss: 1.60\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.5987 - accuracy: 0.7861\n",
      "Density: 180\n",
      "Final Training Loss: 1.3157559806131758e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.506087064743042\n",
      "Final Validation Accuracy: 0.7855172157287598\n",
      "Test Accuracy: 78.61 %\n",
      "Test Loss: 1.60\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 1.4517 - accuracy: 0.7619\n",
      "Density: 190\n",
      "Final Training Loss: 0.009312678128480911\n",
      "Final Training Accuracy: 0.9970689415931702\n",
      "Final Validation Loss: 1.3048592805862427\n",
      "Final Validation Accuracy: 0.77379310131073\n",
      "Test Accuracy: 76.19 %\n",
      "Test Loss: 1.45\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.4239 - accuracy: 0.7712\n",
      "Density: 200\n",
      "Final Training Loss: 0.007618408650159836\n",
      "Final Training Accuracy: 0.9986206889152527\n",
      "Final Validation Loss: 1.4775207042694092\n",
      "Final Validation Accuracy: 0.7668965458869934\n",
      "Test Accuracy: 77.12 %\n",
      "Test Loss: 1.42\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 1.5822 - accuracy: 0.7767\n",
      "Density: 210\n",
      "Final Training Loss: 7.139905937947333e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4841032028198242\n",
      "Final Validation Accuracy: 0.7806896567344666\n",
      "Test Accuracy: 77.67 %\n",
      "Test Loss: 1.58\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.7957 - accuracy: 0.7536\n",
      "Density: 220\n",
      "Final Training Loss: 0.009902803227305412\n",
      "Final Training Accuracy: 0.9977586269378662\n",
      "Final Validation Loss: 1.7038148641586304\n",
      "Final Validation Accuracy: 0.748275876045227\n",
      "Test Accuracy: 75.36 %\n",
      "Test Loss: 1.80\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 1.2723 - accuracy: 0.7784\n",
      "Density: 230\n",
      "Final Training Loss: 0.013662473298609257\n",
      "Final Training Accuracy: 0.997586190700531\n",
      "Final Validation Loss: 1.2870397567749023\n",
      "Final Validation Accuracy: 0.7841379046440125\n",
      "Test Accuracy: 77.84 %\n",
      "Test Loss: 1.27\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.5506 - accuracy: 0.7663\n",
      "Density: 240\n",
      "Final Training Loss: 0.03196065500378609\n",
      "Final Training Accuracy: 0.9944827556610107\n",
      "Final Validation Loss: 1.4849674701690674\n",
      "Final Validation Accuracy: 0.7668965458869934\n",
      "Test Accuracy: 76.63 %\n",
      "Test Loss: 1.55\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 1.2952 - accuracy: 0.7773\n",
      "Density: 250\n",
      "Final Training Loss: 0.027750123292207718\n",
      "Final Training Accuracy: 0.9955172538757324\n",
      "Final Validation Loss: 1.2912003993988037\n",
      "Final Validation Accuracy: 0.7917241454124451\n",
      "Test Accuracy: 77.73 %\n",
      "Test Loss: 1.30\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.6067 - accuracy: 0.7784\n",
      "Density: 260\n",
      "Final Training Loss: 0.006527571007609367\n",
      "Final Training Accuracy: 0.9981034398078918\n",
      "Final Validation Loss: 1.5279226303100586\n",
      "Final Validation Accuracy: 0.7786206603050232\n",
      "Test Accuracy: 77.84 %\n",
      "Test Loss: 1.61\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.3813 - accuracy: 0.7845\n",
      "Density: 270\n",
      "Final Training Loss: 0.0063801417127251625\n",
      "Final Training Accuracy: 0.9981034398078918\n",
      "Final Validation Loss: 1.3153396844863892\n",
      "Final Validation Accuracy: 0.7827585935592651\n",
      "Test Accuracy: 78.45 %\n",
      "Test Loss: 1.38\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.3750 - accuracy: 0.7822\n",
      "Density: 280\n",
      "Final Training Loss: 0.0002487720921635628\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.3755075931549072\n",
      "Final Validation Accuracy: 0.7813792824745178\n",
      "Test Accuracy: 78.22 %\n",
      "Test Loss: 1.38\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 1.4627 - accuracy: 0.7845\n",
      "Density: 290\n",
      "Final Training Loss: 6.157640746096149e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.3712599277496338\n",
      "Final Validation Accuracy: 0.7882758378982544\n",
      "Test Accuracy: 78.45 %\n",
      "Test Loss: 1.46\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.6587 - accuracy: 0.7591\n",
      "Density: 300\n",
      "Final Training Loss: 0.007822887040674686\n",
      "Final Training Accuracy: 0.9974138140678406\n",
      "Final Validation Loss: 1.6248832941055298\n",
      "Final Validation Accuracy: 0.77379310131073\n",
      "Test Accuracy: 75.91 %\n",
      "Test Loss: 1.66\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 1.4738 - accuracy: 0.7845\n",
      "Density: 310\n",
      "Final Training Loss: 0.037166062742471695\n",
      "Final Training Accuracy: 0.9950000047683716\n",
      "Final Validation Loss: 1.496800184249878\n",
      "Final Validation Accuracy: 0.7765517234802246\n",
      "Test Accuracy: 78.45 %\n",
      "Test Loss: 1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 19:20:23.514166: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 671088640 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 67043328/8510832640\n",
      "2024-01-10 19:20:23.514196: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      7340097536\n",
      "InUse:                      4619572912\n",
      "MaxInUse:                   6574647020\n",
      "NumAllocs:                    25780554\n",
      "MaxAllocSize:                760217600\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-01-10 19:20:23.514211: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-01-10 19:20:23.514217: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 189\n",
      "2024-01-10 19:20:23.514221: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 11\n",
      "2024-01-10 19:20:23.514226: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1\n",
      "2024-01-10 19:20:23.514230: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 76, 4\n",
      "2024-01-10 19:20:23.514233: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 2\n",
      "2024-01-10 19:20:23.514237: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 3\n",
      "2024-01-10 19:20:23.514240: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2024-01-10 19:20:23.514244: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1152, 2\n",
      "2024-01-10 19:20:23.514248: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1240, 2\n",
      "2024-01-10 19:20:23.514251: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 3\n",
      "2024-01-10 19:20:23.514255: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2320, 2\n",
      "2024-01-10 19:20:23.514259: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 23560, 2\n",
      "2024-01-10 19:20:23.514263: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 44080, 2\n",
      "2024-01-10 19:20:23.514267: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 440800, 1\n",
      "2024-01-10 19:20:23.514271: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 608174080, 2\n",
      "2024-01-10 19:20:23.514274: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 650117120, 2\n",
      "2024-01-10 19:20:23.514278: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 671088640, 2\n",
      "2024-01-10 19:20:23.514282: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 760217600, 1\n",
      "2024-01-10 19:20:23.514288: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 6845104128\n",
      "2024-01-10 19:20:23.514293: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 4619572912\n",
      "2024-01-10 19:20:23.514296: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 6845104128\n",
      "2024-01-10 19:20:23.514300: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6574647020\n",
      "2024-01-10 19:20:23.514320: W tensorflow/core/framework/op_kernel.cc:1816] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m19\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2101\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2110\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2111\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2115\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "# increase convolutional filters to 64\n",
    "# decrease epochs to 20\n",
    "# Density 150 to 500\n",
    "title_appendix = 'e20_1x_conv64'\n",
    "for i in range(150, 501, 10):\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', paddine20_1x_conv64g='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Density: %.i' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_dens={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b57e4-a316-4441-9095-88499b0b1c35",
   "metadata": {},
   "source": [
    "Die Anzahl der Filter pro layer hat besten falls einen geringen Einfluss auf die Modelgüte.\n",
    "\n",
    "Wie siehts mit der Lernrate von RMS-Propagation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc3bf60-711b-42df-b1d6-eefb89bb731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 15:32:42.028380: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-01-11 15:32:42.028506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6995 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2024-01-11 15:32:45.328644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-01-11 15:32:46.927769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe591c38190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-11 15:32:46.927807: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-11 15:32:46.984109: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-11 15:32:47.322869: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 12ms/step - loss: 1.7850 - accuracy: 0.7894\n",
      "Density: 0\n",
      "Final Training Loss: 4.2470969674468506e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7683717012405396\n",
      "Final Validation Accuracy: 0.7641379237174988\n",
      "Test Accuracy: 78.94 %\n",
      "Test Loss: 1.78\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.8260 - accuracy: 0.7822\n",
      "Density: 0\n",
      "Final Training Loss: 5.756143650614831e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7531464099884033\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 78.22 %\n",
      "Test Loss: 1.83\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.7639 - accuracy: 0.7800\n",
      "Density: 0\n",
      "Final Training Loss: 4.3112447656312725e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.697001338005066\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 78.00 %\n",
      "Test Loss: 1.76\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.7753 - accuracy: 0.7712\n",
      "Density: 0\n",
      "Final Training Loss: 3.413683771213982e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6494520902633667\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 77.12 %\n",
      "Test Loss: 1.78\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.7470 - accuracy: 0.7900\n",
      "Density: 0\n",
      "Final Training Loss: 5.44660338164249e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6357719898223877\n",
      "Final Validation Accuracy: 0.7834482789039612\n",
      "Test Accuracy: 79.00 %\n",
      "Test Loss: 1.75\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.8131 - accuracy: 0.7861\n",
      "Density: 0\n",
      "Final Training Loss: 3.552202372247848e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6171551942825317\n",
      "Final Validation Accuracy: 0.7855172157287598\n",
      "Test Accuracy: 78.61 %\n",
      "Test Loss: 1.81\n"
     ]
    }
   ],
   "source": [
    "# Addapt RMS-Propagation learning rate\n",
    "# convolutional filters: 32; epochs: 50; density: 250\n",
    "title_appendix = 'l1_d50_d250'\n",
    "for i in [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]:\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('RMS propagation learning rate: %.f' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_rmsplr={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02960b46-0169-4564-aa3b-3cebf9ea812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addapt RMS-Propagation learning rate\n",
    "# Repeate with 100 epochs\n",
    "# convolutional filters: 32; epochs: 100; density: 250\n",
    "title_appendix = 'l1_e100_d250'\n",
    "for i in [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]:\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('RMS propagation learning rate: %.5f' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_rmsplr={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2773b8a5-6d69-4533-a52f-e998c0c9c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9395 - accuracy: 0.0562\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.9416658878326416\n",
      "Final Training Accuracy: 0.056551724672317505\n",
      "Final Validation Loss: 2.9402124881744385\n",
      "Final Validation Accuracy: 0.053793102502822876\n",
      "Test Accuracy: 5.62 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 11.1687 - accuracy: 0.3611\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 0.1445300281047821\n",
      "Final Training Accuracy: 0.9613792896270752\n",
      "Final Validation Loss: 10.330340385437012\n",
      "Final Validation Accuracy: 0.4000000059604645\n",
      "Test Accuracy: 36.11 %\n",
      "Test Loss: 11.17\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.9287 - accuracy: 0.6521\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 0.014938338659703732\n",
      "Final Training Accuracy: 0.9967241287231445\n",
      "Final Validation Loss: 1.7592754364013672\n",
      "Final Validation Accuracy: 0.6731034517288208\n",
      "Test Accuracy: 65.21 %\n",
      "Test Loss: 1.93\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9403 - accuracy: 0.0540\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.93688702583313\n",
      "Final Training Accuracy: 0.05396551638841629\n",
      "Final Validation Loss: 2.9393434524536133\n",
      "Final Validation Accuracy: 0.0468965508043766\n",
      "Test Accuracy: 5.40 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9401 - accuracy: 0.0540\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.9367809295654297\n",
      "Final Training Accuracy: 0.05448275804519653\n",
      "Final Validation Loss: 2.9392194747924805\n",
      "Final Validation Accuracy: 0.0468965508043766\n",
      "Test Accuracy: 5.40 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9400 - accuracy: 0.0496\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.937600612640381\n",
      "Final Training Accuracy: 0.05465517193078995\n",
      "Final Validation Loss: 2.939425230026245\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 4.96 %\n",
      "Test Loss: 2.94\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimizer with variing starting learn rates\n",
    "# convolutional filters: 32; epochs: 50; density: 250, rms_propagation learning rate:\n",
    "title_appendix = 'l1_e50_d250'\n",
    "for i in [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]:\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=i)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Adam starting learning rate: %.5f' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_adam={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562b50b-4212-4b8b-a2ec-edb6e7706714",
   "metadata": {},
   "source": [
    "RMS-Propagation glänzt durch eine konstant gute Genauigkeit bei konstant relativ kleinem Loss wärend Adam in Sachen Genauigkeit weit zurück liegt und auch einen größeren loss vorweist. Da die Ergebnisse für verschiedene Start-Lernraten beim Adam-Optimizer stark abweichen muss das Model evtl. noch angepasst werden. \n",
    "\n",
    "Wie sieht's bei 100 Epochen und 500 Neuronen aus? \n",
    "<!-- Der Fairness halber wird das Model auch mit dem RMS-Propagation-Optimizer mit diesen Parametern überprüft. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5db5a77-b79c-449c-8f89-2f5ef9cac913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 11ms/step - loss: 2.9410 - accuracy: 0.0601\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.9410290718078613\n",
      "Final Training Accuracy: 0.052586205303668976\n",
      "Final Validation Loss: 2.941659450531006\n",
      "Final Validation Accuracy: 0.05241379141807556\n",
      "Test Accuracy: 6.01 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.1533 - accuracy: 0.4708\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 0.7707065939903259\n",
      "Final Training Accuracy: 0.7312068939208984\n",
      "Final Validation Loss: 2.234323024749756\n",
      "Final Validation Accuracy: 0.46482759714126587\n",
      "Test Accuracy: 47.08 %\n",
      "Test Loss: 2.15\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.3895 - accuracy: 0.7503\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 2.9437454941216856e-05\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.378556251525879\n",
      "Final Validation Accuracy: 0.7551724314689636\n",
      "Test Accuracy: 75.03 %\n",
      "Test Loss: 1.39\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 2.6352 - accuracy: 0.6957\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 1.8708436755332514e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 2.283128023147583\n",
      "Final Validation Accuracy: 0.7220689654350281\n",
      "Test Accuracy: 69.57 %\n",
      "Test Loss: 2.64\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.8917 - accuracy: 0.7828\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 0.0006866362527944148\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 0.8346307873725891\n",
      "Final Validation Accuracy: 0.7765517234802246\n",
      "Test Accuracy: 78.28 %\n",
      "Test Loss: 0.89\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.9411 - accuracy: 0.3418\n",
      "Adam starting learning rate: 0\n",
      "Final Training Loss: 1.8467915058135986\n",
      "Final Training Accuracy: 0.3701724112033844\n",
      "Final Validation Loss: 1.8950986862182617\n",
      "Final Validation Accuracy: 0.3806896507740021\n",
      "Test Accuracy: 34.18 %\n",
      "Test Loss: 1.94\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimizer with variing starting learn rates\n",
    "# convolutional filters: 32; epochs: 50; density: 250, rms_propagation learning rate:\n",
    "title_appendix = 'l1_e100_d500'\n",
    "for i in [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]:\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=i)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Adam starting learning rate: %.5f' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_adam={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b4dd3-8b4c-44e5-869c-46c04eac6b53",
   "metadata": {},
   "source": [
    "Die Plots für die Genauigkeit und den Loss legen nahe das für eine anfängliche Lernrate von 0.00005 noch mehr drin ist. Ich bin neugierig ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea6929d-cc2e-48b5-a884-af12dec16766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 10ms/step - loss: 1.6676 - accuracy: 0.6251\n",
      "Adam starting learning rate: 0.0005\n",
      "Final Training Loss: 0.12450646609067917\n",
      "Final Training Accuracy: 0.9863793253898621\n",
      "Final Validation Loss: 1.5002870559692383\n",
      "Final Validation Accuracy: 0.6324138045310974\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimizer with variing starting learn rates\n",
    "# convolutional filters: 32; epochs: 250; density: 500, rms_propagation learning rate:\n",
    "title_appendix = 'l1_e250_d500'\n",
    "clear_session() # clear keras session\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "#tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Adam starting learning rate: 0.0005')\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "visualize_history(history, f'{title_appendix}_adam={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "\n",
    "# Free Memory for following models\n",
    "del model\n",
    "gc.collect()\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07ed32-2d6b-40f1-ac60-ffb2d0c31daf",
   "metadata": {},
   "source": [
    "Die neuen Graphen zeigen das Model sich bei 60% Genauigkeit einpendelt während der Validation-Loss sich nach ca. 200 Epochen wieder nach oben bewegt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857932a-9599-4c73-a6c3-707feba05c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varrie the number of layers\n",
    "# convolutional filters: 32; epochs: 100; density: 250, rms_propagation learning rate:\n",
    "title_appendix = 'e100_d250'\n",
    "for i in range(15):\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    for x in range(i):\n",
    "        model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Number of layers: %.i' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    visualize_history(history, f'{title_appendix}_layercount={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728a601-6ee4-476e-8366-16a832a6a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model results:\n",
    "print('Test Accuracy: %.2f %%' % (accuracy*100))\n",
    "print('Test Loss: %.2f' % (loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1fae1b-0bbe-4363-91d8-2d0d92146f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear grafic memory\n",
    "clear_session() # clear keras session\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "# Free Memory for following models\n",
    "del model\n",
    "gc.collect()\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
