{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49759a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import gc\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import relu, tanh, linear\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from joblib import Parallel, delayed #Paralleize calculation\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Column, Integer, ARRAY, MetaData, Table, Text\n",
    "from sqlalchemy.dialects.postgresql import ARRAY as PG_ARRAY\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# Set random seed for reproducability\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "# check if tensorflow uses GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851aac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom adapter function for postgre\n",
    "def adapt_numpy_ndarray(numpy_array):\n",
    "    return AsIs(list(numpy_array))\n",
    "# Register the postgre-adapter\n",
    "register_adapter(np.ndarray, adapt_numpy_ndarray)\n",
    "\n",
    "# Database connection parameters and alchemy engine\n",
    "dbname = 'bathunting'\n",
    "user = 'python'\n",
    "password = 'python_password'\n",
    "host = 'localhost'\n",
    "port = '5432' \n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}')\n",
    "\n",
    "# Functions to get datatf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "\n",
    "def get_target_data(target, limit=0, no_target=False):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    #query = \"\"\n",
    "    if no_target:\n",
    "        query = f\"SELECT new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    else:\n",
    "        query = f\"SELECT target, new_arr FROM batcall where 10 < ANY(new_arr) and target = {target} {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    if no_target:\n",
    "        df = pd.DataFrame(df['new_arr'].tolist())\n",
    "    return df\n",
    "\n",
    "# messy, i know\n",
    "def get_targets_to_data(limit=0):\n",
    "    lmt = \"\" if limit<=0 else f\"LIMIT {limit}\"\n",
    "    query = f\"SELECT target FROM batcall where 10 < ANY(new_arr) {lmt}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df\n",
    "\n",
    "def get_all_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    return all_df\n",
    "\n",
    "def get_data(targets=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], limit=0, no_target=False):\n",
    "    all_df = Parallel(n_jobs=-3, prefer=\"threads\")(delayed(get_target_data)(target, limit, no_target) for target in targets)\n",
    "    df = pd.concat(all_df)\n",
    "    return df\n",
    "\n",
    "# try to visualize only the maximum values per species\n",
    "def spectrogram_range(target):\n",
    "    df = get_target_data(target, limit=0, no_target=True)\n",
    "    max_vals = df.max()\n",
    "    min_vals = df.min()\n",
    "    \n",
    "    abs_pos = max_vals.abs()\n",
    "    abs_neg = min_vals.abs()\n",
    "    mask = (abs_pos > abs_neg).astype(bool)\n",
    "    return min_vals.where(mask, max_vals), max_vals.where(mask, min_vals)\n",
    "\n",
    "def get_targets():\n",
    "    #conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    #cursor = conn.cursor()\n",
    "    query = f\"SELECT target, bat FROM batcall group by target, bat order by target\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    #conn.close()\n",
    "    return df\n",
    "\n",
    "def get_shape(nested_list):\n",
    "    try:\n",
    "        # Initialize shape list\n",
    "        shape = []\n",
    "\n",
    "        # Iterate to calculate the shape\n",
    "        while isinstance(nested_list, list) or isinstance(nested_list, np.ndarray):\n",
    "            shape.append(len(nested_list))\n",
    "            nested_list = nested_list[0]\n",
    "\n",
    "        return tuple(shape)\n",
    "    except (TypeError, IndexError) as e:\n",
    "        # In case the nested lists are not uniformly sized\n",
    "        return f\"Irregular shape - nested lists are not of equal size. \\n ERROR: {e}\"\n",
    "    \n",
    "# Get data to work with\n",
    "def get_features_and_targets(limit=100, scaler=StandardScaler(), categorical=True):\n",
    "    data = get_data(limit=limit)\n",
    "\n",
    "    df = pd.DataFrame(data[\"new_arr\"].tolist())\n",
    "    if scaler != None:\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    labels = pd.DataFrame(data[\"target\"])\n",
    "    if categorical:\n",
    "        labels = to_categorical(labels, num_classes=19)\n",
    "    return df, labels\n",
    "\n",
    "def vogl_conversion(df):\n",
    "    data_reshaped = []\n",
    "    for _,data in df.iterrows():\n",
    "        # Normalize\n",
    "        data -= np.mean(data)\n",
    "        data /= np.std(data)\n",
    "        # Realy no idea just assuming prof did it right\n",
    "        # Calculate spectrogram with FFT\n",
    "        stft = np.abs(librosa.stft(np.array(data), n_fft=512, hop_length=32))\n",
    "        stft = 10 * np.log10(stft)\n",
    "        stft = np.nan_to_num(stft)\n",
    "        # Scale between [0,1] and reduce shape if needed\n",
    "        stft = (stft - np.min(stft)) / (np.max(stft) - np.min(stft))\n",
    "        stft = np.reshape(stft, (257, 138, 1))\n",
    "        stft = stft[:256, -128: , :]\n",
    "        data_reshaped.append(stft)\n",
    "    return np.array(data_reshaped)\n",
    "\n",
    "def confusion_matrix(model, X_test, y_test):\n",
    "    # Confusion Matrix\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)  # y_test are the true labels (one-hot encoded)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot the confusion matrix using Seaborn\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_history(history, title_appendix=''):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model accuracy {title_appendix}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/accuracy_{title_appendix}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model loss {title_appendix}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(f'History/loss_{title_appendix}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db617238",
   "metadata": {},
   "source": [
    "This should be all about Convolutional Neuronal Networks and lowering the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ce4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16710/3122854350.py:101: RuntimeWarning: divide by zero encountered in log10\n",
      "  stft = 10 * np.log10(stft)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7250, 256, 128, 1)\n",
      "(7250, 19)\n"
     ]
    }
   ],
   "source": [
    "df, labels = get_features_and_targets(limit=500, scaler=None)\n",
    "df_reshaped = vogl_conversion(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reshaped, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train[:-3]\n",
    "y_train = y_train[:-3]\n",
    "print(get_shape(X_train))\n",
    "print(get_shape(y_train))\n",
    "# Determine the number of input features\n",
    "input_dim = get_shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aae7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 6ms/step - loss: 2.9404 - accuracy: 0.0485\n",
      "Density: 50\n",
      "Final Training Loss: 2.9372239112854004\n",
      "Final Training Accuracy: 0.05155172571539879\n",
      "Final Validation Loss: 2.9394729137420654\n",
      "Final Validation Accuracy: 0.05517241358757019\n",
      "Test Accuracy: 4.85 %\n",
      "Test Loss: 2.94\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 3.0835 - accuracy: 0.6985\n",
      "Density: 100\n",
      "Final Training Loss: 1.479933757764229e-06\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 2.7108476161956787\n",
      "Final Validation Accuracy: 0.7103448510169983\n",
      "Test Accuracy: 69.85 %\n",
      "Test Loss: 3.08\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 1.7402 - accuracy: 0.7800\n",
      "Density: 150\n",
      "Final Training Loss: 6.372534357979021e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7338581085205078\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.00 %\n",
      "Test Loss: 1.74\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.7007 - accuracy: 0.7778\n",
      "Density: 200\n",
      "Final Training Loss: 5.571984047492151e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5493212938308716\n",
      "Final Validation Accuracy: 0.7765517234802246\n",
      "Test Accuracy: 77.78 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6537 - accuracy: 0.7861\n",
      "Density: 250\n",
      "Final Training Loss: 4.688399428687262e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.555867314338684\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.61 %\n",
      "Test Loss: 1.65\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.6482 - accuracy: 0.7883\n",
      "Density: 300\n",
      "Final Training Loss: 4.449776440651476e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6411774158477783\n",
      "Final Validation Accuracy: 0.7731034755706787\n",
      "Test Accuracy: 78.83 %\n",
      "Test Loss: 1.65\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6550 - accuracy: 0.7806\n",
      "Density: 350\n",
      "Final Training Loss: 3.5713347301680187e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5333704948425293\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 78.06 %\n",
      "Test Loss: 1.66\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7135 - accuracy: 0.7900\n",
      "Density: 400\n",
      "Final Training Loss: 4.5552187089015206e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6754086017608643\n",
      "Final Validation Accuracy: 0.7820689678192139\n",
      "Test Accuracy: 79.00 %\n",
      "Test Loss: 1.71\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6616 - accuracy: 0.7966\n",
      "Density: 450\n",
      "Final Training Loss: 4.723961239960772e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6203457117080688\n",
      "Final Validation Accuracy: 0.7889655232429504\n",
      "Test Accuracy: 79.66 %\n",
      "Test Loss: 1.66\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6714 - accuracy: 0.7933\n",
      "Density: 500\n",
      "Final Training Loss: 3.910874397661246e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6596729755401611\n",
      "Final Validation Accuracy: 0.7799999713897705\n",
      "Test Accuracy: 79.33 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6181 - accuracy: 0.7856\n",
      "Density: 550\n",
      "Final Training Loss: 3.143205162814411e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6524972915649414\n",
      "Final Validation Accuracy: 0.769655168056488\n",
      "Test Accuracy: 78.56 %\n",
      "Test Loss: 1.62\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7515 - accuracy: 0.7773\n",
      "Density: 600\n",
      "Final Training Loss: 3.9116946481954074e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7328212261199951\n",
      "Final Validation Accuracy: 0.7751724123954773\n",
      "Test Accuracy: 77.73 %\n",
      "Test Loss: 1.75\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5878 - accuracy: 0.7938\n",
      "Density: 650\n",
      "Final Training Loss: 3.462193944869796e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5322779417037964\n",
      "Final Validation Accuracy: 0.7924137711524963\n",
      "Test Accuracy: 79.38 %\n",
      "Test Loss: 1.59\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6188 - accuracy: 0.7938\n",
      "Density: 700\n",
      "Final Training Loss: 3.833388859675324e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5643048286437988\n",
      "Final Validation Accuracy: 0.7813792824745178\n",
      "Test Accuracy: 79.38 %\n",
      "Test Loss: 1.62\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5553 - accuracy: 0.7960\n",
      "Density: 750\n",
      "Final Training Loss: 3.8473646668535366e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.496852993965149\n",
      "Final Validation Accuracy: 0.8013793230056763\n",
      "Test Accuracy: 79.60 %\n",
      "Test Loss: 1.56\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.5389 - accuracy: 0.7988\n",
      "Density: 800\n",
      "Final Training Loss: 3.255020715187129e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.4608345031738281\n",
      "Final Validation Accuracy: 0.791034460067749\n",
      "Test Accuracy: 79.88 %\n",
      "Test Loss: 1.54\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.9138 - accuracy: 0.7751\n",
      "Density: 850\n",
      "Final Training Loss: 3.7550708498201857e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.863211750984192\n",
      "Final Validation Accuracy: 0.7806896567344666\n",
      "Test Accuracy: 77.51 %\n",
      "Test Loss: 1.91\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.8605 - accuracy: 0.7817\n",
      "Density: 900\n",
      "Final Training Loss: 3.210418242360902e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7416468858718872\n",
      "Final Validation Accuracy: 0.7834482789039612\n",
      "Test Accuracy: 78.17 %\n",
      "Test Loss: 1.86\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6717 - accuracy: 0.7993\n",
      "Density: 950\n",
      "Final Training Loss: 2.7500266241986537e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6734187602996826\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 79.93 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6954 - accuracy: 0.8037\n",
      "Density: 1000\n",
      "Final Training Loss: 3.232607070913218e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6210309267044067\n",
      "Final Validation Accuracy: 0.8062068819999695\n",
      "Test Accuracy: 80.37 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.7337 - accuracy: 0.7822\n",
      "Density: 1050\n",
      "Final Training Loss: 3.3045392910935334e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7278374433517456\n",
      "Final Validation Accuracy: 0.7793103456497192\n",
      "Test Accuracy: 78.22 %\n",
      "Test Loss: 1.73\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 1.7008 - accuracy: 0.7905\n",
      "Density: 1100\n",
      "Final Training Loss: 2.906847385020228e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.6569880247116089\n",
      "Final Validation Accuracy: 0.7917241454124451\n",
      "Test Accuracy: 79.05 %\n",
      "Test Loss: 1.70\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6665 - accuracy: 0.8054\n",
      "Density: 1150\n",
      "Final Training Loss: 3.6461466379478225e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.5800879001617432\n",
      "Final Validation Accuracy: 0.8013793230056763\n",
      "Test Accuracy: 80.54 %\n",
      "Test Loss: 1.67\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 1.7331 - accuracy: 0.7971\n",
      "Density: 1200\n",
      "Final Training Loss: 2.8416948794074415e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.7120484113693237\n",
      "Final Validation Accuracy: 0.7889655232429504\n",
      "Test Accuracy: 79.71 %\n",
      "Test Loss: 1.73\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1.6452 - accuracy: 0.7982\n",
      "Density: 1250\n",
      "Final Training Loss: 3.577287941425311e-07\n",
      "Final Training Accuracy: 1.0\n",
      "Final Validation Loss: 1.615990161895752\n",
      "Final Validation Accuracy: 0.7903448343276978\n",
      "Test Accuracy: 79.82 %\n",
      "Test Loss: 1.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     17\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1733\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1731\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1733\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m         ):\n\u001b[1;32m   1741\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1401\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1402\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1403\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1406\u001b[0m )\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Density 50 to 5000 -> does not realy matter after 100 and not matter anymore after 500\n",
    "for i in range(50, 5001, 50):\n",
    "    clear_session() # clear keras session\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "    #tf.config.gpu.set_per_process_memory_fraction(0.5)\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(256,128,1)))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "    model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Density: %.i' % (i))\n",
    "    print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    print('Test Accuracy: %.2f %%' % (accuracy*100))\n",
    "    print('Test Loss: %.2f' % (loss))\n",
    "    visualize_history(history, f'dens={i}_finacc={history.history[\"val_accuracy\"][-1]:.2f}')\n",
    "    \n",
    "    # Free Memory for following models\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1fae1b-0bbe-4363-91d8-2d0d92146f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear grafic memory\n",
    "clear_session() # clear keras session\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph\n",
    "# Free Memory for following models\n",
    "del model\n",
    "gc.collect()\n",
    "tf.compat.v1.reset_default_graph()  # Reset the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773b8a5-6d69-4533-a52f-e998c0c9c5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
