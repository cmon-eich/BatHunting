{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrMjhlWCQeDV"
   },
   "source": [
    "This is a simple CNN for the classification of bat species based on echolocation calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDTLN9HWQaqJ"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import sqlite3\n",
    "import os\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import relu, tanh, linear\n",
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCB-dh7FQ0nB",
    "outputId": "084fd460-08f4-4d79-fdec-07031f295de8"
   },
   "outputs": [],
   "source": [
    "# Connect to drive and load db\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "db_path = 'batcallsv14.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1nT2ANXVNxw"
   },
   "outputs": [],
   "source": [
    "# Load calls from db\n",
    "\n",
    "# Set a fixed number of calls per species, or set to 0 to include all calls\n",
    "num_calls = 600\n",
    "\n",
    "# Select specific species for classification\n",
    "targets = [4, 18] # GroÃŸer Abendsegler - Nyctalus noctula, Zwergfledermaus - Pipistrellus pipistrellus\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "def _convert_array(text):\n",
    "\tout = io.BytesIO(text)\n",
    "\tout.seek(0)\n",
    "\treturn np.load(out, allow_pickle=True)\n",
    "def _adapt_array(arr):\n",
    "\tout = io.BytesIO()\n",
    "\tnp.save(out, arr)\n",
    "\tout.seek(0)\n",
    "\treturn sqlite3.Binary(out.read())\n",
    "\n",
    "sqlite3.register_adapter(np.ndarray, _adapt_array)\n",
    "sqlite3.register_converter(\"ARRAY\", _convert_array)\n",
    "\n",
    "with sqlite3.connect(db_path, detect_types=True) as con:\n",
    "  cur = con.cursor()\n",
    "  for target in targets:\n",
    "    if num_calls > 0:\n",
    "      rows = [x for x in cur.execute(f\"SELECT * FROM batcalls WHERE target = {target} LIMIT {num_calls}\")]\n",
    "    else:\n",
    "      rows = [x for x in cur.execute(f\"SELECT * FROM batcalls WHERE target = {target}\")]\n",
    "    all_rows.extend(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VovFAnsUVzZz"
   },
   "outputs": [],
   "source": [
    "# Make spectrograms from db rows\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "  nyq = 0.5 * fs\n",
    "  low = lowcut / nyq\n",
    "  high = highcut / nyq\n",
    "  b, a = butter(order, [low, high], btype='band')\n",
    "  return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "  b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "  y = lfilter(b, a, data)\n",
    "  return y\n",
    "\n",
    "def make_sets(data_rows):\n",
    "  spec_list = []\n",
    "  label_list = []\n",
    "  for row in data_rows:\n",
    "    target_class = targets.index(row[1])\n",
    "    call = row[4].astype(np.float32, order='C') / 32768.0\n",
    "\n",
    "    # Bandpass to filter low and high frequencies\n",
    "    data_bp = butter_bandpass_filter(call, 1500, 12000, 44100, 5)\n",
    "\n",
    "    # Normalize\n",
    "    data_bp -= np.mean(data_bp)\n",
    "    data_bp /= np.std(data_bp)\n",
    "\n",
    "    # Calculate spectrogram with FFT\n",
    "    stft = np.abs(librosa.stft(data_bp, n_fft=512, hop_length=32))\n",
    "    stft = 10 * np.log10(stft)\n",
    "    stft = np.nan_to_num(stft)\n",
    "\n",
    "    # Scale between [0,1] and reduce shape if needed\n",
    "    stft = (stft - np.min(stft)) / (np.max(stft) - np.min(stft))\n",
    "    stft = np.reshape(stft, (257, 138, 1))\n",
    "    stft = stft[:256, -128: , :]\n",
    "\n",
    "    spec_list.append(stft)\n",
    "    label_list.append(target_class)\n",
    "  return spec_list, label_list\n",
    "\n",
    "spectrograms, labels = make_sets(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-QPBaBZWlBX"
   },
   "outputs": [],
   "source": [
    "def test_train_split(spectrograms, labels, batchsize, train_split=0.7, val_split=0.2, test_split=0.1):\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((spectrograms, labels))\n",
    "  dataset = dataset.shuffle(buffer_size=2, seed=42)\n",
    "\n",
    "  train_size = int(train_split * len(spectrograms))\n",
    "  val_size = int(val_split * len(spectrograms))\n",
    "\n",
    "  train_dataset = dataset.take(train_size)\n",
    "  val_dataset = dataset.skip(train_size).take(val_size)\n",
    "  test_dataset = dataset.skip(train_size).skip(val_size)\n",
    "\n",
    "  train_dataset = train_dataset.batch(batchsize)\n",
    "  val_dataset = val_dataset.batch(batchsize)\n",
    "  test_dataset = test_dataset.batch(batchsize)\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o69hHknWz6N"
   },
   "outputs": [],
   "source": [
    "# Simple CNN for binary classification\n",
    "\n",
    "def generate_CNN():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  # Note the input shape is the desired size of the image 200x200 with 3 bytes color\n",
    "  # This is the first convolution\n",
    "  model.add(Conv2D(16, (3,3), activation='relu', input_shape=(256, 128, 1)))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  # The second convolution\n",
    "  model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  # The third convolution\n",
    "  model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  # The fourth convolution\n",
    "  model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  # # The fifth convolution\n",
    "  model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  # Flatten the results to feed into a DNN\n",
    "  model.add(Flatten())\n",
    "  # 512 neuron hidden layer\n",
    "  model.add(Dense(512, activation='relu'))\n",
    "  # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('dandelions') and 1 for the other ('grass')\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "  keras.utils.plot_model(model, \"simple_CNN.png\", show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0OgS_YfiT4-"
   },
   "outputs": [],
   "source": [
    "# Generate and compile model\n",
    "\n",
    "model = generate_CNN()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=RMSprop(lr=0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nhdi1Tos14U"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset, validation_dataset, test_dataset = test_train_split(spectrograms, labels, train_split=0.7, val_split=0.2, test_split=0.1, batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS6P4_O0iWWb"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "epochs = 150\n",
    "history = model.fit(train_dataset,\n",
    "      epochs=epochs,\n",
    "      validation_data = validation_dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s21f_fMkrZyU"
   },
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axiC6I045PjI"
   },
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
